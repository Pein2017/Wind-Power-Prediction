{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np  # noqa\n",
    "import optuna\n",
    "import time\n",
    "import logging  # noqa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from lightgbm import LGBMRegressor, early_stopping, log_evaluation\n",
    "\n",
    "# Enable logging\n",
    "optuna.logging.set_verbosity(optuna.logging.INFO)\n",
    "\n",
    "\n",
    "class TimingCallback:\n",
    "    def __init__(self):\n",
    "        self.start_time = None\n",
    "\n",
    "    def __call__(self, study, trial):\n",
    "        if self.start_time is None:\n",
    "            self.start_time = time.time()\n",
    "        else:\n",
    "            elapsed_time = time.time() - self.start_time\n",
    "            print(f\"Trial {trial.number} finished in {elapsed_time:.2f} seconds.\")\n",
    "            self.start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "# data_dir = \"/data/Pein/Pytorch/Wind-Power-Prediction/data/\"\n",
    "\n",
    "train_path = '3-train_merged_55.csv'\n",
    "test_path = '3-test_merged_55.csv'\n",
    "\n",
    "train_data_selected = pd.read_csv(train_path)\n",
    "test_data_selected = pd.read_csv(test_path)\n",
    "\n",
    "# Set the ['power'] to be non-negative\n",
    "train_data_selected['power'] = train_data_selected['power'].apply(lambda x: x if x >= 0 else 0)\n",
    "test_data_selected['power'] = test_data_selected['power'].apply(lambda x: x if x >= 0 else 0)\n",
    "\n",
    "\n",
    "\n",
    "# Define features and target\n",
    "features = [\n",
    "    col\n",
    "    for col in train_data_selected.columns\n",
    "    if col not in [\"time\", \"lead_hour\", 'initial_time',\"power\"]\n",
    "]\n",
    "X_train = train_data_selected[features]\n",
    "y_train = train_data_selected[\"power\"]\n",
    "X_test = test_data_selected[features]\n",
    "y_test = test_data_selected[\"power\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (14592, 52), Shape of y_train: (14592,)\n",
      "Shape of X_test: (2880, 52), Shape of y_test: (2880,)\n"
     ]
    }
   ],
   "source": [
    "print(f'Shape of X_train: {X_train.shape}, Shape of y_train: {y_train.shape}')\n",
    "print(f'Shape of X_test: {X_test.shape}, Shape of y_test: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding time features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'time' column to datetime\n",
    "train_data_selected[\"time\"] = pd.to_datetime(train_data_selected[\"time\"])\n",
    "test_data_selected[\"time\"] = pd.to_datetime(test_data_selected[\"time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add time features\n",
    "def add_time_features(df):\n",
    "    df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
    "\n",
    "    # Existing time features\n",
    "    df[\"hour\"] = df[\"time\"].dt.hour\n",
    "    df[\"quarter_hour\"] = df[\"time\"].dt.minute // 15\n",
    "\n",
    "    # New time features\n",
    "    df[\"day\"] = df[\"time\"].dt.day\n",
    "    df[\"day_in_week\"] = df[\"time\"].dt.weekday\n",
    "\n",
    "    # Sine and cosine transformations\n",
    "    df[\"hour_sin\"] = np.sin(2 * np.pi * df[\"hour\"] / 24)\n",
    "    df[\"hour_cos\"] = np.cos(2 * np.pi * df[\"hour\"] / 24)\n",
    "\n",
    "    df[\"quarter_hour_sin\"] = np.sin(2 * np.pi * df[\"quarter_hour\"] / 4)\n",
    "    df[\"quarter_hour_cos\"] = np.cos(2 * np.pi * df[\"quarter_hour\"] / 4)\n",
    "\n",
    "    df[\"day_sin\"] = np.sin(2 * np.pi * df[\"day\"] / 31)\n",
    "    df[\"day_cos\"] = np.cos(2 * np.pi * df[\"day\"] / 31)\n",
    "\n",
    "    df[\"day_in_week_sin\"] = np.sin(2 * np.pi * df[\"day_in_week\"] / 7)\n",
    "    df[\"day_in_week_cos\"] = np.cos(2 * np.pi * df[\"day_in_week\"] / 7)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add time features to both train and test data\n",
    "train_data_selected = add_time_features(train_data_selected)\n",
    "test_data_selected = add_time_features(test_data_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features including the new time features\n",
    "time_features = [\n",
    "    \"hour\",\n",
    "    \"quarter_hour\",\n",
    "    \"day\",\n",
    "    # \"day_in_week\",\n",
    "    \"hour_sin\",\n",
    "    \"hour_cos\",\n",
    "    \"quarter_hour_sin\",\n",
    "    \"quarter_hour_cos\",\n",
    "    \"day_sin\",\n",
    "    \"day_cos\",\n",
    "    # \"day_in_week_sin\",\n",
    "    # \"day_in_week_cos\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = features  + time_features\n",
    "\n",
    "X_train = train_data_selected[all_features]\n",
    "X_test = test_data_selected[all_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def objective(\n",
    "    trial,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test=None,\n",
    "    y_test=None,\n",
    "    use_test_for_validation_flag=False,\n",
    "    use_sklearn_split = False,\n",
    "):\n",
    "    \n",
    "    params = {\n",
    "        \"objective\": \"regression\",\n",
    "        \"metric\": \"rmse\",\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 10, 80),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 10, 50),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 500, 1500),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 10, 50),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.3, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"num_threads\": 16,\n",
    "        \"seed\": 42,\n",
    "    }\n",
    "\n",
    "    model = LGBMRegressor(**params)\n",
    "\n",
    "    if use_test_for_validation_flag and X_test is not None and y_test is not None:\n",
    "        X_val_split, y_val_split = X_test, y_test\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=[(X_val_split, y_val_split)],\n",
    "            eval_metric=\"rmse\",\n",
    "            callbacks=[early_stopping(stopping_rounds=15), log_evaluation(period=500)],\n",
    "        )\n",
    "        preds = model.predict(X_val_split)\n",
    "    else:\n",
    "        \n",
    "        if use_sklearn_split:\n",
    "            # Use sklearn's train_test_split\n",
    "            X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "                X_train, y_train, test_size=0.2, random_state=42)\n",
    "        else:\n",
    "            # Calculate the split index\n",
    "            split_index = int(0.8 * len(X_train))\n",
    "\n",
    "            # Split the data manually\n",
    "            X_train_split, X_val_split = X_train[:split_index], X_train[split_index:]\n",
    "            y_train_split, y_val_split = y_train[:split_index], y_train[split_index:]\n",
    "        \n",
    "        model.fit(\n",
    "            X_train_split,\n",
    "            y_train_split,\n",
    "            eval_set=[(X_val_split, y_val_split)],\n",
    "            eval_metric=\"rmse\",\n",
    "            callbacks=[early_stopping(stopping_rounds=100), log_evaluation(period=500)],\n",
    "        )\n",
    "        preds = model.predict(X_val_split)\n",
    "\n",
    "    # Calculate RMSE\n",
    "    rmse = root_mean_squared_error(y_val_split, preds)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-15 17:25:46,935] A new study created in memory with name: no-name-182b8974-5f90-409c-9360-b84f35d3a171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002925 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13422\n",
      "[LightGBM] [Info] Number of data points in the train set: 11673, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 105784.364306\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-15 17:25:47,515] Trial 0 finished with value: 66672.32036769795 and parameters: {'num_leaves': 15, 'max_depth': 23, 'learning_rate': 0.02492148539094784, 'n_estimators': 650, 'min_child_samples': 16, 'subsample': 0.7865766454128991, 'colsample_bytree': 0.8479795769134237}. Best is trial 0 with value: 66672.32036769795.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[199]\tvalid_0's rmse: 66672.3\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002260 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13422\n",
      "[LightGBM] [Info] Number of data points in the train set: 11673, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 105784.364306\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-15 17:25:47,941] Trial 1 finished with value: 67016.25081615792 and parameters: {'num_leaves': 18, 'max_depth': 19, 'learning_rate': 0.03813624414734297, 'n_estimators': 1144, 'min_child_samples': 34, 'subsample': 0.8300798499868236, 'colsample_bytree': 0.6099124850313824}. Best is trial 0 with value: 66672.32036769795.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[162]\tvalid_0's rmse: 67016.3\n",
      "Trial 1 finished in 0.43 seconds.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001599 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13422\n",
      "[LightGBM] [Info] Number of data points in the train set: 11673, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 105784.364306\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-15 17:25:48,618] Trial 2 finished with value: 68023.32035367792 and parameters: {'num_leaves': 64, 'max_depth': 25, 'learning_rate': 0.05188382970501199, 'n_estimators': 973, 'min_child_samples': 38, 'subsample': 0.33944024751559815, 'colsample_bytree': 0.8582559004267389}. Best is trial 0 with value: 66672.32036769795.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[100]\tvalid_0's rmse: 68023.3\n",
      "Trial 2 finished in 0.68 seconds.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001669 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13422\n",
      "[LightGBM] [Info] Number of data points in the train set: 11673, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 105784.364306\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-15 17:25:48,917] Trial 3 finished with value: 68784.9410242906 and parameters: {'num_leaves': 40, 'max_depth': 42, 'learning_rate': 0.13433431374324697, 'n_estimators': 1371, 'min_child_samples': 46, 'subsample': 0.8692597354266967, 'colsample_bytree': 0.7912792597139707}. Best is trial 0 with value: 66672.32036769795.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[31]\tvalid_0's rmse: 68784.9\n",
      "Trial 3 finished in 0.30 seconds.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001610 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13422\n",
      "[LightGBM] [Info] Number of data points in the train set: 11673, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 105784.364306\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-15 17:25:49,232] Trial 4 finished with value: 67824.26479661587 and parameters: {'num_leaves': 37, 'max_depth': 37, 'learning_rate': 0.09883749714331959, 'n_estimators': 801, 'min_child_samples': 13, 'subsample': 0.7502629169448335, 'colsample_bytree': 0.7060143511699986}. Best is trial 0 with value: 66672.32036769795.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[41]\tvalid_0's rmse: 67824.3\n",
      "Trial 4 finished in 0.31 seconds.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001485 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13422\n",
      "[LightGBM] [Info] Number of data points in the train set: 11673, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 105784.364306\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-15 17:25:49,766] Trial 5 finished with value: 67337.9682548772 and parameters: {'num_leaves': 54, 'max_depth': 48, 'learning_rate': 0.07700192204482569, 'n_estimators': 1494, 'min_child_samples': 36, 'subsample': 0.8162637604894329, 'colsample_bytree': 0.9956034868588317}. Best is trial 0 with value: 66672.32036769795.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[74]\tvalid_0's rmse: 67338\n",
      "Trial 5 finished in 0.53 seconds.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001745 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13422\n",
      "[LightGBM] [Info] Number of data points in the train set: 11673, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 105784.364306\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-15 17:25:50,051] Trial 6 finished with value: 67312.61983334826 and parameters: {'num_leaves': 16, 'max_depth': 34, 'learning_rate': 0.05438476311398182, 'n_estimators': 914, 'min_child_samples': 18, 'subsample': 0.44068392697408054, 'colsample_bytree': 0.8923778469751158}. Best is trial 0 with value: 66672.32036769795.\n",
      "[I 2024-08-15 17:25:50,219] Trial 7 finished with value: 67265.10477500617 and parameters: {'num_leaves': 15, 'max_depth': 28, 'learning_rate': 0.1850782116676188, 'n_estimators': 716, 'min_child_samples': 17, 'subsample': 0.5212540975106517, 'colsample_bytree': 0.5870951206837318}. Best is trial 0 with value: 66672.32036769795.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[113]\tvalid_0's rmse: 67312.6\n",
      "Trial 6 finished in 0.28 seconds.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001813 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13422\n",
      "[LightGBM] [Info] Number of data points in the train set: 11673, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 105784.364306\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[22]\tvalid_0's rmse: 67265.1\n",
      "Trial 7 finished in 0.17 seconds.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001651 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13422\n",
      "[LightGBM] [Info] Number of data points in the train set: 11673, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 105784.364306\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-15 17:25:50,692] Trial 8 finished with value: 68594.74168030538 and parameters: {'num_leaves': 74, 'max_depth': 19, 'learning_rate': 0.13617140008168402, 'n_estimators': 1418, 'min_child_samples': 50, 'subsample': 0.7161320240200888, 'colsample_bytree': 0.6731146245336079}. Best is trial 0 with value: 66672.32036769795.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Early stopping, best iteration is:\n",
      "[29]\tvalid_0's rmse: 68594.7\n",
      "Trial 8 finished in 0.47 seconds.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001626 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13422\n",
      "[LightGBM] [Info] Number of data points in the train set: 11673, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 105784.364306\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-15 17:25:51,041] Trial 9 finished with value: 66126.73272823748 and parameters: {'num_leaves': 45, 'max_depth': 23, 'learning_rate': 0.17801774348406232, 'n_estimators': 1445, 'min_child_samples': 47, 'subsample': 0.7293072402595091, 'colsample_bytree': 0.9898948059484636}. Best is trial 9 with value: 66126.73272823748.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[23]\tvalid_0's rmse: 66126.7\n",
      "Trial 9 finished in 0.35 seconds.\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001995 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13422\n",
      "[LightGBM] [Info] Number of data points in the train set: 11673, number of used features: 61\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Start training from score 105784.364306\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-15 17:25:51,378] Trial 10 finished with value: 69306.63487728058 and parameters: {'num_leaves': 31, 'max_depth': 11, 'learning_rate': 0.191670012768715, 'n_estimators': 1251, 'min_child_samples': 28, 'subsample': 0.9996113805242436, 'colsample_bytree': 0.9654904513977842}. Best is trial 9 with value: 66126.73272823748.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[14]\tvalid_0's rmse: 69306.6\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Trial 10 finished in 0.34 seconds.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002263 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13422\n",
      "[LightGBM] [Info] Number of data points in the train set: 11673, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 105784.364306\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-15 17:25:53,154] Trial 11 finished with value: 68307.77210256713 and parameters: {'num_leaves': 53, 'max_depth': 21, 'learning_rate': 0.010874323270268021, 'n_estimators': 518, 'min_child_samples': 25, 'subsample': 0.5608954161002693, 'colsample_bytree': 0.8819106546609187}. Best is trial 9 with value: 66126.73272823748.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500]\tvalid_0's rmse: 68436.5\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[450]\tvalid_0's rmse: 68307.8\n",
      "Trial 11 finished in 1.78 seconds.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001778 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13422\n",
      "[LightGBM] [Info] Number of data points in the train set: 11673, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 105784.364306\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-15 17:25:53,464] Trial 12 finished with value: 66777.97493932008 and parameters: {'num_leaves': 27, 'max_depth': 11, 'learning_rate': 0.13677308109654157, 'n_estimators': 510, 'min_child_samples': 43, 'subsample': 0.6443768795595819, 'colsample_bytree': 0.7904252527352098}. Best is trial 9 with value: 66126.73272823748.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[32]\tvalid_0's rmse: 66778\n",
      "Trial 12 finished in 0.31 seconds.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001596 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13422\n",
      "[LightGBM] [Info] Number of data points in the train set: 11673, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 105784.364306\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-15 17:25:53,845] Trial 13 finished with value: 67499.25083921826 and parameters: {'num_leaves': 50, 'max_depth': 25, 'learning_rate': 0.16599509924524186, 'n_estimators': 1127, 'min_child_samples': 22, 'subsample': 0.9378948338037529, 'colsample_bytree': 0.5009649316701343}. Best is trial 9 with value: 66126.73272823748.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[32]\tvalid_0's rmse: 67499.3\n",
      "Trial 13 finished in 0.38 seconds.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001767 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13422\n",
      "[LightGBM] [Info] Number of data points in the train set: 11673, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 105784.364306\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-15 17:25:54,465] Trial 14 finished with value: 70169.97552620983 and parameters: {'num_leaves': 78, 'max_depth': 32, 'learning_rate': 0.10790048114023813, 'n_estimators': 687, 'min_child_samples': 31, 'subsample': 0.6776827813450419, 'colsample_bytree': 0.9335020425646819}. Best is trial 9 with value: 66126.73272823748.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[41]\tvalid_0's rmse: 70170\n",
      "Trial 14 finished in 0.62 seconds.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001521 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13422\n",
      "[LightGBM] [Info] Number of data points in the train set: 11673, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 105784.364306\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[500]\tvalid_0's rmse: 66580.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-15 17:25:55,224] Trial 15 finished with value: 65739.98221690734 and parameters: {'num_leaves': 10, 'max_depth': 16, 'learning_rate': 0.012909538157342448, 'n_estimators': 1082, 'min_child_samples': 12, 'subsample': 0.6140953107725337, 'colsample_bytree': 0.8300534305565738}. Best is trial 15 with value: 65739.98221690734.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[728]\tvalid_0's rmse: 65740\n",
      "Trial 15 finished in 0.76 seconds.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001875 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13422\n",
      "[LightGBM] [Info] Number of data points in the train set: 11673, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 105784.364306\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-15 17:25:55,574] Trial 16 finished with value: 67665.54326742933 and parameters: {'num_leaves': 27, 'max_depth': 14, 'learning_rate': 0.16528372292003557, 'n_estimators': 1236, 'min_child_samples': 11, 'subsample': 0.6427294235693647, 'colsample_bytree': 0.7829327872381497}. Best is trial 15 with value: 65739.98221690734.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[34]\tvalid_0's rmse: 67665.5\n",
      "Trial 16 finished in 0.35 seconds.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002114 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13422\n",
      "[LightGBM] [Info] Number of data points in the train set: 11673, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 105784.364306\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-15 17:25:56,313] Trial 17 finished with value: 69071.41258423016 and parameters: {'num_leaves': 65, 'max_depth': 15, 'learning_rate': 0.08445330223368193, 'n_estimators': 1106, 'min_child_samples': 41, 'subsample': 0.5290470444393731, 'colsample_bytree': 0.9290153265415981}. Best is trial 15 with value: 65739.98221690734.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Early stopping, best iteration is:\n",
      "[95]\tvalid_0's rmse: 69071.4\n",
      "Trial 17 finished in 0.74 seconds.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001590 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13422\n",
      "[LightGBM] [Info] Number of data points in the train set: 11673, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 105784.364306\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-15 17:25:56,722] Trial 18 finished with value: 68029.88608087871 and parameters: {'num_leaves': 44, 'max_depth': 16, 'learning_rate': 0.11926907734405161, 'n_estimators': 1302, 'min_child_samples': 22, 'subsample': 0.5763689205758367, 'colsample_bytree': 0.9992270880430033}. Best is trial 15 with value: 65739.98221690734.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[33]\tvalid_0's rmse: 68029.9\n",
      "Trial 18 finished in 0.41 seconds.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001719 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13422\n",
      "[LightGBM] [Info] Number of data points in the train set: 11673, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 105784.364306\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-15 17:25:57,236] Trial 19 finished with value: 68041.4345312645 and parameters: {'num_leaves': 66, 'max_depth': 28, 'learning_rate': 0.15603884765359305, 'n_estimators': 893, 'min_child_samples': 49, 'subsample': 0.43936322978833287, 'colsample_bytree': 0.7144099366319887}. Best is trial 15 with value: 65739.98221690734.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[36]\tvalid_0's rmse: 68041.4\n",
      "Trial 19 finished in 0.51 seconds.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001732 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13422\n",
      "[LightGBM] [Info] Number of data points in the train set: 11673, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 105784.364306\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-15 17:25:57,612] Trial 20 finished with value: 67533.45769330811 and parameters: {'num_leaves': 34, 'max_depth': 40, 'learning_rate': 0.07335655939255928, 'n_estimators': 1048, 'min_child_samples': 31, 'subsample': 0.6033057990895455, 'colsample_bytree': 0.8283248955244971}. Best is trial 15 with value: 65739.98221690734.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[51]\tvalid_0's rmse: 67533.5\n",
      "Trial 20 finished in 0.38 seconds.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001557 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13422\n",
      "[LightGBM] [Info] Number of data points in the train set: 11673, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 105784.364306\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-15 17:25:58,218] Trial 21 finished with value: 65934.3336045431 and parameters: {'num_leaves': 11, 'max_depth': 22, 'learning_rate': 0.011663946804831295, 'n_estimators': 621, 'min_child_samples': 15, 'subsample': 0.7516236368382061, 'colsample_bytree': 0.8426340570876922}. Best is trial 15 with value: 65739.98221690734.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500]\tvalid_0's rmse: 66595.4\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[589]\tvalid_0's rmse: 65934.3\n",
      "Trial 21 finished in 0.61 seconds.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001641 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13422\n",
      "[LightGBM] [Info] Number of data points in the train set: 11673, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 105784.364306\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[500]\tvalid_0's rmse: 65434.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-15 17:25:59,063] Trial 22 finished with value: 65145.628618067436 and parameters: {'num_leaves': 11, 'max_depth': 18, 'learning_rate': 0.019114033699704894, 'n_estimators': 1485, 'min_child_samples': 11, 'subsample': 0.7222384863256942, 'colsample_bytree': 0.9213403246085562}. Best is trial 22 with value: 65145.628618067436.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[653]\tvalid_0's rmse: 65145.6\n",
      "Trial 22 finished in 0.84 seconds.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002359 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13422\n",
      "[LightGBM] [Info] Number of data points in the train set: 11673, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 105784.364306\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-15 17:25:59,819] Trial 23 finished with value: 65380.53357340086 and parameters: {'num_leaves': 10, 'max_depth': 17, 'learning_rate': 0.01397772176500813, 'n_estimators': 784, 'min_child_samples': 12, 'subsample': 0.8750592317426404, 'colsample_bytree': 0.9069662874673612}. Best is trial 22 with value: 65145.628618067436.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500]\tvalid_0's rmse: 65411.2\n",
      "Early stopping, best iteration is:\n",
      "[511]\tvalid_0's rmse: 65380.5\n",
      "Trial 23 finished in 0.76 seconds.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001625 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13422\n",
      "[LightGBM] [Info] Number of data points in the train set: 11673, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 105784.364306\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-15 17:26:00,468] Trial 24 finished with value: 65070.400147098524 and parameters: {'num_leaves': 23, 'max_depth': 17, 'learning_rate': 0.032836943700487606, 'n_estimators': 791, 'min_child_samples': 10, 'subsample': 0.9166068377635276, 'colsample_bytree': 0.9172201388711717}. Best is trial 24 with value: 65070.400147098524.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[259]\tvalid_0's rmse: 65070.4\n",
      "Trial 24 finished in 0.65 seconds.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001572 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13422\n",
      "[LightGBM] [Info] Number of data points in the train set: 11673, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 105784.364306\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-15 17:26:00,905] Trial 25 finished with value: 67878.93919828579 and parameters: {'num_leaves': 22, 'max_depth': 10, 'learning_rate': 0.03538533516585298, 'n_estimators': 798, 'min_child_samples': 20, 'subsample': 0.8839365379133461, 'colsample_bytree': 0.9238931531282645}. Best is trial 24 with value: 65070.400147098524.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[150]\tvalid_0's rmse: 67878.9\n",
      "Trial 25 finished in 0.44 seconds.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001564 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13422\n",
      "[LightGBM] [Info] Number of data points in the train set: 11673, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 105784.364306\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-15 17:26:01,341] Trial 26 finished with value: 66889.00344512996 and parameters: {'num_leaves': 22, 'max_depth': 19, 'learning_rate': 0.05440573411956387, 'n_estimators': 789, 'min_child_samples': 10, 'subsample': 0.9374130179763374, 'colsample_bytree': 0.9102133952373954}. Best is trial 24 with value: 65070.400147098524.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[154]\tvalid_0's rmse: 66889\n",
      "Trial 26 finished in 0.44 seconds.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001669 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13422\n",
      "[LightGBM] [Info] Number of data points in the train set: 11673, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 105784.364306\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-15 17:26:01,808] Trial 27 finished with value: 67024.0342190091 and parameters: {'num_leaves': 22, 'max_depth': 13, 'learning_rate': 0.030300532784675087, 'n_estimators': 856, 'min_child_samples': 14, 'subsample': 0.9537195569905171, 'colsample_bytree': 0.9433299670590505}. Best is trial 24 with value: 65070.400147098524.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[166]\tvalid_0's rmse: 67024\n",
      "Trial 27 finished in 0.47 seconds.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001480 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13422\n",
      "[LightGBM] [Info] Number of data points in the train set: 11673, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 105784.364306\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-15 17:26:02,091] Trial 28 finished with value: 66633.85305085807 and parameters: {'num_leaves': 10, 'max_depth': 18, 'learning_rate': 0.04635807529669439, 'n_estimators': 943, 'min_child_samples': 19, 'subsample': 0.8931459791884895, 'colsample_bytree': 0.8820843070045483}. Best is trial 24 with value: 65070.400147098524.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[146]\tvalid_0's rmse: 66633.9\n",
      "Trial 28 finished in 0.28 seconds.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001793 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13422\n",
      "[LightGBM] [Info] Number of data points in the train set: 11673, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 105784.364306\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-15 17:26:02,700] Trial 29 finished with value: 66004.01667130472 and parameters: {'num_leaves': 18, 'max_depth': 26, 'learning_rate': 0.02609676978967953, 'n_estimators': 583, 'min_child_samples': 10, 'subsample': 0.8018952318719619, 'colsample_bytree': 0.9576584468751066}. Best is trial 24 with value: 65070.400147098524.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[232]\tvalid_0's rmse: 66004\n",
      "Trial 29 finished in 0.61 seconds.\n",
      "Best parameters found:  {'num_leaves': 23, 'max_depth': 17, 'learning_rate': 0.032836943700487606, 'n_estimators': 791, 'min_child_samples': 10, 'subsample': 0.9166068377635276, 'colsample_bytree': 0.9172201388711717}\n"
     ]
    }
   ],
   "source": [
    "# Create the study and optimize\n",
    "use_test_for_validation_flag = False\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(\n",
    "    lambda trial: objective(\n",
    "        trial,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        use_test_for_validation_flag=use_test_for_validation_flag,\n",
    "    ),\n",
    "    n_trials=30,\n",
    "    callbacks=[TimingCallback()],\n",
    ")\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = study.best_params\n",
    "print(\"Best parameters found: \", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_params = {\n",
    "#     \"num_leaves\": 70,\n",
    "#     \"max_depth\": 36,\n",
    "#     \"learning_rate\": 0.09304350950671668,\n",
    "#     \"n_estimators\": 1158,\n",
    "#     \"min_child_samples\": 18,\n",
    "#     \"subsample\": 0.579731306036922,\n",
    "#     \"colsample_bytree\": 0.8511910376678277,\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'num_leaves': 23, 'max_depth': 17, 'learning_rate': 0.032836943700487606, 'n_estimators': 791, 'min_child_samples': 10, 'subsample': 0.9166068377635276, 'colsample_bytree': 0.9172201388711717}\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002393 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13423\n",
      "[LightGBM] [Info] Number of data points in the train set: 14592, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 101382.392957\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters found: \", best_params)\n",
    "best_params[\"num_threads\"] = 16\n",
    "\n",
    "# Train the model with the best parameters\n",
    "best_model = LGBMRegressor(**best_params)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Extract feature importances\n",
    "feature_importances = best_model.feature_importances_\n",
    "\n",
    "# Create a DataFrame to hold the features and their importances\n",
    "importance_df = pd.DataFrame(\n",
    "    {\"Feature\": X_train.columns, \"Importance\": feature_importances}\n",
    ")\n",
    "\n",
    "# Sort the DataFrame by importance\n",
    "importance_df = importance_df.sort_values(by=\"Importance\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on the test set: 61308.7057\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = root_mean_squared_error(y_test, y_pred)\n",
    "print(f\"RMSE on the test set: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save important_features to csv file\n",
    "if use_test_for_validation_flag:\n",
    "    importance_df.to_csv(\"farm_important_features_use_test_for_validation.csv\", index=False)\n",
    "else:\n",
    "    importance_df.to_csv(\"farm_important_features.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the threshold for feature importance\n",
    "importance_threshold = 0\n",
    "\n",
    "# Select features with importance greater than or equal to the threshold\n",
    "selected_features = importance_df[importance_df[\"Importance\"] >= importance_threshold][\n",
    "    \"Feature\"\n",
    "].tolist()\n",
    "\n",
    "# Separate selected features into original input features and time features\n",
    "selected_input_features = [\n",
    "    feature for feature in selected_features if feature not in time_features\n",
    "]\n",
    "selected_time_features = [\n",
    "    feature for feature in selected_features if feature in time_features\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the filtered features with the power column and time features\n",
    "train_data_final = pd.concat(\n",
    "    [\n",
    "        train_data_selected[\"time\"],\n",
    "        X_train[selected_input_features],\n",
    "        train_data_selected[\"power\"],\n",
    "        train_data_selected[time_features],\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "test_data_final = pd.concat(\n",
    "    [\n",
    "        test_data_selected[\"time\"],\n",
    "        X_test[selected_input_features],\n",
    "        test_data_selected[\"power\"],\n",
    "        test_data_selected[time_features],\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# Ensure the final order of columns\n",
    "final_columns = [\"time\"] + selected_input_features + [\"power\"] + time_features\n",
    "train_data_final = train_data_final[final_columns]\n",
    "test_data_final = test_data_final[final_columns]\n",
    "\n",
    "# Display the new shapes of train_data_final and test_data_final\n",
    "print(\"Shape of train_data_final:\", train_data_final.shape)\n",
    "print(\"Shape of test_data_final:\", test_data_final.shape)\n",
    "print(\"Selected input features:\", selected_input_features)\n",
    "print(f\"Train columns : {train_data_final.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final datasets to CSV\n",
    "file_name_common = '66_withTime'\n",
    "train_data_final.to_csv( f\"4-train_{file_name_common}.csv\", index=False)\n",
    "test_data_final.to_csv( f\"4-test_{file_name_common}.csv\", index=False)\n",
    "print(\"Final datasets saved to CSV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pein_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
