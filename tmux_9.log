[I 2024-08-19 05:20:17,590] Using an existing study with name '24-08-19-no_time-farm_66' instead of creating a new one.
Creating study "24-08-19-no_time-farm_66" with storage "sqlite:////data/Pein/Pytorch/Wind-Power-Prediction/optuna_results/24-08-19-no_time/24-08-19-no_time-farm_66.db?mode=wal"...
Using sampler cma with seed 72157
Using sampler : CmaEsSampler, pruner : <optuna.pruners._median.MedianPruner object at 0x7f31fe0028f0>
  0%|          | 0/24 [00:00<?, ?it/s]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                        0%|          | 0/24 [00:00<?, ?it/s]                                        0%|          | 0/24 [00:00<?, ?it/s]                                        0%|          | 0/24 [00:00<?, ?it/s]                                        0%|          | 0/24 [00:00<?, ?it/s]                                        0%|          | 0/24 [00:00<?, ?it/s]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA A100 80GB PCIe') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 702 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
702 K     Trainable params
0         Non-trainable params
702 K     Total params
2.812     Total estimated model params size (MB)
115       Modules in train mode
0         Modules in eval mode
Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1023, in _run_stage
    self._run_sanity_check()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1052, in _run_sanity_check
    val_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py", line 178, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 128, in run
    batch, batch_idx, dataloader_idx = next(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 133, in __next__
    batch = super().__next__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 60, in __next__
    batch = next(self.iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 341, in __next__
    out = next(self._iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 142, in __next__
    out = next(self.iterators[0])
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1344, in _next_data
    return self._process_data(data)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1370, in _process_data
    data.reraise()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/_utils.py", line 706, in reraise
    raise exception
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 309, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 55, in fetch
    return self.collate_fn(data)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 317, in default_collate
    return collate(batch, collate_fn_map=default_collate_fn_map)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 174, in collate
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 174, in <listcomp>
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 142, in collate
    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 212, in collate_tensor_fn
    storage = elem._typed_storage()._new_shared(numel, device=elem.device)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/storage.py", line 968, in _new_shared
    untyped_storage = torch.UntypedStorage._new_shared(size * self._element_size(), device=device)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/storage.py", line 302, in _new_shared
    return cls._new_using_fd_cpu(size)
RuntimeError: unable to write to file </torch_192583_140291016_0>: No space left on device (28)

                                        0%|          | 0/24 [00:01<?, ?it/s]Best trial: 0. Best value: inf:   0%|          | 0/24 [00:01<?, ?it/s]Best trial: 0. Best value: inf:   4%|▍         | 1/24 [00:01<00:38,  1.66s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                              Best trial: 0. Best value: inf:   4%|▍         | 1/24 [00:12<00:38,  1.66s/it]                                                                              Best trial: 0. Best value: inf:   4%|▍         | 1/24 [00:12<00:38,  1.66s/it]                                                                              Best trial: 0. Best value: inf:   4%|▍         | 1/24 [00:12<00:38,  1.66s/it]                                                                              Best trial: 0. Best value: inf:   4%|▍         | 1/24 [00:12<00:38,  1.66s/it]                                                                              Best trial: 0. Best value: inf:   4%|▍         | 1/24 [00:12<00:38,  1.66s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 791 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
791 K     Trainable params
0         Non-trainable params
791 K     Total params
3.166     Total estimated model params size (MB)
135       Modules in train mode
0         Modules in eval mode
ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1131, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/queues.py", line 114, in get
    raise Empty
_queue.Empty

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1023, in _run_stage
    self._run_sanity_check()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1052, in _run_sanity_check
    val_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py", line 178, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 128, in run
    batch, batch_idx, dataloader_idx = next(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 133, in __next__
    batch = super().__next__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 60, in __next__
    batch = next(self.iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 341, in __next__
    out = next(self._iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 142, in __next__
    out = next(self.iterators[0])
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1327, in _next_data
    idx, data = self._get_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1293, in _get_data
    success, data = self._try_get_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1144, in _try_get_data
    raise RuntimeError(f'DataLoader worker (pid(s) {pids_str}) exited unexpectedly') from e
RuntimeError: DataLoader worker (pid(s) 197879) exited unexpectedly
                                                                              Best trial: 0. Best value: inf:   4%|▍         | 1/24 [00:18<00:38,  1.66s/it]Best trial: 0. Best value: inf:   4%|▍         | 1/24 [00:18<00:38,  1.66s/it]Best trial: 0. Best value: inf:   8%|▊         | 2/24 [00:18<03:48, 10.41s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                              Best trial: 0. Best value: inf:   8%|▊         | 2/24 [00:18<03:48, 10.41s/it]                                                                              Best trial: 0. Best value: inf:   8%|▊         | 2/24 [00:18<03:48, 10.41s/it]                                                                              Best trial: 0. Best value: inf:   8%|▊         | 2/24 [00:18<03:48, 10.41s/it]                                                                              Best trial: 0. Best value: inf:   8%|▊         | 2/24 [00:18<03:48, 10.41s/it]                                                                              Best trial: 0. Best value: inf:   8%|▊         | 2/24 [00:18<03:48, 10.41s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 485 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
485 K     Trainable params
0         Non-trainable params
485 K     Total params
1.943     Total estimated model params size (MB)
115       Modules in train mode
0         Modules in eval mode
ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1131, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/queues.py", line 114, in get
    raise Empty
_queue.Empty

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1023, in _run_stage
    self._run_sanity_check()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1052, in _run_sanity_check
    val_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py", line 178, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 128, in run
    batch, batch_idx, dataloader_idx = next(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 133, in __next__
    batch = super().__next__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 60, in __next__
    batch = next(self.iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 341, in __next__
    out = next(self._iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 142, in __next__
    out = next(self.iterators[0])
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1327, in _next_data
    idx, data = self._get_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1293, in _get_data
    success, data = self._try_get_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1144, in _try_get_data
    raise RuntimeError(f'DataLoader worker (pid(s) {pids_str}) exited unexpectedly') from e
RuntimeError: DataLoader worker (pid(s) 201234) exited unexpectedly
                                                                              Best trial: 0. Best value: inf:   8%|▊         | 2/24 [00:23<03:48, 10.41s/it]Best trial: 0. Best value: inf:   8%|▊         | 2/24 [00:23<03:48, 10.41s/it]Best trial: 0. Best value: inf:  12%|█▎        | 3/24 [00:23<02:53,  8.28s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                              Best trial: 0. Best value: inf:  12%|█▎        | 3/24 [00:24<02:53,  8.28s/it]                                                                              Best trial: 0. Best value: inf:  12%|█▎        | 3/24 [00:24<02:53,  8.28s/it]                                                                              Best trial: 0. Best value: inf:  12%|█▎        | 3/24 [00:24<02:53,  8.28s/it]                                                                              Best trial: 0. Best value: inf:  12%|█▎        | 3/24 [00:24<02:53,  8.28s/it]                                                                              Best trial: 0. Best value: inf:  12%|█▎        | 3/24 [00:24<02:53,  8.28s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 1.5 M  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
1.5 M     Trainable params
0         Non-trainable params
1.5 M     Total params
5.978     Total estimated model params size (MB)
115       Modules in train mode
0         Modules in eval mode
ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1131, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/queues.py", line 114, in get
    raise Empty
_queue.Empty

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1023, in _run_stage
    self._run_sanity_check()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1052, in _run_sanity_check
    val_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py", line 178, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 128, in run
    batch, batch_idx, dataloader_idx = next(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 133, in __next__
    batch = super().__next__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 60, in __next__
    batch = next(self.iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 341, in __next__
    out = next(self._iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 142, in __next__
    out = next(self.iterators[0])
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1327, in _next_data
    idx, data = self._get_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1293, in _get_data
    success, data = self._try_get_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1144, in _try_get_data
    raise RuntimeError(f'DataLoader worker (pid(s) {pids_str}) exited unexpectedly') from e
RuntimeError: DataLoader worker (pid(s) 204152) exited unexpectedly
                                                                              Best trial: 0. Best value: inf:  12%|█▎        | 3/24 [00:30<02:53,  8.28s/it]Best trial: 0. Best value: inf:  12%|█▎        | 3/24 [00:30<02:53,  8.28s/it]Best trial: 0. Best value: inf:  17%|█▋        | 4/24 [00:30<02:31,  7.58s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                              Best trial: 0. Best value: inf:  17%|█▋        | 4/24 [00:30<02:31,  7.58s/it]                                                                              Best trial: 0. Best value: inf:  17%|█▋        | 4/24 [00:30<02:31,  7.58s/it]                                                                              Best trial: 0. Best value: inf:  17%|█▋        | 4/24 [00:30<02:31,  7.58s/it]                                                                              Best trial: 0. Best value: inf:  17%|█▋        | 4/24 [00:30<02:31,  7.58s/it]                                                                              Best trial: 0. Best value: inf:  17%|█▋        | 4/24 [00:30<02:31,  7.58s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 694 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
694 K     Trainable params
0         Non-trainable params
694 K     Total params
2.779     Total estimated model params size (MB)
95        Modules in train mode
0         Modules in eval mode
ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1025, in _run_stage
    self.fit_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 205, in run
    self.advance()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 363, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 141, in run
    self.on_advance_end(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 295, in on_advance_end
    self.val_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py", line 178, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 142, in run
    return self.on_run_end()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 254, in on_run_end
    self._on_evaluation_epoch_end()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 333, in _on_evaluation_epoch_end
    call._call_callback_hooks(trainer, hook_name)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 218, in _call_callback_hooks
    fn(trainer, trainer.lightning_module, *args, **kwargs)
  File "/data/Pein/Pytorch/Wind-Power-Prediction/utils/callback.py", line 83, in on_validation_epoch_end
    self._update_metrics(trainer, pl_module)
  File "/data/Pein/Pytorch/Wind-Power-Prediction/utils/callback.py", line 119, in _update_metrics
    ) = full_inference(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/utils/inference.py", line 39, in full_inference
    train_rmse, train_custom_acc = invert_and_compute_metrics(train_loader)
  File "/data/Pein/Pytorch/Wind-Power-Prediction/utils/inference.py", line 17, in invert_and_compute_metrics
    for batch in loader:
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1344, in _next_data
    return self._process_data(data)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1370, in _process_data
    data.reraise()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/_utils.py", line 706, in reraise
    raise exception
RuntimeError: Caught RuntimeError in DataLoader worker process 1.
Original Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 309, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 55, in fetch
    return self.collate_fn(data)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 317, in default_collate
    return collate(batch, collate_fn_map=default_collate_fn_map)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 174, in collate
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 174, in <listcomp>
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 142, in collate
    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 212, in collate_tensor_fn
    storage = elem._typed_storage()._new_shared(numel, device=elem.device)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/storage.py", line 968, in _new_shared
    untyped_storage = torch.UntypedStorage._new_shared(size * self._element_size(), device=device)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/storage.py", line 302, in _new_shared
    return cls._new_using_fd_cpu(size)
RuntimeError: unable to write to file </torch_209931_3251203028_0>: No space left on device (28)

                                                                              Best trial: 0. Best value: inf:  17%|█▋        | 4/24 [00:43<02:31,  7.58s/it]Best trial: 0. Best value: inf:  17%|█▋        | 4/24 [00:43<02:31,  7.58s/it]Best trial: 0. Best value: inf:  21%|██        | 5/24 [00:43<03:03,  9.64s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                              Best trial: 0. Best value: inf:  21%|██        | 5/24 [01:04<03:03,  9.64s/it]                                                                              Best trial: 0. Best value: inf:  21%|██        | 5/24 [01:04<03:03,  9.64s/it]                                                                              Best trial: 0. Best value: inf:  21%|██        | 5/24 [01:04<03:03,  9.64s/it]                                                                              Best trial: 0. Best value: inf:  21%|██        | 5/24 [01:04<03:03,  9.64s/it]                                                                              Best trial: 0. Best value: inf:  21%|██        | 5/24 [01:04<03:03,  9.64s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 464 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
464 K     Trainable params
0         Non-trainable params
464 K     Total params
1.860     Total estimated model params size (MB)
75        Modules in train mode
0         Modules in eval mode
ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1023, in _run_stage
    self._run_sanity_check()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1052, in _run_sanity_check
    val_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py", line 178, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 128, in run
    batch, batch_idx, dataloader_idx = next(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 133, in __next__
    batch = super().__next__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 60, in __next__
    batch = next(self.iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 341, in __next__
    out = next(self._iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 142, in __next__
    out = next(self.iterators[0])
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1344, in _next_data
    return self._process_data(data)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1370, in _process_data
    data.reraise()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/_utils.py", line 706, in reraise
    raise exception
RuntimeError: Caught RuntimeError in DataLoader worker process 1.
Original Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 309, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 55, in fetch
    return self.collate_fn(data)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 317, in default_collate
    return collate(batch, collate_fn_map=default_collate_fn_map)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 174, in collate
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 174, in <listcomp>
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 142, in collate
    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 212, in collate_tensor_fn
    storage = elem._typed_storage()._new_shared(numel, device=elem.device)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/storage.py", line 968, in _new_shared
    untyped_storage = torch.UntypedStorage._new_shared(size * self._element_size(), device=device)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/storage.py", line 302, in _new_shared
    return cls._new_using_fd_cpu(size)
RuntimeError: unable to write to file </torch_226055_2679092338_0>: No space left on device (28)

                                                                              Best trial: 0. Best value: inf:  21%|██        | 5/24 [01:04<03:03,  9.64s/it]Best trial: 0. Best value: inf:  21%|██        | 5/24 [01:04<03:03,  9.64s/it]Best trial: 0. Best value: inf:  25%|██▌       | 6/24 [01:04<04:04, 13.57s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                              Best trial: 0. Best value: inf:  25%|██▌       | 6/24 [01:10<04:04, 13.57s/it]                                                                              Best trial: 0. Best value: inf:  25%|██▌       | 6/24 [01:10<04:04, 13.57s/it]                                                                              Best trial: 0. Best value: inf:  25%|██▌       | 6/24 [01:10<04:04, 13.57s/it]                                                                              Best trial: 0. Best value: inf:  25%|██▌       | 6/24 [01:10<04:04, 13.57s/it]                                                                              Best trial: 0. Best value: inf:  25%|██▌       | 6/24 [01:10<04:04, 13.57s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 630 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
630 K     Trainable params
0         Non-trainable params
630 K     Total params
2.521     Total estimated model params size (MB)
75        Modules in train mode
0         Modules in eval mode
ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1131, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/queues.py", line 114, in get
    raise Empty
_queue.Empty

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1023, in _run_stage
    self._run_sanity_check()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1052, in _run_sanity_check
    val_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py", line 178, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 128, in run
    batch, batch_idx, dataloader_idx = next(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 133, in __next__
    batch = super().__next__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 60, in __next__
    batch = next(self.iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 341, in __next__
    out = next(self._iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 142, in __next__
    out = next(self.iterators[0])
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1327, in _next_data
    idx, data = self._get_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1293, in _get_data
    success, data = self._try_get_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1144, in _try_get_data
    raise RuntimeError(f'DataLoader worker (pid(s) {pids_str}) exited unexpectedly') from e
RuntimeError: DataLoader worker (pid(s) 228936) exited unexpectedly
                                                                              Best trial: 0. Best value: inf:  25%|██▌       | 6/24 [01:15<04:04, 13.57s/it]Best trial: 0. Best value: inf:  25%|██▌       | 6/24 [01:15<04:04, 13.57s/it]Best trial: 0. Best value: inf:  29%|██▉       | 7/24 [01:15<03:35, 12.67s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                              Best trial: 0. Best value: inf:  29%|██▉       | 7/24 [01:16<03:35, 12.67s/it]                                                                              Best trial: 0. Best value: inf:  29%|██▉       | 7/24 [01:16<03:35, 12.67s/it]                                                                              Best trial: 0. Best value: inf:  29%|██▉       | 7/24 [01:16<03:35, 12.67s/it]                                                                              Best trial: 0. Best value: inf:  29%|██▉       | 7/24 [01:16<03:35, 12.67s/it]                                                                              Best trial: 0. Best value: inf:  29%|██▉       | 7/24 [01:16<03:35, 12.67s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 543 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
543 K     Trainable params
0         Non-trainable params
543 K     Total params
2.174     Total estimated model params size (MB)
95        Modules in train mode
0         Modules in eval mode
ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1131, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/queues.py", line 114, in get
    raise Empty
_queue.Empty

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1023, in _run_stage
    self._run_sanity_check()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1052, in _run_sanity_check
    val_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py", line 178, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 128, in run
    batch, batch_idx, dataloader_idx = next(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 133, in __next__
    batch = super().__next__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 60, in __next__
    batch = next(self.iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 341, in __next__
    out = next(self._iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 142, in __next__
    out = next(self.iterators[0])
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1327, in _next_data
    idx, data = self._get_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1293, in _get_data
    success, data = self._try_get_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1144, in _try_get_data
    raise RuntimeError(f'DataLoader worker (pid(s) {pids_str}) exited unexpectedly') from e
RuntimeError: DataLoader worker (pid(s) 232122) exited unexpectedly
                                                                              Best trial: 0. Best value: inf:  29%|██▉       | 7/24 [01:21<03:35, 12.67s/it]Best trial: 0. Best value: inf:  29%|██▉       | 7/24 [01:21<03:35, 12.67s/it]Best trial: 0. Best value: inf:  33%|███▎      | 8/24 [01:21<02:48, 10.54s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                              Best trial: 0. Best value: inf:  33%|███▎      | 8/24 [01:21<02:48, 10.54s/it]                                                                              Best trial: 0. Best value: inf:  33%|███▎      | 8/24 [01:21<02:48, 10.54s/it]                                                                              Best trial: 0. Best value: inf:  33%|███▎      | 8/24 [01:21<02:48, 10.54s/it]                                                                              Best trial: 0. Best value: inf:  33%|███▎      | 8/24 [01:21<02:48, 10.54s/it]                                                                              Best trial: 0. Best value: inf:  33%|███▎      | 8/24 [01:21<02:48, 10.54s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 1.0 M  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
1.0 M     Trainable params
0         Non-trainable params
1.0 M     Total params
4.146     Total estimated model params size (MB)
135       Modules in train mode
0         Modules in eval mode
ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1023, in _run_stage
    self._run_sanity_check()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1052, in _run_sanity_check
    val_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py", line 178, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 128, in run
    batch, batch_idx, dataloader_idx = next(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 133, in __next__
    batch = super().__next__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 60, in __next__
    batch = next(self.iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 341, in __next__
    out = next(self._iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 142, in __next__
    out = next(self.iterators[0])
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1344, in _next_data
    return self._process_data(data)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1370, in _process_data
    data.reraise()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/_utils.py", line 706, in reraise
    raise exception
RuntimeError: Caught RuntimeError in DataLoader worker process 1.
Original Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 309, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 55, in fetch
    return self.collate_fn(data)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 317, in default_collate
    return collate(batch, collate_fn_map=default_collate_fn_map)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 174, in collate
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 174, in <listcomp>
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 142, in collate
    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 212, in collate_tensor_fn
    storage = elem._typed_storage()._new_shared(numel, device=elem.device)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/storage.py", line 968, in _new_shared
    untyped_storage = torch.UntypedStorage._new_shared(size * self._element_size(), device=device)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/storage.py", line 302, in _new_shared
    return cls._new_using_fd_cpu(size)
RuntimeError: unable to write to file </torch_234302_1723001085_0>: No space left on device (28)

                                                                              Best trial: 0. Best value: inf:  33%|███▎      | 8/24 [01:27<02:48, 10.54s/it]Best trial: 0. Best value: inf:  33%|███▎      | 8/24 [01:27<02:48, 10.54s/it]Best trial: 0. Best value: inf:  38%|███▊      | 9/24 [01:27<02:15,  9.06s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                              Best trial: 0. Best value: inf:  38%|███▊      | 9/24 [01:27<02:15,  9.06s/it]                                                                              Best trial: 0. Best value: inf:  38%|███▊      | 9/24 [01:27<02:15,  9.06s/it]                                                                              Best trial: 0. Best value: inf:  38%|███▊      | 9/24 [01:27<02:15,  9.06s/it]                                                                              Best trial: 0. Best value: inf:  38%|███▊      | 9/24 [01:27<02:15,  9.06s/it]                                                                              Best trial: 0. Best value: inf:  38%|███▊      | 9/24 [01:27<02:15,  9.06s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 517 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
517 K     Trainable params
0         Non-trainable params
517 K     Total params
2.072     Total estimated model params size (MB)
95        Modules in train mode
0         Modules in eval mode
Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1023, in _run_stage
    self._run_sanity_check()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1052, in _run_sanity_check
    val_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py", line 178, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 128, in run
    batch, batch_idx, dataloader_idx = next(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 133, in __next__
    batch = super().__next__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 60, in __next__
    batch = next(self.iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 341, in __next__
    out = next(self._iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 142, in __next__
    out = next(self.iterators[0])
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1344, in _next_data
    return self._process_data(data)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1370, in _process_data
    data.reraise()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/_utils.py", line 706, in reraise
    raise exception
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 309, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 55, in fetch
    return self.collate_fn(data)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 317, in default_collate
    return collate(batch, collate_fn_map=default_collate_fn_map)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 174, in collate
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 174, in <listcomp>
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 142, in collate
    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 212, in collate_tensor_fn
    storage = elem._typed_storage()._new_shared(numel, device=elem.device)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/storage.py", line 968, in _new_shared
    untyped_storage = torch.UntypedStorage._new_shared(size * self._element_size(), device=device)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/storage.py", line 302, in _new_shared
    return cls._new_using_fd_cpu(size)
RuntimeError: unable to write to file </torch_236178_3176217442_0>: No space left on device (28)

                                                                              Best trial: 0. Best value: inf:  38%|███▊      | 9/24 [01:38<02:15,  9.06s/it]Best trial: 0. Best value: inf:  38%|███▊      | 9/24 [01:38<02:15,  9.06s/it]Best trial: 0. Best value: inf:  42%|████▏     | 10/24 [01:38<02:14,  9.59s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                               Best trial: 0. Best value: inf:  42%|████▏     | 10/24 [01:38<02:14,  9.59s/it]                                                                               Best trial: 0. Best value: inf:  42%|████▏     | 10/24 [01:38<02:14,  9.59s/it]                                                                               Best trial: 0. Best value: inf:  42%|████▏     | 10/24 [01:38<02:14,  9.59s/it]                                                                               Best trial: 0. Best value: inf:  42%|████▏     | 10/24 [01:38<02:14,  9.59s/it]                                                                               Best trial: 0. Best value: inf:  42%|████▏     | 10/24 [01:38<02:14,  9.59s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 472 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
472 K     Trainable params
0         Non-trainable params
472 K     Total params
1.889     Total estimated model params size (MB)
75        Modules in train mode
0         Modules in eval mode
ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1131, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/queues.py", line 114, in get
    raise Empty
_queue.Empty

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1023, in _run_stage
    self._run_sanity_check()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1052, in _run_sanity_check
    val_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py", line 178, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 128, in run
    batch, batch_idx, dataloader_idx = next(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 133, in __next__
    batch = super().__next__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 60, in __next__
    batch = next(self.iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 341, in __next__
    out = next(self._iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 142, in __next__
    out = next(self.iterators[0])
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1327, in _next_data
    idx, data = self._get_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1293, in _get_data
    success, data = self._try_get_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1144, in _try_get_data
    raise RuntimeError(f'DataLoader worker (pid(s) {pids_str}) exited unexpectedly') from e
RuntimeError: DataLoader worker (pid(s) 241173, 241174) exited unexpectedly
                                                                               Best trial: 0. Best value: inf:  42%|████▏     | 10/24 [01:44<02:14,  9.59s/it]Best trial: 0. Best value: inf:  42%|████▏     | 10/24 [01:44<02:14,  9.59s/it]Best trial: 0. Best value: inf:  46%|████▌     | 11/24 [01:44<01:49,  8.40s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                               Best trial: 0. Best value: inf:  46%|████▌     | 11/24 [01:44<01:49,  8.40s/it]                                                                               Best trial: 0. Best value: inf:  46%|████▌     | 11/24 [01:44<01:49,  8.40s/it]                                                                               Best trial: 0. Best value: inf:  46%|████▌     | 11/24 [01:44<01:49,  8.40s/it]                                                                               Best trial: 0. Best value: inf:  46%|████▌     | 11/24 [01:44<01:49,  8.40s/it]                                                                               Best trial: 0. Best value: inf:  46%|████▌     | 11/24 [01:44<01:49,  8.40s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 400 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
400 K     Trainable params
0         Non-trainable params
400 K     Total params
1.603     Total estimated model params size (MB)
75        Modules in train mode
0         Modules in eval mode
Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1023, in _run_stage
    self._run_sanity_check()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1052, in _run_sanity_check
    val_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py", line 178, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 128, in run
    batch, batch_idx, dataloader_idx = next(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 133, in __next__
    batch = super().__next__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 60, in __next__
    batch = next(self.iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 341, in __next__
    out = next(self._iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 142, in __next__
    out = next(self.iterators[0])
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1344, in _next_data
    return self._process_data(data)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1370, in _process_data
    data.reraise()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/_utils.py", line 706, in reraise
    raise exception
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 309, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 55, in fetch
    return self.collate_fn(data)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 317, in default_collate
    return collate(batch, collate_fn_map=default_collate_fn_map)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 174, in collate
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 174, in <listcomp>
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 142, in collate
    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 212, in collate_tensor_fn
    storage = elem._typed_storage()._new_shared(numel, device=elem.device)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/storage.py", line 968, in _new_shared
    untyped_storage = torch.UntypedStorage._new_shared(size * self._element_size(), device=device)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/storage.py", line 302, in _new_shared
    return cls._new_using_fd_cpu(size)
RuntimeError: unable to write to file </torch_242297_3152180568_0>: No space left on device (28)

                                                                               Best trial: 0. Best value: inf:  46%|████▌     | 11/24 [01:54<01:49,  8.40s/it]Best trial: 0. Best value: inf:  46%|████▌     | 11/24 [01:54<01:49,  8.40s/it]Best trial: 0. Best value: inf:  50%|█████     | 12/24 [01:54<01:49,  9.17s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                               Best trial: 0. Best value: inf:  50%|█████     | 12/24 [01:55<01:49,  9.17s/it]                                                                               Best trial: 0. Best value: inf:  50%|█████     | 12/24 [01:55<01:49,  9.17s/it]                                                                               Best trial: 0. Best value: inf:  50%|█████     | 12/24 [01:55<01:49,  9.17s/it]                                                                               Best trial: 0. Best value: inf:  50%|█████     | 12/24 [01:55<01:49,  9.17s/it]                                                                               Best trial: 0. Best value: inf:  50%|█████     | 12/24 [01:55<01:49,  9.17s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 594 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
594 K     Trainable params
0         Non-trainable params
594 K     Total params
2.379     Total estimated model params size (MB)
95        Modules in train mode
0         Modules in eval mode
ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1025, in _run_stage
    self.fit_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 205, in run
    self.advance()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 363, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 140, in run
    self.advance(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 212, in advance
    batch, _, __ = next(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 133, in __next__
    batch = super().__next__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 60, in __next__
    batch = next(self.iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 341, in __next__
    out = next(self._iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 78, in __next__
    out[i] = next(self.iterators[i])
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1327, in _next_data
    idx, data = self._get_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1293, in _get_data
    success, data = self._try_get_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1131, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/queues.py", line 122, in get
    return _ForkingPickler.loads(res)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/multiprocessing/reductions.py", line 496, in rebuild_storage_fd
    fd = df.detach()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/resource_sharer.py", line 86, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/connection.py", line 508, in Client
    answer_challenge(c, authkey)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/connection.py", line 752, in answer_challenge
    message = connection.recv_bytes(256)         # reject large message
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
ConnectionResetError: [Errno 104] Connection reset by peer
                                                                               Best trial: 0. Best value: inf:  50%|█████     | 12/24 [01:56<01:49,  9.17s/it]Best trial: 0. Best value: inf:  50%|█████     | 12/24 [01:56<01:49,  9.17s/it]Best trial: 0. Best value: inf:  54%|█████▍    | 13/24 [01:56<01:15,  6.86s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                               Best trial: 0. Best value: inf:  54%|█████▍    | 13/24 [01:56<01:15,  6.86s/it]                                                                               Best trial: 0. Best value: inf:  54%|█████▍    | 13/24 [01:56<01:15,  6.86s/it]                                                                               Best trial: 0. Best value: inf:  54%|█████▍    | 13/24 [01:56<01:15,  6.86s/it]                                                                               Best trial: 0. Best value: inf:  54%|█████▍    | 13/24 [01:56<01:15,  6.86s/it]                                                                               Best trial: 0. Best value: inf:  54%|█████▍    | 13/24 [01:56<01:15,  6.86s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 559 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
559 K     Trainable params
0         Non-trainable params
559 K     Total params
2.239     Total estimated model params size (MB)
75        Modules in train mode
0         Modules in eval mode
ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1131, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/queues.py", line 122, in get
    return _ForkingPickler.loads(res)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/multiprocessing/reductions.py", line 496, in rebuild_storage_fd
    fd = df.detach()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/resource_sharer.py", line 86, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/connection.py", line 508, in Client
    answer_challenge(c, authkey)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/connection.py", line 752, in answer_challenge
    message = connection.recv_bytes(256)         # reject large message
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1025, in _run_stage
    self.fit_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 205, in run
    self.advance()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 363, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 140, in run
    self.advance(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 212, in advance
    batch, _, __ = next(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 133, in __next__
    batch = super().__next__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 60, in __next__
    batch = next(self.iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 341, in __next__
    out = next(self._iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 78, in __next__
    out[i] = next(self.iterators[i])
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1327, in _next_data
    idx, data = self._get_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1293, in _get_data
    success, data = self._try_get_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1144, in _try_get_data
    raise RuntimeError(f'DataLoader worker (pid(s) {pids_str}) exited unexpectedly') from e
RuntimeError: DataLoader worker (pid(s) 246910) exited unexpectedly
                                                                               Best trial: 0. Best value: inf:  54%|█████▍    | 13/24 [01:57<01:15,  6.86s/it]Best trial: 0. Best value: inf:  54%|█████▍    | 13/24 [01:57<01:15,  6.86s/it]Best trial: 0. Best value: inf:  58%|█████▊    | 14/24 [01:57<00:52,  5.23s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                               Best trial: 0. Best value: inf:  58%|█████▊    | 14/24 [01:58<00:52,  5.23s/it]                                                                               Best trial: 0. Best value: inf:  58%|█████▊    | 14/24 [01:58<00:52,  5.23s/it]                                                                               Best trial: 0. Best value: inf:  58%|█████▊    | 14/24 [01:58<00:52,  5.23s/it]                                                                               Best trial: 0. Best value: inf:  58%|█████▊    | 14/24 [01:58<00:52,  5.23s/it]                                                                               Best trial: 0. Best value: inf:  58%|█████▊    | 14/24 [01:58<00:52,  5.23s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 765 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
765 K     Trainable params
0         Non-trainable params
765 K     Total params
3.062     Total estimated model params size (MB)
75        Modules in train mode
0         Modules in eval mode
ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1025, in _run_stage
    self.fit_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 205, in run
    self.advance()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 363, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 141, in run
    self.on_advance_end(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 295, in on_advance_end
    self.val_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py", line 178, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 142, in run
    return self.on_run_end()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 254, in on_run_end
    self._on_evaluation_epoch_end()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 333, in _on_evaluation_epoch_end
    call._call_callback_hooks(trainer, hook_name)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 218, in _call_callback_hooks
    fn(trainer, trainer.lightning_module, *args, **kwargs)
  File "/data/Pein/Pytorch/Wind-Power-Prediction/utils/callback.py", line 83, in on_validation_epoch_end
    self._update_metrics(trainer, pl_module)
  File "/data/Pein/Pytorch/Wind-Power-Prediction/utils/callback.py", line 119, in _update_metrics
    ) = full_inference(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/utils/inference.py", line 39, in full_inference
    train_rmse, train_custom_acc = invert_and_compute_metrics(train_loader)
  File "/data/Pein/Pytorch/Wind-Power-Prediction/utils/inference.py", line 17, in invert_and_compute_metrics
    for batch in loader:
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1344, in _next_data
    return self._process_data(data)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1370, in _process_data
    data.reraise()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/_utils.py", line 706, in reraise
    raise exception
RuntimeError: Caught RuntimeError in DataLoader worker process 3.
Original Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 309, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 55, in fetch
    return self.collate_fn(data)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 317, in default_collate
    return collate(batch, collate_fn_map=default_collate_fn_map)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 174, in collate
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 174, in <listcomp>
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 142, in collate
    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 212, in collate_tensor_fn
    storage = elem._typed_storage()._new_shared(numel, device=elem.device)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/storage.py", line 968, in _new_shared
    untyped_storage = torch.UntypedStorage._new_shared(size * self._element_size(), device=device)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/storage.py", line 302, in _new_shared
    return cls._new_using_fd_cpu(size)
RuntimeError: unable to write to file </torch_248564_3838944033_0>: No space left on device (28)

                                                                               Best trial: 0. Best value: inf:  58%|█████▊    | 14/24 [02:14<00:52,  5.23s/it]Best trial: 0. Best value: inf:  58%|█████▊    | 14/24 [02:14<00:52,  5.23s/it]Best trial: 0. Best value: inf:  62%|██████▎   | 15/24 [02:14<01:18,  8.76s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                               Best trial: 0. Best value: inf:  62%|██████▎   | 15/24 [02:35<01:18,  8.76s/it]                                                                               Best trial: 0. Best value: inf:  62%|██████▎   | 15/24 [02:35<01:18,  8.76s/it]                                                                               Best trial: 0. Best value: inf:  62%|██████▎   | 15/24 [02:35<01:18,  8.76s/it]                                                                               Best trial: 0. Best value: inf:  62%|██████▎   | 15/24 [02:35<01:18,  8.76s/it]                                                                               Best trial: 0. Best value: inf:  62%|██████▎   | 15/24 [02:35<01:18,  8.76s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 483 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
483 K     Trainable params
0         Non-trainable params
483 K     Total params
1.935     Total estimated model params size (MB)
55        Modules in train mode
0         Modules in eval mode
ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1025, in _run_stage
    self.fit_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 205, in run
    self.advance()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 363, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 140, in run
    self.advance(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 212, in advance
    batch, _, __ = next(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 133, in __next__
    batch = super().__next__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 60, in __next__
    batch = next(self.iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 341, in __next__
    out = next(self._iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 78, in __next__
    out[i] = next(self.iterators[i])
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1344, in _next_data
    return self._process_data(data)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1370, in _process_data
    data.reraise()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/_utils.py", line 706, in reraise
    raise exception
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 309, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 55, in fetch
    return self.collate_fn(data)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 317, in default_collate
    return collate(batch, collate_fn_map=default_collate_fn_map)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 174, in collate
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 174, in <listcomp>
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 142, in collate
    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 212, in collate_tensor_fn
    storage = elem._typed_storage()._new_shared(numel, device=elem.device)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/storage.py", line 968, in _new_shared
    untyped_storage = torch.UntypedStorage._new_shared(size * self._element_size(), device=device)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/storage.py", line 302, in _new_shared
    return cls._new_using_fd_cpu(size)
RuntimeError: unable to write to file </torch_268520_1380073249_0>: No space left on device (28)

ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
                                                                                Best trial: 0. Best value: inf:  62%|██████▎   | 15/24 [02:36<01:18,  8.76s/it]Best trial: 0. Best value: inf:  62%|██████▎   | 15/24 [02:36<01:18,  8.76s/it]Best trial: 0. Best value: inf:  67%|██████▋   | 16/24 [02:36<01:40, 12.59s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                               Best trial: 0. Best value: inf:  67%|██████▋   | 16/24 [02:36<01:40, 12.59s/it]                                                                               Best trial: 0. Best value: inf:  67%|██████▋   | 16/24 [02:36<01:40, 12.59s/it]                                                                               Best trial: 0. Best value: inf:  67%|██████▋   | 16/24 [02:36<01:40, 12.59s/it]                                                                               Best trial: 0. Best value: inf:  67%|██████▋   | 16/24 [02:36<01:40, 12.59s/it]                                                                               Best trial: 0. Best value: inf:  67%|██████▋   | 16/24 [02:36<01:40, 12.59s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 874 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
874 K     Trainable params
0         Non-trainable params
874 K     Total params
3.497     Total estimated model params size (MB)
75        Modules in train mode
0         Modules in eval mode
ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1025, in _run_stage
    self.fit_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 205, in run
    self.advance()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 363, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 140, in run
    self.advance(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 212, in advance
    batch, _, __ = next(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 133, in __next__
    batch = super().__next__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 60, in __next__
    batch = next(self.iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 341, in __next__
    out = next(self._iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 78, in __next__
    out[i] = next(self.iterators[i])
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1344, in _next_data
    return self._process_data(data)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1370, in _process_data
    data.reraise()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/_utils.py", line 706, in reraise
    raise exception
RuntimeError: Caught RuntimeError in DataLoader worker process 2.
Original Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 309, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 55, in fetch
    return self.collate_fn(data)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 317, in default_collate
    return collate(batch, collate_fn_map=default_collate_fn_map)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 174, in collate
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 174, in <listcomp>
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 142, in collate
    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 212, in collate_tensor_fn
    storage = elem._typed_storage()._new_shared(numel, device=elem.device)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/storage.py", line 968, in _new_shared
    untyped_storage = torch.UntypedStorage._new_shared(size * self._element_size(), device=device)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/storage.py", line 302, in _new_shared
    return cls._new_using_fd_cpu(size)
RuntimeError: unable to write to file </torch_269802_126056035_0>: No space left on device (28)

                                                                               Best trial: 0. Best value: inf:  67%|██████▋   | 16/24 [02:37<01:40, 12.59s/it]Best trial: 0. Best value: inf:  67%|██████▋   | 16/24 [02:37<01:40, 12.59s/it]Best trial: 0. Best value: inf:  71%|███████   | 17/24 [02:37<01:04,  9.16s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                               Best trial: 0. Best value: inf:  71%|███████   | 17/24 [02:37<01:04,  9.16s/it]                                                                               Best trial: 0. Best value: inf:  71%|███████   | 17/24 [02:37<01:04,  9.16s/it]                                                                               Best trial: 0. Best value: inf:  71%|███████   | 17/24 [02:37<01:04,  9.16s/it]                                                                               Best trial: 0. Best value: inf:  71%|███████   | 17/24 [02:37<01:04,  9.16s/it]                                                                               Best trial: 0. Best value: inf:  71%|███████   | 17/24 [02:37<01:04,  9.16s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 795 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
795 K     Trainable params
0         Non-trainable params
795 K     Total params
3.182     Total estimated model params size (MB)
95        Modules in train mode
0         Modules in eval mode
ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1131, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/queues.py", line 114, in get
    raise Empty
_queue.Empty

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1025, in _run_stage
    self.fit_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 205, in run
    self.advance()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 363, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 140, in run
    self.advance(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 212, in advance
    batch, _, __ = next(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 133, in __next__
    batch = super().__next__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 60, in __next__
    batch = next(self.iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 341, in __next__
    out = next(self._iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 78, in __next__
    out[i] = next(self.iterators[i])
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1327, in _next_data
    idx, data = self._get_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1293, in _get_data
    success, data = self._try_get_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1144, in _try_get_data
    raise RuntimeError(f'DataLoader worker (pid(s) {pids_str}) exited unexpectedly') from e
RuntimeError: DataLoader worker (pid(s) 270224) exited unexpectedly
                                                                               Best trial: 0. Best value: inf:  71%|███████   | 17/24 [02:43<01:04,  9.16s/it]Best trial: 0. Best value: inf:  71%|███████   | 17/24 [02:43<01:04,  9.16s/it]Best trial: 0. Best value: inf:  75%|███████▌  | 18/24 [02:43<00:49,  8.31s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                               Best trial: 0. Best value: inf:  75%|███████▌  | 18/24 [02:44<00:49,  8.31s/it]                                                                               Best trial: 0. Best value: inf:  75%|███████▌  | 18/24 [02:44<00:49,  8.31s/it]                                                                               Best trial: 0. Best value: inf:  75%|███████▌  | 18/24 [02:44<00:49,  8.31s/it]                                                                               Best trial: 0. Best value: inf:  75%|███████▌  | 18/24 [02:44<00:49,  8.31s/it]                                                                               Best trial: 0. Best value: inf:  75%|███████▌  | 18/24 [02:44<00:49,  8.31s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 647 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
647 K     Trainable params
0         Non-trainable params
647 K     Total params
2.591     Total estimated model params size (MB)
55        Modules in train mode
0         Modules in eval mode
Metric Loss/val improved. New best score: 1.205
Metric Loss/val improved by 0.133 >= min_delta = 0.0. New best score: 1.073
Metric Loss/val improved by 0.077 >= min_delta = 0.0. New best score: 0.996
Monitored metric Loss/val did not improve in the last 15 records. Best score: 0.996. Signaling Trainer to stop.
                                                                               Best trial: 0. Best value: inf:  75%|███████▌  | 18/24 [03:45<00:49,  8.31s/it]Best trial: 519. Best value: 1.05948:  75%|███████▌  | 18/24 [03:45<00:49,  8.31s/it]Best trial: 519. Best value: 1.05948:  79%|███████▉  | 19/24 [03:45<02:02, 24.40s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                                     Best trial: 519. Best value: 1.05948:  79%|███████▉  | 19/24 [03:46<02:02, 24.40s/it]                                                                                     Best trial: 519. Best value: 1.05948:  79%|███████▉  | 19/24 [03:46<02:02, 24.40s/it]                                                                                     Best trial: 519. Best value: 1.05948:  79%|███████▉  | 19/24 [03:46<02:02, 24.40s/it]                                                                                     Best trial: 519. Best value: 1.05948:  79%|███████▉  | 19/24 [03:46<02:02, 24.40s/it]                                                                                     Best trial: 519. Best value: 1.05948:  79%|███████▉  | 19/24 [03:46<02:02, 24.40s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 813 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
813 K     Trainable params
0         Non-trainable params
813 K     Total params
3.252     Total estimated model params size (MB)
95        Modules in train mode
0         Modules in eval mode
Metric Loss/val improved. New best score: 1.192
Metric Loss/val improved by 0.016 >= min_delta = 0.0. New best score: 1.176
Metric Loss/val improved by 0.094 >= min_delta = 0.0. New best score: 1.083
Metric Loss/val improved by 0.020 >= min_delta = 0.0. New best score: 1.063
Metric Loss/val improved by 0.026 >= min_delta = 0.0. New best score: 1.037
Metric Loss/val improved by 0.080 >= min_delta = 0.0. New best score: 0.957
Monitored metric Loss/val did not improve in the last 15 records. Best score: 0.957. Signaling Trainer to stop.
                                                                                     Best trial: 519. Best value: 1.05948:  79%|███████▉  | 19/24 [05:48<02:02, 24.40s/it]Best trial: 520. Best value: 0.924176:  79%|███████▉  | 19/24 [05:48<02:02, 24.40s/it]Best trial: 520. Best value: 0.924176:  83%|████████▎ | 20/24 [05:48<03:35, 53.84s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                                      Best trial: 520. Best value: 0.924176:  83%|████████▎ | 20/24 [05:48<03:35, 53.84s/it]                                                                                      Best trial: 520. Best value: 0.924176:  83%|████████▎ | 20/24 [05:48<03:35, 53.84s/it]                                                                                      Best trial: 520. Best value: 0.924176:  83%|████████▎ | 20/24 [05:48<03:35, 53.84s/it]                                                                                      Best trial: 520. Best value: 0.924176:  83%|████████▎ | 20/24 [05:48<03:35, 53.84s/it]                                                                                      Best trial: 520. Best value: 0.924176:  83%|████████▎ | 20/24 [05:48<03:35, 53.84s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 620 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
620 K     Trainable params
0         Non-trainable params
620 K     Total params
2.481     Total estimated model params size (MB)
55        Modules in train mode
0         Modules in eval mode
Metric Loss/val improved. New best score: 1.129
Metric Loss/val improved by 0.202 >= min_delta = 0.0. New best score: 0.927
Monitored metric Loss/val did not improve in the last 15 records. Best score: 0.927. Signaling Trainer to stop.
                                                                                      Best trial: 520. Best value: 0.924176:  83%|████████▎ | 20/24 [06:43<03:35, 53.84s/it]Best trial: 520. Best value: 0.924176:  83%|████████▎ | 20/24 [06:43<03:35, 53.84s/it]Best trial: 520. Best value: 0.924176:  88%|████████▊ | 21/24 [06:43<02:42, 54.15s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                                      Best trial: 520. Best value: 0.924176:  88%|████████▊ | 21/24 [06:43<02:42, 54.15s/it]                                                                                      Best trial: 520. Best value: 0.924176:  88%|████████▊ | 21/24 [06:43<02:42, 54.15s/it]                                                                                      Best trial: 520. Best value: 0.924176:  88%|████████▊ | 21/24 [06:43<02:42, 54.15s/it]                                                                                      Best trial: 520. Best value: 0.924176:  88%|████████▊ | 21/24 [06:43<02:42, 54.15s/it]                                                                                      Best trial: 520. Best value: 0.924176:  88%|████████▊ | 21/24 [06:43<02:42, 54.15s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 685 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
685 K     Trainable params
0         Non-trainable params
685 K     Total params
2.744     Total estimated model params size (MB)
75        Modules in train mode
0         Modules in eval mode
Metric Loss/val improved. New best score: 1.180
Metric Loss/val improved by 0.075 >= min_delta = 0.0. New best score: 1.106
Metric Loss/val improved by 0.113 >= min_delta = 0.0. New best score: 0.992
Metric Loss/val improved by 0.109 >= min_delta = 0.0. New best score: 0.884
Monitored metric Loss/val did not improve in the last 15 records. Best score: 0.884. Signaling Trainer to stop.
                                                                                      Best trial: 520. Best value: 0.924176:  88%|████████▊ | 21/24 [08:38<02:42, 54.15s/it]Best trial: 520. Best value: 0.924176:  88%|████████▊ | 21/24 [08:38<02:42, 54.15s/it]Best trial: 520. Best value: 0.924176:  92%|█████████▏| 22/24 [08:38<02:25, 72.53s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                                      Best trial: 520. Best value: 0.924176:  92%|█████████▏| 22/24 [08:38<02:25, 72.53s/it]                                                                                      Best trial: 520. Best value: 0.924176:  92%|█████████▏| 22/24 [08:38<02:25, 72.53s/it]                                                                                      Best trial: 520. Best value: 0.924176:  92%|█████████▏| 22/24 [08:38<02:25, 72.53s/it]                                                                                      Best trial: 520. Best value: 0.924176:  92%|█████████▏| 22/24 [08:38<02:25, 72.53s/it]                                                                                      Best trial: 520. Best value: 0.924176:  92%|█████████▏| 22/24 [08:38<02:25, 72.53s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 610 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
610 K     Trainable params
0         Non-trainable params
610 K     Total params
2.443     Total estimated model params size (MB)
55        Modules in train mode
0         Modules in eval mode
Metric Loss/val improved. New best score: 1.220
Metric Loss/val improved by 0.168 >= min_delta = 0.0. New best score: 1.052
Metric Loss/val improved by 0.021 >= min_delta = 0.0. New best score: 1.031
Metric Loss/val improved by 0.072 >= min_delta = 0.0. New best score: 0.958
Metric Loss/val improved by 0.086 >= min_delta = 0.0. New best score: 0.873
Monitored metric Loss/val did not improve in the last 15 records. Best score: 0.873. Signaling Trainer to stop.
                                                                                      Best trial: 520. Best value: 0.924176:  92%|█████████▏| 22/24 [10:34<02:25, 72.53s/it]Best trial: 520. Best value: 0.924176:  92%|█████████▏| 22/24 [10:34<02:25, 72.53s/it]Best trial: 520. Best value: 0.924176:  96%|█████████▌| 23/24 [10:34<01:25, 85.65s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                                      Best trial: 520. Best value: 0.924176:  96%|█████████▌| 23/24 [10:34<01:25, 85.65s/it]                                                                                      Best trial: 520. Best value: 0.924176:  96%|█████████▌| 23/24 [10:34<01:25, 85.65s/it]                                                                                      Best trial: 520. Best value: 0.924176:  96%|█████████▌| 23/24 [10:34<01:25, 85.65s/it]                                                                                      Best trial: 520. Best value: 0.924176:  96%|█████████▌| 23/24 [10:34<01:25, 85.65s/it]                                                                                      Best trial: 520. Best value: 0.924176:  96%|█████████▌| 23/24 [10:34<01:25, 85.65s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 733 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
733 K     Trainable params
0         Non-trainable params
733 K     Total params
2.935     Total estimated model params size (MB)
75        Modules in train mode
0         Modules in eval mode
Metric Loss/val improved. New best score: 1.171
Metric Loss/val improved by 0.126 >= min_delta = 0.0. New best score: 1.045
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.045. Signaling Trainer to stop.
                                                                                      Best trial: 520. Best value: 0.924176:  96%|█████████▌| 23/24 [11:33<01:25, 85.65s/it]Best trial: 520. Best value: 0.924176:  96%|█████████▌| 23/24 [11:33<01:25, 85.65s/it]Best trial: 520. Best value: 0.924176: 100%|██████████| 24/24 [11:33<00:00, 77.49s/it]Best trial: 520. Best value: 0.924176: 100%|██████████| 24/24 [11:33<00:00, 28.88s/it]
[W 2024-08-19 05:20:17,888] The parameter 'use_pos_enc' in trial#146 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:17,893] The parameter 'norm_type' in trial#146 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:17,923] The parameter 'feat_conv_kernel' in trial#146 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:17,930] The parameter 'skip_connection_mode' in trial#146 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:17,941] The parameter 'mlp_norm' in trial#146 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=80, n_heads=4, e_layers=5, hidden_d_model=44, seq_layers=2, last_d_model=480, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=20, pos_d_model=16, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=False, bidirectional=False, norm_type='layer', num_heads=4, fc_layer_type='mlp', conv_out_dim=192, feat_conv_kernel=11, skip_connection_mode='conv_mlp', conv_norm=True, mlp_norm=False, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-conv_mlp-conv-True-mlp-False-use_pos-False-seq_len-8-lr-0.0033-d-80-hid_d-44-last_d-480-tok_d-4-time_d-20-pos_d-16-e_layers-5-tok_conv_k-9-conv_out_d-192-feat_conv_k-11-dropout-0.14-norm_type-layer-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.003343109119674029, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:20:19,255] Trial 146 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 44, 'token_conv_kernel': 9, 'last_d_model': 480, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 20, 'pos_d_model': 16, 'd_model': 80, 'conv_out_dim': 192, 'e_layers': 5, 'learning_rate': 0.003343109119674029, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': False, 'norm_type': 'layer', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 11, 'skip_connection_mode': 'conv_mlp', 'conv_norm': True, 'mlp_norm': False, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:20:29,684] The parameter 'use_pos_enc' in trial#175 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:29,690] The parameter 'norm_type' in trial#175 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:29,722] The parameter 'feat_conv_kernel' in trial#175 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:29,738] The parameter 'skip_connection_mode' in trial#175 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:29,758] The parameter 'mlp_norm' in trial#175 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=144, n_heads=4, e_layers=6, hidden_d_model=40, seq_layers=2, last_d_model=352, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=28, pos_d_model=24, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=False, bidirectional=False, norm_type='layer', num_heads=4, fc_layer_type='mlp', conv_out_dim=128, feat_conv_kernel=9, skip_connection_mode='conv_mlp', conv_norm=True, mlp_norm=True, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-conv_mlp-conv-True-mlp-True-use_pos-False-seq_len-8-lr-0.0052-d-144-hid_d-40-last_d-352-tok_d-4-time_d-28-pos_d-24-e_layers-6-tok_conv_k-9-conv_out_d-128-feat_conv_k-9-dropout-0.16-norm_type-layer-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.005192853462400568, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:20:35,782] Trial 175 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 40, 'token_conv_kernel': 9, 'last_d_model': 352, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 28, 'pos_d_model': 24, 'd_model': 144, 'conv_out_dim': 128, 'e_layers': 6, 'learning_rate': 0.005192853462400568, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': False, 'norm_type': 'layer', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 9, 'skip_connection_mode': 'conv_mlp', 'conv_norm': True, 'mlp_norm': True, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:20:35,977] The parameter 'use_pos_enc' in trial#199 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:35,982] The parameter 'norm_type' in trial#199 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:35,995] The parameter 'feat_conv_kernel' in trial#199 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:36,000] The parameter 'skip_connection_mode' in trial#199 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:36,010] The parameter 'mlp_norm' in trial#199 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=80, n_heads=4, e_layers=5, hidden_d_model=32, seq_layers=2, last_d_model=352, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=28, pos_d_model=24, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=4, fc_layer_type='mlp', conv_out_dim=128, feat_conv_kernel=11, skip_connection_mode='full', conv_norm=True, mlp_norm=False, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-full-conv-True-mlp-False-use_pos-True-seq_len-8-lr-0.0044-d-80-hid_d-32-last_d-352-tok_d-4-time_d-28-pos_d-24-e_layers-5-tok_conv_k-9-conv_out_d-128-feat_conv_k-11-dropout-0.16-norm_type-batch-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.0043846968800842516, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:20:41,502] Trial 199 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 32, 'token_conv_kernel': 9, 'last_d_model': 352, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 28, 'pos_d_model': 24, 'd_model': 80, 'conv_out_dim': 128, 'e_layers': 5, 'learning_rate': 0.0043846968800842516, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 11, 'skip_connection_mode': 'full', 'conv_norm': True, 'mlp_norm': False, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:20:41,949] The parameter 'use_pos_enc' in trial#217 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:41,961] The parameter 'norm_type' in trial#217 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:41,997] The parameter 'feat_conv_kernel' in trial#217 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:42,003] The parameter 'skip_connection_mode' in trial#217 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:42,012] The parameter 'mlp_norm' in trial#217 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=208, n_heads=4, e_layers=5, hidden_d_model=40, seq_layers=2, last_d_model=416, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=8, time_d_model=32, pos_d_model=16, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=False, bidirectional=False, norm_type='batch', num_heads=4, fc_layer_type='mlp', conv_out_dim=192, feat_conv_kernel=9, skip_connection_mode='full', conv_norm=True, mlp_norm=True, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-full-conv-True-mlp-True-use_pos-False-seq_len-8-lr-0.0085-d-208-hid_d-40-last_d-416-tok_d-8-time_d-32-pos_d-16-e_layers-5-tok_conv_k-9-conv_out_d-192-feat_conv_k-9-dropout-0.16-norm_type-batch-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.008454974417104666, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:20:48,036] Trial 217 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 40, 'token_conv_kernel': 9, 'last_d_model': 416, 'seq_len': 8, 'token_d_model': 8, 'time_d_model': 32, 'pos_d_model': 16, 'd_model': 208, 'conv_out_dim': 192, 'e_layers': 5, 'learning_rate': 0.008454974417104666, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': False, 'norm_type': 'batch', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 9, 'skip_connection_mode': 'full', 'conv_norm': True, 'mlp_norm': True, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:20:48,318] The parameter 'use_pos_enc' in trial#246 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:48,323] The parameter 'norm_type' in trial#246 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:48,337] The parameter 'feat_conv_kernel' in trial#246 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:48,342] The parameter 'skip_connection_mode' in trial#246 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:48,351] The parameter 'mlp_norm' in trial#246 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=112, n_heads=4, e_layers=4, hidden_d_model=40, seq_layers=2, last_d_model=480, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=32, pos_d_model=20, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=4, fc_layer_type='mlp', conv_out_dim=192, feat_conv_kernel=9, skip_connection_mode='full', conv_norm=True, mlp_norm=True, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-full-conv-True-mlp-True-use_pos-True-seq_len-8-lr-0.0037-d-112-hid_d-40-last_d-480-tok_d-4-time_d-32-pos_d-20-e_layers-4-tok_conv_k-9-conv_out_d-192-feat_conv_k-9-dropout-0.16-norm_type-batch-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.003689334582135482, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:21:01,321] Trial 246 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 40, 'token_conv_kernel': 9, 'last_d_model': 480, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 32, 'pos_d_model': 20, 'd_model': 112, 'conv_out_dim': 192, 'e_layers': 4, 'learning_rate': 0.003689334582135482, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 9, 'skip_connection_mode': 'full', 'conv_norm': True, 'mlp_norm': True, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:21:21,891] The parameter 'use_pos_enc' in trial#375 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:21,896] The parameter 'norm_type' in trial#375 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:21,908] The parameter 'feat_conv_kernel' in trial#375 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:21,912] The parameter 'skip_connection_mode' in trial#375 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:21,920] The parameter 'mlp_norm' in trial#375 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=144, n_heads=4, e_layers=3, hidden_d_model=24, seq_layers=2, last_d_model=160, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=24, pos_d_model=12, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=4, fc_layer_type='mlp', conv_out_dim=128, feat_conv_kernel=11, skip_connection_mode='full', conv_norm=True, mlp_norm=True, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-full-conv-True-mlp-True-use_pos-True-seq_len-8-lr-0.0096-d-144-hid_d-24-last_d-160-tok_d-4-time_d-24-pos_d-12-e_layers-3-tok_conv_k-9-conv_out_d-128-feat_conv_k-11-dropout-0.14-norm_type-batch-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.009598090625645905, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:21:22,531] Trial 375 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 24, 'token_conv_kernel': 9, 'last_d_model': 160, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 24, 'pos_d_model': 12, 'd_model': 144, 'conv_out_dim': 128, 'e_layers': 3, 'learning_rate': 0.009598090625645905, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 11, 'skip_connection_mode': 'full', 'conv_norm': True, 'mlp_norm': True, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:21:27,800] The parameter 'use_pos_enc' in trial#396 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:27,804] The parameter 'norm_type' in trial#396 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:27,817] The parameter 'feat_conv_kernel' in trial#396 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:27,820] The parameter 'skip_connection_mode' in trial#396 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:27,828] The parameter 'mlp_norm' in trial#396 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=144, n_heads=4, e_layers=3, hidden_d_model=28, seq_layers=2, last_d_model=224, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=20, pos_d_model=20, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.12, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=4, fc_layer_type='mlp', conv_out_dim=192, feat_conv_kernel=11, skip_connection_mode='conv_mlp', conv_norm=True, mlp_norm=True, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-conv_mlp-conv-True-mlp-True-use_pos-True-seq_len-8-lr-0.0207-d-144-hid_d-28-last_d-224-tok_d-4-time_d-20-pos_d-20-e_layers-3-tok_conv_k-9-conv_out_d-192-feat_conv_k-11-dropout-0.12-norm_type-layer-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.020747899633100825, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:21:33,349] Trial 396 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 28, 'token_conv_kernel': 9, 'last_d_model': 224, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 20, 'pos_d_model': 20, 'd_model': 144, 'conv_out_dim': 192, 'e_layers': 3, 'learning_rate': 0.020747899633100825, 'dropout': 0.12000000000000001, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 11, 'skip_connection_mode': 'conv_mlp', 'conv_norm': True, 'mlp_norm': True, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:21:33,756] The parameter 'use_pos_enc' in trial#425 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:33,762] The parameter 'norm_type' in trial#425 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:33,779] The parameter 'feat_conv_kernel' in trial#425 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:33,784] The parameter 'skip_connection_mode' in trial#425 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:33,795] The parameter 'mlp_norm' in trial#425 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=112, n_heads=4, e_layers=4, hidden_d_model=40, seq_layers=2, last_d_model=288, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=12, pos_d_model=24, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.12, use_pos_enc=False, bidirectional=False, norm_type='layer', num_heads=4, fc_layer_type='mlp', conv_out_dim=128, feat_conv_kernel=9, skip_connection_mode='full', conv_norm=True, mlp_norm=False, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-full-conv-True-mlp-False-use_pos-False-seq_len-8-lr-0.0142-d-112-hid_d-40-last_d-288-tok_d-4-time_d-12-pos_d-24-e_layers-4-tok_conv_k-9-conv_out_d-128-feat_conv_k-9-dropout-0.12-norm_type-layer-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.01418882815506659, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:21:39,345] Trial 425 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 40, 'token_conv_kernel': 9, 'last_d_model': 288, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 12, 'pos_d_model': 24, 'd_model': 112, 'conv_out_dim': 128, 'e_layers': 4, 'learning_rate': 0.01418882815506659, 'dropout': 0.12000000000000001, 'combine_type': 'add', 'use_pos_enc': False, 'norm_type': 'layer', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 9, 'skip_connection_mode': 'full', 'conv_norm': True, 'mlp_norm': False, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:21:39,540] The parameter 'use_pos_enc' in trial#444 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:39,545] The parameter 'norm_type' in trial#444 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:39,561] The parameter 'feat_conv_kernel' in trial#444 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:39,566] The parameter 'skip_connection_mode' in trial#444 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:39,577] The parameter 'mlp_norm' in trial#444 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=176, n_heads=4, e_layers=6, hidden_d_model=36, seq_layers=2, last_d_model=352, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=28, pos_d_model=20, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.12, use_pos_enc=False, bidirectional=False, norm_type='layer', num_heads=4, fc_layer_type='mlp', conv_out_dim=192, feat_conv_kernel=9, skip_connection_mode='full', conv_norm=True, mlp_norm=True, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-full-conv-True-mlp-True-use_pos-False-seq_len-8-lr-0.0147-d-176-hid_d-36-last_d-352-tok_d-4-time_d-28-pos_d-20-e_layers-6-tok_conv_k-9-conv_out_d-192-feat_conv_k-9-dropout-0.12-norm_type-layer-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.014732522542224414, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:21:45,152] Trial 444 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 36, 'token_conv_kernel': 9, 'last_d_model': 352, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 28, 'pos_d_model': 20, 'd_model': 176, 'conv_out_dim': 192, 'e_layers': 6, 'learning_rate': 0.014732522542224414, 'dropout': 0.12000000000000001, 'combine_type': 'add', 'use_pos_enc': False, 'norm_type': 'layer', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 9, 'skip_connection_mode': 'full', 'conv_norm': True, 'mlp_norm': True, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:21:45,333] The parameter 'use_pos_enc' in trial#458 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:45,337] The parameter 'norm_type' in trial#458 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:45,372] The parameter 'feat_conv_kernel' in trial#458 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:45,386] The parameter 'skip_connection_mode' in trial#458 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:45,398] The parameter 'mlp_norm' in trial#458 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=112, n_heads=4, e_layers=4, hidden_d_model=36, seq_layers=2, last_d_model=288, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=20, pos_d_model=20, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=4, fc_layer_type='mlp', conv_out_dim=128, feat_conv_kernel=9, skip_connection_mode='conv_mlp', conv_norm=True, mlp_norm=True, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-conv_mlp-conv-True-mlp-True-use_pos-True-seq_len-8-lr-0.0093-d-112-hid_d-36-last_d-288-tok_d-4-time_d-20-pos_d-20-e_layers-4-tok_conv_k-9-conv_out_d-128-feat_conv_k-9-dropout-0.16-norm_type-layer-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.009297068379874112, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:21:55,928] Trial 458 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 36, 'token_conv_kernel': 9, 'last_d_model': 288, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 20, 'pos_d_model': 20, 'd_model': 112, 'conv_out_dim': 128, 'e_layers': 4, 'learning_rate': 0.009297068379874112, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 9, 'skip_connection_mode': 'conv_mlp', 'conv_norm': True, 'mlp_norm': True, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:21:56,140] The parameter 'use_pos_enc' in trial#481 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:56,146] The parameter 'norm_type' in trial#481 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:56,161] The parameter 'feat_conv_kernel' in trial#481 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:56,165] The parameter 'skip_connection_mode' in trial#481 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:56,175] The parameter 'mlp_norm' in trial#481 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=176, n_heads=4, e_layers=3, hidden_d_model=36, seq_layers=2, last_d_model=224, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=32, pos_d_model=24, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=4, fc_layer_type='mlp', conv_out_dim=64, feat_conv_kernel=11, skip_connection_mode='conv_mlp', conv_norm=True, mlp_norm=True, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-conv_mlp-conv-True-mlp-True-use_pos-True-seq_len-8-lr-0.0055-d-176-hid_d-36-last_d-224-tok_d-4-time_d-32-pos_d-24-e_layers-3-tok_conv_k-9-conv_out_d-64-feat_conv_k-11-dropout-0.14-norm_type-layer-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.005505885336706053, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:22:01,619] Trial 481 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 36, 'token_conv_kernel': 9, 'last_d_model': 224, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 32, 'pos_d_model': 24, 'd_model': 176, 'conv_out_dim': 64, 'e_layers': 3, 'learning_rate': 0.005505885336706053, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 11, 'skip_connection_mode': 'conv_mlp', 'conv_norm': True, 'mlp_norm': True, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:22:01,820] The parameter 'use_pos_enc' in trial#489 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:22:01,825] The parameter 'norm_type' in trial#489 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:22:01,844] The parameter 'feat_conv_kernel' in trial#489 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:22:01,851] The parameter 'skip_connection_mode' in trial#489 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:22:01,864] The parameter 'mlp_norm' in trial#489 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=144, n_heads=4, e_layers=3, hidden_d_model=40, seq_layers=2, last_d_model=224, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=20, pos_d_model=16, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=4, fc_layer_type='mlp', conv_out_dim=64, feat_conv_kernel=9, skip_connection_mode='full', conv_norm=True, mlp_norm=False, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-full-conv-True-mlp-False-use_pos-True-seq_len-8-lr-0.0168-d-144-hid_d-40-last_d-224-tok_d-4-time_d-20-pos_d-16-e_layers-3-tok_conv_k-9-conv_out_d-64-feat_conv_k-9-dropout-0.16-norm_type-layer-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.01675176113619401, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:22:12,523] Trial 489 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 40, 'token_conv_kernel': 9, 'last_d_model': 224, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 20, 'pos_d_model': 16, 'd_model': 144, 'conv_out_dim': 64, 'e_layers': 3, 'learning_rate': 0.01675176113619401, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 9, 'skip_connection_mode': 'full', 'conv_norm': True, 'mlp_norm': False, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:22:12,781] The parameter 'use_pos_enc' in trial#501 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:22:12,787] The parameter 'norm_type' in trial#501 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:22:12,807] The parameter 'feat_conv_kernel' in trial#501 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:22:12,813] The parameter 'skip_connection_mode' in trial#501 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:22:12,826] The parameter 'mlp_norm' in trial#501 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=144, n_heads=4, e_layers=4, hidden_d_model=32, seq_layers=2, last_d_model=288, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=24, pos_d_model=24, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.12, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=4, fc_layer_type='mlp', conv_out_dim=128, feat_conv_kernel=11, skip_connection_mode='conv_mlp', conv_norm=True, mlp_norm=False, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-conv_mlp-conv-True-mlp-False-use_pos-True-seq_len-8-lr-0.0155-d-144-hid_d-32-last_d-288-tok_d-4-time_d-24-pos_d-24-e_layers-4-tok_conv_k-9-conv_out_d-128-feat_conv_k-11-dropout-0.12-norm_type-layer-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.015529239137901019, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:22:14,088] Trial 501 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 32, 'token_conv_kernel': 9, 'last_d_model': 288, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 24, 'pos_d_model': 24, 'd_model': 144, 'conv_out_dim': 128, 'e_layers': 4, 'learning_rate': 0.015529239137901019, 'dropout': 0.12000000000000001, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 11, 'skip_connection_mode': 'conv_mlp', 'conv_norm': True, 'mlp_norm': False, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:22:14,266] The parameter 'use_pos_enc' in trial#504 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:22:14,272] The parameter 'norm_type' in trial#504 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:22:14,290] The parameter 'feat_conv_kernel' in trial#504 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:22:14,297] The parameter 'skip_connection_mode' in trial#504 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:22:14,309] The parameter 'mlp_norm' in trial#504 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=208, n_heads=4, e_layers=3, hidden_d_model=36, seq_layers=2, last_d_model=224, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=20, pos_d_model=24, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=4, fc_layer_type='mlp', conv_out_dim=64, feat_conv_kernel=11, skip_connection_mode='conv_mlp', conv_norm=True, mlp_norm=True, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-conv_mlp-conv-True-mlp-True-use_pos-True-seq_len-8-lr-0.0173-d-208-hid_d-36-last_d-224-tok_d-4-time_d-20-pos_d-24-e_layers-3-tok_conv_k-9-conv_out_d-64-feat_conv_k-11-dropout-0.16-norm_type-layer-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.01728508444417629, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:22:15,547] Trial 504 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 36, 'token_conv_kernel': 9, 'last_d_model': 224, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 20, 'pos_d_model': 24, 'd_model': 208, 'conv_out_dim': 64, 'e_layers': 3, 'learning_rate': 0.01728508444417629, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 11, 'skip_connection_mode': 'conv_mlp', 'conv_norm': True, 'mlp_norm': True, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:22:15,753] The parameter 'use_pos_enc' in trial#506 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:22:15,759] The parameter 'norm_type' in trial#506 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:22:15,773] The parameter 'feat_conv_kernel' in trial#506 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:22:15,778] The parameter 'skip_connection_mode' in trial#506 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:22:15,788] The parameter 'mlp_norm' in trial#506 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=176, n_heads=4, e_layers=3, hidden_d_model=44, seq_layers=2, last_d_model=224, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=28, pos_d_model=20, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=4, fc_layer_type='mlp', conv_out_dim=192, feat_conv_kernel=9, skip_connection_mode='full', conv_norm=True, mlp_norm=True, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-full-conv-True-mlp-True-use_pos-True-seq_len-8-lr-0.0225-d-176-hid_d-44-last_d-224-tok_d-4-time_d-28-pos_d-20-e_layers-3-tok_conv_k-9-conv_out_d-192-feat_conv_k-9-dropout-0.14-norm_type-batch-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.022467067855742238, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:22:32,494] Trial 506 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 44, 'token_conv_kernel': 9, 'last_d_model': 224, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 28, 'pos_d_model': 20, 'd_model': 176, 'conv_out_dim': 192, 'e_layers': 3, 'learning_rate': 0.022467067855742238, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 9, 'skip_connection_mode': 'full', 'conv_norm': True, 'mlp_norm': True, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:22:52,730] The parameter 'use_pos_enc' in trial#516 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:22:52,735] The parameter 'norm_type' in trial#516 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:22:52,748] The parameter 'feat_conv_kernel' in trial#516 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:22:52,754] The parameter 'skip_connection_mode' in trial#516 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:22:52,763] The parameter 'mlp_norm' in trial#516 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=176, n_heads=4, e_layers=2, hidden_d_model=32, seq_layers=2, last_d_model=160, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=32, pos_d_model=20, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=False, bidirectional=False, norm_type='batch', num_heads=4, fc_layer_type='mlp', conv_out_dim=128, feat_conv_kernel=11, skip_connection_mode='conv_mlp', conv_norm=True, mlp_norm=False, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-conv_mlp-conv-True-mlp-False-use_pos-False-seq_len-8-lr-0.0356-d-176-hid_d-32-last_d-160-tok_d-4-time_d-32-pos_d-20-e_layers-2-tok_conv_k-9-conv_out_d-128-feat_conv_k-11-dropout-0.14-norm_type-batch-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.035550228092281505, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:22:53,964] Trial 516 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 32, 'token_conv_kernel': 9, 'last_d_model': 160, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 32, 'pos_d_model': 20, 'd_model': 176, 'conv_out_dim': 128, 'e_layers': 2, 'learning_rate': 0.035550228092281505, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': False, 'norm_type': 'batch', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 11, 'skip_connection_mode': 'conv_mlp', 'conv_norm': True, 'mlp_norm': False, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:22:54,239] The parameter 'use_pos_enc' in trial#517 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:22:54,245] The parameter 'norm_type' in trial#517 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:22:54,260] The parameter 'feat_conv_kernel' in trial#517 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:22:54,266] The parameter 'skip_connection_mode' in trial#517 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:22:54,277] The parameter 'mlp_norm' in trial#517 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=176, n_heads=4, e_layers=3, hidden_d_model=32, seq_layers=2, last_d_model=288, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=20, pos_d_model=24, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=4, fc_layer_type='mlp', conv_out_dim=256, feat_conv_kernel=11, skip_connection_mode='full', conv_norm=True, mlp_norm=True, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-full-conv-True-mlp-True-use_pos-True-seq_len-8-lr-0.0253-d-176-hid_d-32-last_d-288-tok_d-4-time_d-20-pos_d-24-e_layers-3-tok_conv_k-9-conv_out_d-256-feat_conv_k-11-dropout-0.14-norm_type-batch-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.02527909755385924, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:22:55,161] Trial 517 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 32, 'token_conv_kernel': 9, 'last_d_model': 288, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 20, 'pos_d_model': 24, 'd_model': 176, 'conv_out_dim': 256, 'e_layers': 3, 'learning_rate': 0.02527909755385924, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 11, 'skip_connection_mode': 'full', 'conv_norm': True, 'mlp_norm': True, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:22:55,403] The parameter 'use_pos_enc' in trial#518 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:22:55,409] The parameter 'norm_type' in trial#518 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:22:55,425] The parameter 'feat_conv_kernel' in trial#518 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:22:55,431] The parameter 'skip_connection_mode' in trial#518 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:22:55,442] The parameter 'mlp_norm' in trial#518 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=176, n_heads=4, e_layers=4, hidden_d_model=28, seq_layers=2, last_d_model=288, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=32, pos_d_model=20, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=4, fc_layer_type='mlp', conv_out_dim=192, feat_conv_kernel=9, skip_connection_mode='conv_mlp', conv_norm=True, mlp_norm=True, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-conv_mlp-conv-True-mlp-True-use_pos-True-seq_len-8-lr-0.0482-d-176-hid_d-28-last_d-288-tok_d-4-time_d-32-pos_d-20-e_layers-4-tok_conv_k-9-conv_out_d-192-feat_conv_k-9-dropout-0.14-norm_type-batch-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.04824414390149706, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:23:01,506] Trial 518 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 28, 'token_conv_kernel': 9, 'last_d_model': 288, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 32, 'pos_d_model': 20, 'd_model': 176, 'conv_out_dim': 192, 'e_layers': 4, 'learning_rate': 0.04824414390149706, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 9, 'skip_connection_mode': 'conv_mlp', 'conv_norm': True, 'mlp_norm': True, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:23:01,683] The parameter 'use_pos_enc' in trial#519 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:23:01,690] The parameter 'norm_type' in trial#519 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:23:01,705] The parameter 'feat_conv_kernel' in trial#519 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:23:01,711] The parameter 'skip_connection_mode' in trial#519 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:23:01,722] The parameter 'mlp_norm' in trial#519 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=176, n_heads=4, e_layers=2, hidden_d_model=44, seq_layers=2, last_d_model=160, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=32, pos_d_model=20, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=4, fc_layer_type='mlp', conv_out_dim=192, feat_conv_kernel=9, skip_connection_mode='conv_mlp', conv_norm=True, mlp_norm=False, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-conv_mlp-conv-True-mlp-False-use_pos-True-seq_len-8-lr-0.0194-d-176-hid_d-44-last_d-160-tok_d-4-time_d-32-pos_d-20-e_layers-2-tok_conv_k-9-conv_out_d-192-feat_conv_k-9-dropout-0.14-norm_type-layer-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.019367928723434204, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
[I 2024-08-19 05:24:03,387] Trial 519 finished with value: 1.0594771681725978 and parameters: {'num_heads': 4, 'hidden_d_model': 44, 'token_conv_kernel': 9, 'last_d_model': 160, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 32, 'pos_d_model': 20, 'd_model': 176, 'conv_out_dim': 192, 'e_layers': 2, 'learning_rate': 0.019367928723434204, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 9, 'skip_connection_mode': 'conv_mlp', 'conv_norm': True, 'mlp_norm': False, 'scale_y_type': 'standard'}. Best is trial 519 with value: 1.0594771681725978.
[W 2024-08-19 05:24:03,617] The parameter 'use_pos_enc' in trial#521 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:24:03,624] The parameter 'norm_type' in trial#521 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:24:03,646] The parameter 'feat_conv_kernel' in trial#521 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:24:03,652] The parameter 'skip_connection_mode' in trial#521 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:24:03,666] The parameter 'mlp_norm' in trial#521 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=176, n_heads=4, e_layers=4, hidden_d_model=36, seq_layers=2, last_d_model=160, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=24, pos_d_model=20, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=False, bidirectional=False, norm_type='layer', num_heads=4, fc_layer_type='mlp', conv_out_dim=192, feat_conv_kernel=9, skip_connection_mode='full', conv_norm=True, mlp_norm=True, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-full-conv-True-mlp-True-use_pos-False-seq_len-8-lr-0.0097-d-176-hid_d-36-last_d-160-tok_d-4-time_d-24-pos_d-20-e_layers-4-tok_conv_k-9-conv_out_d-192-feat_conv_k-9-dropout-0.14-norm_type-layer-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.009713655057274477, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
[I 2024-08-19 05:26:05,830] Trial 521 finished with value: 1.047265489399433 and parameters: {'num_heads': 4, 'hidden_d_model': 36, 'token_conv_kernel': 9, 'last_d_model': 160, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 24, 'pos_d_model': 20, 'd_model': 176, 'conv_out_dim': 192, 'e_layers': 4, 'learning_rate': 0.009713655057274477, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': False, 'norm_type': 'layer', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 9, 'skip_connection_mode': 'full', 'conv_norm': True, 'mlp_norm': True, 'scale_y_type': 'standard'}. Best is trial 520 with value: 0.9241759982705116.
[W 2024-08-19 05:26:06,061] The parameter 'use_pos_enc' in trial#524 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:26:06,069] The parameter 'norm_type' in trial#524 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:26:06,089] The parameter 'feat_conv_kernel' in trial#524 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:26:06,097] The parameter 'skip_connection_mode' in trial#524 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:26:06,111] The parameter 'mlp_norm' in trial#524 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=176, n_heads=4, e_layers=2, hidden_d_model=32, seq_layers=2, last_d_model=160, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=28, pos_d_model=24, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=4, fc_layer_type='mlp', conv_out_dim=192, feat_conv_kernel=9, skip_connection_mode='full', conv_norm=True, mlp_norm=True, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-full-conv-True-mlp-True-use_pos-True-seq_len-8-lr-0.0294-d-176-hid_d-32-last_d-160-tok_d-4-time_d-28-pos_d-24-e_layers-2-tok_conv_k-9-conv_out_d-192-feat_conv_k-9-dropout-0.14-norm_type-layer-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.029378124418039126, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
[I 2024-08-19 05:27:00,710] Trial 524 finished with value: 3.0279812118411065 and parameters: {'num_heads': 4, 'hidden_d_model': 32, 'token_conv_kernel': 9, 'last_d_model': 160, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 28, 'pos_d_model': 24, 'd_model': 176, 'conv_out_dim': 192, 'e_layers': 2, 'learning_rate': 0.029378124418039126, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 9, 'skip_connection_mode': 'full', 'conv_norm': True, 'mlp_norm': True, 'scale_y_type': 'standard'}. Best is trial 520 with value: 0.9241759982705116.
[W 2024-08-19 05:27:00,944] The parameter 'use_pos_enc' in trial#525 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:27:00,952] The parameter 'norm_type' in trial#525 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:27:00,975] The parameter 'feat_conv_kernel' in trial#525 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:27:00,983] The parameter 'skip_connection_mode' in trial#525 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:27:00,999] The parameter 'mlp_norm' in trial#525 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=176, n_heads=4, e_layers=3, hidden_d_model=20, seq_layers=2, last_d_model=288, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=24, pos_d_model=20, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=4, fc_layer_type='mlp', conv_out_dim=192, feat_conv_kernel=9, skip_connection_mode='conv_mlp', conv_norm=True, mlp_norm=True, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-conv_mlp-conv-True-mlp-True-use_pos-True-seq_len-8-lr-0.0292-d-176-hid_d-20-last_d-288-tok_d-4-time_d-24-pos_d-20-e_layers-3-tok_conv_k-9-conv_out_d-192-feat_conv_k-9-dropout-0.14-norm_type-batch-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.02921280774955735, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
[I 2024-08-19 05:28:56,096] Trial 525 finished with value: 0.9886067487299443 and parameters: {'num_heads': 4, 'hidden_d_model': 20, 'token_conv_kernel': 9, 'last_d_model': 288, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 24, 'pos_d_model': 20, 'd_model': 176, 'conv_out_dim': 192, 'e_layers': 3, 'learning_rate': 0.02921280774955735, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 9, 'skip_connection_mode': 'conv_mlp', 'conv_norm': True, 'mlp_norm': True, 'scale_y_type': 'standard'}. Best is trial 520 with value: 0.9241759982705116.
[W 2024-08-19 05:28:56,317] The parameter 'use_pos_enc' in trial#526 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:28:56,325] The parameter 'norm_type' in trial#526 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:28:56,347] The parameter 'feat_conv_kernel' in trial#526 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:28:56,354] The parameter 'skip_connection_mode' in trial#526 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:28:56,368] The parameter 'mlp_norm' in trial#526 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=176, n_heads=4, e_layers=2, hidden_d_model=36, seq_layers=2, last_d_model=96, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=28, pos_d_model=16, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.12, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=4, fc_layer_type='mlp', conv_out_dim=192, feat_conv_kernel=11, skip_connection_mode='conv_mlp', conv_norm=True, mlp_norm=False, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-conv_mlp-conv-True-mlp-False-use_pos-True-seq_len-8-lr-0.0278-d-176-hid_d-36-last_d-96-tok_d-4-time_d-28-pos_d-16-e_layers-2-tok_conv_k-9-conv_out_d-192-feat_conv_k-11-dropout-0.12-norm_type-batch-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.02780603003392909, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
[I 2024-08-19 05:30:52,344] Trial 526 finished with value: 1.0102276087366044 and parameters: {'num_heads': 4, 'hidden_d_model': 36, 'token_conv_kernel': 9, 'last_d_model': 96, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 28, 'pos_d_model': 16, 'd_model': 176, 'conv_out_dim': 192, 'e_layers': 2, 'learning_rate': 0.02780603003392909, 'dropout': 0.12000000000000001, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 11, 'skip_connection_mode': 'conv_mlp', 'conv_norm': True, 'mlp_norm': False, 'scale_y_type': 'standard'}. Best is trial 520 with value: 0.9241759982705116.
[W 2024-08-19 05:30:52,557] The parameter 'use_pos_enc' in trial#527 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:30:52,565] The parameter 'norm_type' in trial#527 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:30:52,585] The parameter 'feat_conv_kernel' in trial#527 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:30:52,592] The parameter 'skip_connection_mode' in trial#527 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:30:52,606] The parameter 'mlp_norm' in trial#527 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=176, n_heads=4, e_layers=3, hidden_d_model=40, seq_layers=2, last_d_model=96, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=32, pos_d_model=20, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=4, fc_layer_type='mlp', conv_out_dim=192, feat_conv_kernel=11, skip_connection_mode='full', conv_norm=True, mlp_norm=False, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-full-conv-True-mlp-False-use_pos-True-seq_len-8-lr-0.0275-d-176-hid_d-40-last_d-96-tok_d-4-time_d-32-pos_d-20-e_layers-3-tok_conv_k-9-conv_out_d-192-feat_conv_k-11-dropout-0.14-norm_type-layer-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.027529415450031954, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
[I 2024-08-19 05:31:50,788] Trial 527 finished with value: 1.994456911087036 and parameters: {'num_heads': 4, 'hidden_d_model': 40, 'token_conv_kernel': 9, 'last_d_model': 96, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 32, 'pos_d_model': 20, 'd_model': 176, 'conv_out_dim': 192, 'e_layers': 3, 'learning_rate': 0.027529415450031954, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 11, 'skip_connection_mode': 'full', 'conv_norm': True, 'mlp_norm': False, 'scale_y_type': 'standard'}. Best is trial 520 with value: 0.9241759982705116.
Number of finished trials: 528
Best trial:
Value: 0.9241759982705116
Params: 
    num_heads: 4
    hidden_d_model: 40
    token_conv_kernel: 9
    last_d_model: 96
    seq_len: 8
    token_d_model: 4
    time_d_model: 28
    pos_d_model: 16
    d_model: 144
    conv_out_dim: 192
    e_layers: 3
    learning_rate: 0.011312946661734042
    dropout: 0.14
    combine_type: add
    use_pos_enc: True
    norm_type: batch
    batch_size: 1024
    train_epochs: 50
    feat_conv_kernel: 11
    skip_connection_mode: conv_mlp
    conv_norm: True
    mlp_norm: True
    scale_y_type: standard
Top 10 trials saved to /data/Pein/Pytorch/Wind-Power-Prediction/optuna_results/24-08-19-no_time/24-08-19-no_time-farm_66_top10_params.json
Total time taken:  693.9020 seconds,  11.57 minutes
Done!
