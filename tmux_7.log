[I 2024-08-19 05:20:10,989] Using an existing study with name '24-08-19-no_time-farm_66' instead of creating a new one.
Creating study "24-08-19-no_time-farm_66" with storage "sqlite:////data/Pein/Pytorch/Wind-Power-Prediction/optuna_results/24-08-19-no_time/24-08-19-no_time-farm_66.db?mode=wal"...
Using sampler cma with seed 6200
Using sampler : CmaEsSampler, pruner : <optuna.pruners._median.MedianPruner object at 0x7fac06b2a860>
  0%|          | 0/24 [00:00<?, ?it/s]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                        0%|          | 0/24 [00:00<?, ?it/s]                                        0%|          | 0/24 [00:00<?, ?it/s]                                        0%|          | 0/24 [00:00<?, ?it/s]                                        0%|          | 0/24 [00:00<?, ?it/s]                                        0%|          | 0/24 [00:00<?, ?it/s]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA A100 80GB PCIe') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 539 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
539 K     Trainable params
0         Non-trainable params
539 K     Total params
2.158     Total estimated model params size (MB)
135       Modules in train mode
0         Modules in eval mode
Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1023, in _run_stage
    self._run_sanity_check()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1052, in _run_sanity_check
    val_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py", line 178, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 113, in run
    self.reset()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 231, in reset
    iter(data_fetcher)  # creates the iterator inside the fetcher
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 104, in __iter__
    super().__iter__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 51, in __iter__
    self.iterator = iter(self.combined_loader)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 351, in __iter__
    iter(iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 155, in __iter__
    self._load_current_iterator()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 173, in _load_current_iterator
    self.iterators = [iter(self.iterables[self._iterator_idx])]
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 440, in __iter__
    return self._get_iterator()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 388, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1011, in __init__
    self._worker_result_queue = multiprocessing_context.Queue()  # type: ignore[var-annotated]
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/context.py", line 103, in Queue
    return Queue(maxsize, ctx=self.get_context())
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/queues.py", line 43, in __init__
    self._rlock = ctx.Lock()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/context.py", line 68, in Lock
    return Lock(ctx=self.get_context())
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/synchronize.py", line 162, in __init__
    SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/synchronize.py", line 57, in __init__
    sl = self._semlock = _multiprocessing.SemLock(
OSError: [Errno 28] No space left on device
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7faca800da20>
Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1477, in __del__
    self._shutdown_workers()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1408, in _shutdown_workers
    if not self._shutdown:
AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_shutdown'
                                        0%|          | 0/24 [00:01<?, ?it/s]Best trial: 0. Best value: inf:   0%|          | 0/24 [00:01<?, ?it/s]Best trial: 0. Best value: inf:   4%|▍         | 1/24 [00:01<00:33,  1.46s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                              Best trial: 0. Best value: inf:   4%|▍         | 1/24 [00:01<00:33,  1.46s/it]                                                                              Best trial: 0. Best value: inf:   4%|▍         | 1/24 [00:01<00:33,  1.46s/it]                                                                              Best trial: 0. Best value: inf:   4%|▍         | 1/24 [00:01<00:33,  1.46s/it]                                                                              Best trial: 0. Best value: inf:   4%|▍         | 1/24 [00:01<00:33,  1.46s/it]                                                                              Best trial: 0. Best value: inf:   4%|▍         | 1/24 [00:01<00:33,  1.46s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 627 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
627 K     Trainable params
0         Non-trainable params
627 K     Total params
2.510     Total estimated model params size (MB)
155       Modules in train mode
0         Modules in eval mode
ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1023, in _run_stage
    self._run_sanity_check()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1052, in _run_sanity_check
    val_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py", line 178, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 135, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 396, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_args)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 319, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 411, in validation_step
    return self.lightning_module.validation_step(*args, **kwargs)
  File "/data/Pein/Pytorch/Wind-Power-Prediction/exp/pl_exp.py", line 118, in validation_step
    return self.common_step(batch, batch_idx, phase, dataloader_idx)
  File "/data/Pein/Pytorch/Wind-Power-Prediction/exp/pl_exp.py", line 89, in common_step
    loss = self.process_batch(batch, self.criterion)
  File "/data/Pein/Pytorch/Wind-Power-Prediction/exp/pl_exp.py", line 186, in process_batch
    outputs = self(batch_x, batch_x_mark, dec_inp, batch_y_mark)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Pein/Pytorch/Wind-Power-Prediction/exp/pl_exp.py", line 280, in forward
    return self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Pein/Pytorch/Wind-Power-Prediction/models/MLP_v3.py", line 503, in forward
    x = self.init_block(x, x_mark)  # Shape: [batch, seq_len, conv_out_dim]
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Pein/Pytorch/Wind-Power-Prediction/models/MLP_v3.py", line 111, in forward
    X_token_out = self.token_embedding(x)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Pein/Pytorch/Wind-Power-Prediction/models/layers/Embed.py", line 272, in forward
    conv_out = self.conv1d(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 308, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 304, in _conv_forward
    return F.conv1d(input, weight, bias, self.stride,
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/signal_handling.py", line 67, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 187810) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit.
                                                                              Best trial: 0. Best value: inf:   4%|▍         | 1/24 [00:02<00:33,  1.46s/it]Best trial: 0. Best value: inf:   4%|▍         | 1/24 [00:02<00:33,  1.46s/it]Best trial: 0. Best value: inf:   8%|▊         | 2/24 [00:02<00:28,  1.28s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                              Best trial: 0. Best value: inf:   8%|▊         | 2/24 [00:02<00:28,  1.28s/it]                                                                              Best trial: 0. Best value: inf:   8%|▊         | 2/24 [00:02<00:28,  1.28s/it]                                                                              Best trial: 0. Best value: inf:   8%|▊         | 2/24 [00:02<00:28,  1.28s/it]                                                                              Best trial: 0. Best value: inf:   8%|▊         | 2/24 [00:02<00:28,  1.28s/it]                                                                              Best trial: 0. Best value: inf:   8%|▊         | 2/24 [00:02<00:28,  1.28s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 751 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
751 K     Trainable params
0         Non-trainable params
751 K     Total params
3.008     Total estimated model params size (MB)
155       Modules in train mode
0         Modules in eval mode
Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1023, in _run_stage
    self._run_sanity_check()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1052, in _run_sanity_check
    val_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py", line 178, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 113, in run
    self.reset()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 231, in reset
    iter(data_fetcher)  # creates the iterator inside the fetcher
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 104, in __iter__
    super().__iter__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 51, in __iter__
    self.iterator = iter(self.combined_loader)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 351, in __iter__
    iter(iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 155, in __iter__
    self._load_current_iterator()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 173, in _load_current_iterator
    self.iterators = [iter(self.iterables[self._iterator_idx])]
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 440, in __iter__
    return self._get_iterator()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 388, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1020, in __init__
    index_queue = multiprocessing_context.Queue()  # type: ignore[var-annotated]
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/context.py", line 103, in Queue
    return Queue(maxsize, ctx=self.get_context())
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/queues.py", line 43, in __init__
    self._rlock = ctx.Lock()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/context.py", line 68, in Lock
    return Lock(ctx=self.get_context())
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/synchronize.py", line 162, in __init__
    SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/synchronize.py", line 57, in __init__
    sl = self._semlock = _multiprocessing.SemLock(
OSError: [Errno 28] No space left on device
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7faca800da20>
Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1477, in __del__
    self._shutdown_workers()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1435, in _shutdown_workers
    if self._persistent_workers or self._workers_status[worker_id]:
AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_workers_status'
                                                                              Best trial: 0. Best value: inf:   8%|▊         | 2/24 [00:03<00:28,  1.28s/it]Best trial: 0. Best value: inf:   8%|▊         | 2/24 [00:03<00:28,  1.28s/it]Best trial: 0. Best value: inf:  12%|█▎        | 3/24 [00:03<00:23,  1.13s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                              Best trial: 0. Best value: inf:  12%|█▎        | 3/24 [00:03<00:23,  1.13s/it]                                                                              Best trial: 0. Best value: inf:  12%|█▎        | 3/24 [00:03<00:23,  1.13s/it]                                                                              Best trial: 0. Best value: inf:  12%|█▎        | 3/24 [00:03<00:23,  1.13s/it]                                                                              Best trial: 0. Best value: inf:  12%|█▎        | 3/24 [00:03<00:23,  1.13s/it]                                                                              Best trial: 0. Best value: inf:  12%|█▎        | 3/24 [00:03<00:23,  1.13s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 578 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
578 K     Trainable params
0         Non-trainable params
578 K     Total params
2.314     Total estimated model params size (MB)
155       Modules in train mode
0         Modules in eval mode
Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1023, in _run_stage
    self._run_sanity_check()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1052, in _run_sanity_check
    val_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py", line 178, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 113, in run
    self.reset()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 231, in reset
    iter(data_fetcher)  # creates the iterator inside the fetcher
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 104, in __iter__
    super().__iter__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 51, in __iter__
    self.iterator = iter(self.combined_loader)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 351, in __iter__
    iter(iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 155, in __iter__
    self._load_current_iterator()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 173, in _load_current_iterator
    self.iterators = [iter(self.iterables[self._iterator_idx])]
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 440, in __iter__
    return self._get_iterator()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 388, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1011, in __init__
    self._worker_result_queue = multiprocessing_context.Queue()  # type: ignore[var-annotated]
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/context.py", line 103, in Queue
    return Queue(maxsize, ctx=self.get_context())
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/queues.py", line 43, in __init__
    self._rlock = ctx.Lock()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/context.py", line 68, in Lock
    return Lock(ctx=self.get_context())
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/synchronize.py", line 162, in __init__
    SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/synchronize.py", line 57, in __init__
    sl = self._semlock = _multiprocessing.SemLock(
OSError: [Errno 28] No space left on device
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7faca800da20>
Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1477, in __del__
    self._shutdown_workers()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1408, in _shutdown_workers
    if not self._shutdown:
AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_shutdown'
                                                                              Best trial: 0. Best value: inf:  12%|█▎        | 3/24 [00:04<00:23,  1.13s/it]Best trial: 0. Best value: inf:  12%|█▎        | 3/24 [00:04<00:23,  1.13s/it]Best trial: 0. Best value: inf:  17%|█▋        | 4/24 [00:04<00:21,  1.08s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                              Best trial: 0. Best value: inf:  17%|█▋        | 4/24 [00:04<00:21,  1.08s/it]                                                                              Best trial: 0. Best value: inf:  17%|█▋        | 4/24 [00:04<00:21,  1.08s/it]                                                                              Best trial: 0. Best value: inf:  17%|█▋        | 4/24 [00:04<00:21,  1.08s/it]                                                                              Best trial: 0. Best value: inf:  17%|█▋        | 4/24 [00:04<00:21,  1.08s/it]                                                                              Best trial: 0. Best value: inf:  17%|█▋        | 4/24 [00:04<00:21,  1.08s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 907 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
907 K     Trainable params
0         Non-trainable params
907 K     Total params
3.631     Total estimated model params size (MB)
155       Modules in train mode
0         Modules in eval mode
ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1023, in _run_stage
    self._run_sanity_check()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1052, in _run_sanity_check
    val_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py", line 178, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 128, in run
    batch, batch_idx, dataloader_idx = next(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 133, in __next__
    batch = super().__next__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 60, in __next__
    batch = next(self.iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 341, in __next__
    out = next(self._iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 142, in __next__
    out = next(self.iterators[0])
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1344, in _next_data
    return self._process_data(data)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1370, in _process_data
    data.reraise()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/_utils.py", line 706, in reraise
    raise exception
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 309, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 55, in fetch
    return self.collate_fn(data)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 317, in default_collate
    return collate(batch, collate_fn_map=default_collate_fn_map)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 174, in collate
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 174, in <listcomp>
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 142, in collate
    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 212, in collate_tensor_fn
    storage = elem._typed_storage()._new_shared(numel, device=elem.device)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/storage.py", line 968, in _new_shared
    untyped_storage = torch.UntypedStorage._new_shared(size * self._element_size(), device=device)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/storage.py", line 302, in _new_shared
    return cls._new_using_fd_cpu(size)
RuntimeError: unable to write to file </torch_190690_1376433433_0>: No space left on device (28)

                                                                              Best trial: 0. Best value: inf:  17%|█▋        | 4/24 [00:10<00:21,  1.08s/it]Best trial: 0. Best value: inf:  17%|█▋        | 4/24 [00:10<00:21,  1.08s/it]Best trial: 0. Best value: inf:  21%|██        | 5/24 [00:10<00:54,  2.88s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                              Best trial: 0. Best value: inf:  21%|██        | 5/24 [00:10<00:54,  2.88s/it]                                                                              Best trial: 0. Best value: inf:  21%|██        | 5/24 [00:10<00:54,  2.88s/it]                                                                              Best trial: 0. Best value: inf:  21%|██        | 5/24 [00:10<00:54,  2.88s/it]                                                                              Best trial: 0. Best value: inf:  21%|██        | 5/24 [00:10<00:54,  2.88s/it]                                                                              Best trial: 0. Best value: inf:  21%|██        | 5/24 [00:10<00:54,  2.88s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 721 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
721 K     Trainable params
0         Non-trainable params
721 K     Total params
2.886     Total estimated model params size (MB)
155       Modules in train mode
0         Modules in eval mode
ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1131, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/queues.py", line 114, in get
    raise Empty
_queue.Empty

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1025, in _run_stage
    self.fit_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 205, in run
    self.advance()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 363, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 140, in run
    self.advance(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 212, in advance
    batch, _, __ = next(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 133, in __next__
    batch = super().__next__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 60, in __next__
    batch = next(self.iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 341, in __next__
    out = next(self._iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 78, in __next__
    out[i] = next(self.iterators[i])
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1327, in _next_data
    idx, data = self._get_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1293, in _get_data
    success, data = self._try_get_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1144, in _try_get_data
    raise RuntimeError(f'DataLoader worker (pid(s) {pids_str}) exited unexpectedly') from e
RuntimeError: DataLoader worker (pid(s) 193861) exited unexpectedly
                                                                              Best trial: 0. Best value: inf:  21%|██        | 5/24 [00:17<00:54,  2.88s/it]Best trial: 0. Best value: inf:  21%|██        | 5/24 [00:17<00:54,  2.88s/it]Best trial: 0. Best value: inf:  25%|██▌       | 6/24 [00:17<01:14,  4.15s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                              Best trial: 0. Best value: inf:  25%|██▌       | 6/24 [00:17<01:14,  4.15s/it]                                                                              Best trial: 0. Best value: inf:  25%|██▌       | 6/24 [00:17<01:14,  4.15s/it]                                                                              Best trial: 0. Best value: inf:  25%|██▌       | 6/24 [00:17<01:14,  4.15s/it]                                                                              Best trial: 0. Best value: inf:  25%|██▌       | 6/24 [00:17<01:14,  4.15s/it]                                                                              Best trial: 0. Best value: inf:  25%|██▌       | 6/24 [00:17<01:14,  4.15s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 707 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
707 K     Trainable params
0         Non-trainable params
707 K     Total params
2.830     Total estimated model params size (MB)
135       Modules in train mode
0         Modules in eval mode
ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1025, in _run_stage
    self.fit_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 205, in run
    self.advance()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 363, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 140, in run
    self.advance(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 212, in advance
    batch, _, __ = next(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 133, in __next__
    batch = super().__next__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 60, in __next__
    batch = next(self.iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 341, in __next__
    out = next(self._iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 78, in __next__
    out[i] = next(self.iterators[i])
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1327, in _next_data
    idx, data = self._get_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1293, in _get_data
    success, data = self._try_get_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1131, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/queues.py", line 122, in get
    return _ForkingPickler.loads(res)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/multiprocessing/reductions.py", line 496, in rebuild_storage_fd
    fd = df.detach()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/resource_sharer.py", line 86, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/connection.py", line 508, in Client
    answer_challenge(c, authkey)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/connection.py", line 752, in answer_challenge
    message = connection.recv_bytes(256)         # reject large message
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
ConnectionResetError: [Errno 104] Connection res/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                              Best trial: 0. Best value: inf:  29%|██▉       | 7/24 [01:30<02:33,  9.02s/it]                                                                              Best trial: 0. Best value: inf:  29%|██▉       | 7/24 [01:30<02:33,  9.02s/it]                                                                              Best trial: 0. Best value: inf:  29%|██▉       | 7/24 [01:30<02:33,  9.02s/it]                                                                              Best trial: 0. Best value: inf:  29%|██▉       | 7/24 [01:30<02:33,  9.02s/it]                                                                              Best trial: 0. Best value: inf:  29%|██▉       | 7/24 [01:30<02:33,  9.02s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 465 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
465 K     Trainable params
0         Non-trainable params
465 K     Total params
1.862     Total estimated model params size (MB)
75        Modules in train mode
0         Modules in eval mode
ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1131, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/queues.py", line 114, in get
    raise Empty
_queue.Empty

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1023, in _run_stage
    self._run_sanity_check()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1052, in _run_sanity_check
    val_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py", line 178, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 128, in run
    batch, batch_idx, dataloader_idx = next(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 133, in __next__
    batch = super().__next__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 60, in __next__
    batch = next(self.iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 341, in __next__
    out = next(self._iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 142, in __next__
    out = next(self.iterators[0])
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1327, in _next_data
    idx, data = self._get_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1293, in _get_data
    success, data = self._try_get_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1144, in _try_get_data
    raise RuntimeError(f'DataLoader worker (pid(s) {pids_str}) exited unexpectedly') from e
RuntimeError: DataLoader worker (pid(s) 206514) exited unexpectedly
                                                                              Best trial: 0. Best value: inf:  29%|██▉       | 7/24 [01:36<02:33,  9.02s/it]Best trial: 0. Best value: inf:  29%|██▉       | 7/24 [01:36<02:33,  9.02s/it]Best trial: 0. Best value: inf:  33%|███▎      | 8/24 [01:36<05:34, 20.92s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                              Best trial: 0. Best value: inf:  33%|███▎      | 8/24 [01:36<05:34, 20.92s/it]                                                                              Best trial: 0. Best value: inf:  33%|███▎      | 8/24 [01:36<05:34, 20.92s/it]                                                                              Best trial: 0. Best value: inf:  33%|███▎      | 8/24 [01:36<05:34, 20.92s/it]                                                                              Best trial: 0. Best value: inf:  33%|███▎      | 8/24 [01:36<05:34, 20.92s/it]                                                                              Best trial: 0. Best value: inf:  33%|███▎      | 8/24 [01:36<05:34, 20.92s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 779 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
779 K     Trainable params
0         Non-trainable params
779 K     Total params
3.118     Total estimated model params size (MB)
135       Modules in train mode
0         Modules in eval mode
ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1025, in _run_stage
    self.fit_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 205, in run
    self.advance()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 363, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 140, in run
    self.advance(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 212, in advance
    batch, _, __ = next(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 133, in __next__
    batch = super().__next__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 60, in __next__
    batch = next(self.iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 341, in __next__
    out = next(self._iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 78, in __next__
    out[i] = next(self.iterators[i])
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1344, in _next_data
    return self._process_data(data)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1370, in _process_data
    data.reraise()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/_utils.py", line 706, in reraise
    raise exception
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 309, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 55, in fetch
    return self.collate_fn(data)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 317, in default_collate
    return collate(batch, collate_fn_map=default_collate_fn_map)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 174, in collate
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 174, in <listcomp>
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 142, in collate
    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 212, in collate_tensor_fn
    storage = elem._typed_storage()._new_shared(numel, device=elem.device)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/storage.py", line 968, in _new_shared
    untyped_storage = torch.UntypedStorage._new_shared(size * self._element_size(), device=device)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/storage.py", line 302, in _new_shared
    return cls._new_using_fd_cpu(size)
RuntimeError: unable to write to file </torch_211756_2808799614_0>: No space left on device (28)

                                                                              Best trial: 0. Best value: inf:  33%|███▎      | 8/24 [01:48<05:34, 20.92s/it]Best trial: 0. Best value: inf:  33%|███▎      | 8/24 [01:48<05:34, 20.92s/it]Best trial: 0. Best value: inf:  38%|███▊      | 9/24 [01:48<04:32, 18.17s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                              Best trial: 0. Best value: inf:  38%|███▊      | 9/24 [01:49<04:32, 18.17s/it]                                                                              Best trial: 0. Best value: inf:  38%|███▊      | 9/24 [01:49<04:32, 18.17s/it]                                                                              Best trial: 0. Best value: inf:  38%|███▊      | 9/24 [01:49<04:32, 18.17s/it]                                                                              Best trial: 0. Best value: inf:  38%|███▊      | 9/24 [01:49<04:32, 18.17s/it]                                                                              Best trial: 0. Best value: inf:  38%|███▊      | 9/24 [01:49<04:32, 18.17s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 621 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
621 K     Trainable params
0         Non-trainable params
621 K     Total params
2.484     Total estimated model params size (MB)
115       Modules in train mode
0         Modules in eval mode
ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1131, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/queues.py", line 114, in get
    raise Empty
_queue.Empty

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1023, in _run_stage
    self._run_sanity_check()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1052, in _run_sanity_check
    val_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py", line 178, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 128, in run
    batch, batch_idx, dataloader_idx = next(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 133, in __next__
    batch = super().__next__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 60, in __next__
    batch = next(self.iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 341, in __next__
    out = next(self._iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 142, in __next__
    out = next(self.iterators[0])
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1327, in _next_data
    idx, data = self._get_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1293, in _get_data
    success, data = self._try_get_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1144, in _try_get_data
    raise RuntimeError(f'DataLoader worker (pid(s) {pids_str}) exited unexpectedly') from e
RuntimeError: DataLoader worker (pid(s) 216516) exited unexpectedly
                                                                              Best trial: 0. Best value: inf:  38%|███▊      | 9/24 [01:54<04:32, 18.17s/it]Best trial: 0. Best value: inf:  38%|███▊      | 9/24 [01:54<04:32, 18.17s/it]Best trial: 0. Best value: inf:  42%|████▏     | 10/24 [01:54<03:22, 14.45s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                               Best trial: 0. Best value: inf:  42%|████▏     | 10/24 [01:54<03:22, 14.45s/it]                                                                               Best trial: 0. Best value: inf:  42%|████▏     | 10/24 [01:54<03:22, 14.45s/it]                                                                               Best trial: 0. Best value: inf:  42%|████▏     | 10/24 [01:55<03:22, 14.45s/it]                                                                               Best trial: 0. Best value: inf:  42%|████▏     | 10/24 [01:55<03:22, 14.45s/it]                                                                               Best trial: 0. Best value: inf:  42%|████▏     | 10/24 [01:55<03:22, 14.45s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 710 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
710 K     Trainable params
0         Non-trainable params
710 K     Total params
2.841     Total estimated model params size (MB)
135       Modules in train mode
0         Modules in eval mode
Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1023, in _run_stage
    self._run_sanity_check()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1052, in _run_sanity_check
    val_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py", line 178, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 113, in run
    self.reset()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 231, in reset
    iter(data_fetcher)  # creates the iterator inside the fetcher
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 104, in __iter__
    super().__iter__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 51, in __iter__
    self.iterator = iter(self.combined_loader)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 351, in __iter__
    iter(iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 155, in __iter__
    self._load_current_iterator()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 173, in _load_current_iterator
    self.iterators = [iter(self.iterables[self._iterator_idx])]
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 440, in __iter__
    return self._get_iterator()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 388, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1011, in __init__
    self._worker_result_queue = multiprocessing_context.Queue()  # type: ignore[var-annotated]
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/context.py", line 103, in Queue
    return Queue(maxsize, ctx=self.get_context())
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/queues.py", line 43, in __init__
    self._rlock = ctx.Lock()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/context.py", line 68, in Lock
    return Lock(ctx=self.get_context())
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/synchronize.py", line 162, in __init__
    SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/synchronize.py", line 57, in __init__
    sl = self._semlock = _multiprocessing.SemLock(
OSError: [Errno 28] No space left on device
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff1ce10da20>
Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1477, in __del__
    self._shutdown_workers()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1408, in _shutdown_workers
    if not self._shutdown:
AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_shutdown'
                                                                               Best trial: 0. Best value: inf:  42%|████▏     | 10/24 [01:55<03:22, 14.45s/it]Best trial: 0. Best value: inf:  42%|████▏     | 10/24 [01:55<03:22, 14.45s/it]Best trial: 0. Best value: inf:  46%|████▌     | 11/24 [01:55<02:13, 10.23s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                               Best trial: 0. Best value: inf:  46%|████▌     | 11/24 [01:55<02:13, 10.23s/it]                                                                               Best trial: 0. Best value: inf:  46%|████▌     | 11/24 [01:55<02:13, 10.23s/it]                                                                               Best trial: 0. Best value: inf:  46%|████▌     | 11/24 [01:55<02:13, 10.23s/it]                                                                               Best trial: 0. Best value: inf:  46%|████▌     | 11/24 [01:55<02:13, 10.23s/it]                                                                               Best trial: 0. Best value: inf:  46%|████▌     | 11/24 [01:55<02:13, 10.23s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 464 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
464 K     Trainable params
0         Non-trainable params
464 K     Total params
1.858     Total estimated model params size (MB)
75        Modules in train mode
0         Modules in eval mode
ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1131, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/queues.py", line 114, in get
    raise Empty
_queue.Empty

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1023, in _run_stage
    self._run_sanity_check()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1052, in _run_sanity_check
    val_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py", line 178, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 128, in run
    batch, batch_idx, dataloader_idx = next(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 133, in __next__
    batch = super().__next__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 60, in __next__
    batch = next(self.iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 341, in __next__
    out = next(self._iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 142, in __next__
    out = next(self.iterators[0])
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1327, in _next_data
    idx, data = self._get_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1293, in _get_data
    success, data = self._try_get_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1144, in _try_get_data
    raise RuntimeError(f'DataLoader worker (pid(s) {pids_str}) exited unexpectedly') from e
RuntimeError: DataLoader worker (pid(s) 220806) exited unexpectedly
                                                                               Best trial: 0. Best value: inf:  46%|████▌     | 11/24 [02:01<02:13, 10.23s/it]Best trial: 0. Best value: inf:  46%|████▌     | 11/24 [02:01<02:13, 10.23s/it]Best trial: 0. Best value: inf:  50%|█████     | 12/24 [02:01<01:47,  8.97s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                               Best trial: 0. Best value: inf:  50%|█████     | 12/24 [02:01<01:47,  8.97s/it]                                                                               Best trial: 0. Best value: inf:  50%|█████     | 12/24 [02:01<01:47,  8.97s/it]                                                                               Best trial: 0. Best value: inf:  50%|█████     | 12/24 [02:01<01:47,  8.97s/it]                                                                               Best trial: 0. Best value: inf:  50%|█████     | 12/24 [02:01<01:47,  8.97s/it]                                                                               Best trial: 0. Best value: inf:  50%|█████     | 12/24 [02:01<01:47,  8.97s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 535 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
535 K     Trainable params
0         Non-trainable params
535 K     Total params
2.142     Total estimated model params size (MB)
95        Modules in train mode
0         Modules in eval mode
ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1131, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/queues.py", line 114, in get
    raise Empty
_queue.Empty

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1023, in _run_stage
    self._run_sanity_check()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1052, in _run_sanity_check
    val_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py", line 178, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 128, in run
    batch, batch_idx, dataloader_idx = next(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 133, in __next__
    batch = super().__next__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 60, in __next__
    batch = next(self.iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 341, in __next__
    out = next(self._iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 142, in __next__
    out = next(self.iterators[0])
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1327, in _next_data
    idx, data = self._get_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1293, in _get_data
    success, data = self._try_get_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1144, in _try_get_data
    raise RuntimeError(f'DataLoader worker (pid(s) {pids_str}) exited unexpectedly') from e
RuntimeError: DataLoader worker (pid(s) 223709, 223765) exited unexpectedly
                                                                               Best trial: 0. Best value: inf:  50%|█████     | 12/24 [02:07<01:47,  8.97s/it]Best trial: 0. Best value: inf:  50%|█████     | 12/24 [02:07<01:47,  8.97s/it]Best trial: 0. Best value: inf:  54%|█████▍    | 13/24 [02:07<01:27,  8.00s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                               Best trial: 0. Best value: inf:  54%|█████▍    | 13/24 [02:07<01:27,  8.00s/it]                                                                               Best trial: 0. Best value: inf:  54%|█████▍    | 13/24 [02:07<01:27,  8.00s/it]                                                                               Best trial: 0. Best value: inf:  54%|█████▍    | 13/24 [02:07<01:27,  8.00s/it]                                                                               Best trial: 0. Best value: inf:  54%|█████▍    | 13/24 [02:07<01:27,  8.00s/it]                                                                               Best trial: 0. Best value: inf:  54%|█████▍    | 13/24 [02:07<01:27,  8.00s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 479 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
479 K     Trainable params
0         Non-trainable params
479 K     Total params
1.918     Total estimated model params size (MB)
55        Modules in train mode
0         Modules in eval mode
ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1023, in _run_stage
    self._run_sanity_check()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1052, in _run_sanity_check
    val_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py", line 178, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 128, in run
    batch, batch_idx, dataloader_idx = next(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 133, in __next__
    batch = super().__next__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 60, in __next__
    batch = next(self.iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 341, in __next__
    out = next(self._iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 142, in __next__
    out = next(self.iterators[0])
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1344, in _next_data
    return self._process_data(data)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1370, in _process_data
    data.reraise()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/_utils.py", line 706, in reraise
    raise exception
RuntimeError: Caught RuntimeError in DataLoader worker process 1.
Original Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 309, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 55, in fetch
    return self.collate_fn(data)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 317, in default_collate
    return collate(batch, collate_fn_map=default_collate_fn_map)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 174, in collate
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 174, in <listcomp>
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 142, in collate
    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 212, in collate_tensor_fn
    storage = elem._typed_storage()._new_shared(numel, device=elem.device)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/storage.py", line 968, in _new_shared
    untyped_storage = torch.UntypedStorage._new_shared(size * self._element_size(), device=device)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/storage.py", line 302, in _new_shared
    return cls._new_using_fd_cpu(size)
RuntimeError: unable to write to file </torch_225819_704381977_3>: No space left on device (28)

                                                                               Best trial: 0. Best value: inf:  54%|█████▍    | 13/24 [02:13<01:27,  8.00s/it]Best trial: 0. Best value: inf:  54%|█████▍    | 13/24 [02:13<01:27,  8.00s/it]Best trial: 0. Best value: inf:  58%|█████▊    | 14/24 [02:13<01:13,  7.34s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                               Best trial: 0. Best value: inf:  58%|█████▊    | 14/24 [02:13<01:13,  7.34s/it]                                                                               Best trial: 0. Best value: inf:  58%|█████▊    | 14/24 [02:13<01:13,  7.34s/it]                                                                               Best trial: 0. Best value: inf:  58%|█████▊    | 14/24 [02:13<01:13,  7.34s/it]                                                                               Best trial: 0. Best value: inf:  58%|█████▊    | 14/24 [02:13<01:13,  7.34s/it]                                                                               Best trial: 0. Best value: inf:  58%|█████▊    | 14/24 [02:13<01:13,  7.34s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 514 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
514 K     Trainable params
0         Non-trainable params
514 K     Total params
2.059     Total estimated model params size (MB)
75        Modules in train mode
0         Modules in eval mode
ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1131, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/queues.py", line 114, in get
    raise Empty
_queue.Empty

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1023, in _run_stage
    self._run_sanity_check()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1052, in _run_sanity_check
    val_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py", line 178, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 128, in run
    batch, batch_idx, dataloader_idx = next(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 133, in __next__
    batch = super().__next__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 60, in __next__
    batch = next(self.iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 341, in __next__
    out = next(self._iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 142, in __next__
    out = next(self.iterators[0])
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1327, in _next_data
    idx, data = self._get_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1293, in _get_data
    success, data = self._try_get_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1144, in _try_get_data
    raise RuntimeError(f'DataLoader worker (pid(s) {pids_str}) exited unexpectedly') from e
RuntimeError: DataLoader worker (pid(s) 228872) exited unexpectedly
                                                                               Best trial: 0. Best value: inf:  58%|█████▊    | 14/24 [02:18<01:13,  7.34s/it]Best trial: 0. Best value: inf:  58%|█████▊    | 14/24 [02:18<01:13,  7.34s/it]Best trial: 0. Best value: inf:  62%|██████▎   | 15/24 [02:18<01:02,  6.91s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                               Best trial: 0. Best value: inf:  62%|██████▎   | 15/24 [02:19<01:02,  6.91s/it]                                                                               Best trial: 0. Best value: inf:  62%|██████▎   | 15/24 [02:19<01:02,  6.91s/it]                                                                               Best trial: 0. Best value: inf:  62%|██████▎   | 15/24 [02:19<01:02,  6.91s/it]                                                                               Best trial: 0. Best value: inf:  62%|██████▎   | 15/24 [02:19<01:02,  6.91s/it]                                                                               Best trial: 0. Best value: inf:  62%|██████▎   | 15/24 [02:19<01:02,  6.91s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 983 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
983 K     Trainable params
0         Non-trainable params
983 K     Total params
3.933     Total estimated model params size (MB)
95        Modules in train mode
0         Modules in eval mode
Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1023, in _run_stage
    self._run_sanity_check()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1052, in _run_sanity_check
    val_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py", line 178, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 113, in run
    self.reset()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 231, in reset
    iter(data_fetcher)  # creates the iterator inside the fetcher
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 104, in __iter__
    super().__iter__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 51, in __iter__
    self.iterator = iter(self.combined_loader)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 351, in __iter__
    iter(iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 155, in __iter__
    self._load_current_iterator()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 173, in _load_current_iterator
    self.iterators = [iter(self.iterables[self._iterator_idx])]
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 440, in __iter__
    return self._get_iterator()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 388, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1020, in __init__
    index_queue = multiprocessing_context.Queue()  # type: ignore[var-annotated]
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/context.py", line 103, in Queue
    return Queue(maxsize, ctx=self.get_context())
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/queues.py", line 43, in __init__
    self._rlock = ctx.Lock()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/context.py", line 68, in Lock
    return Lock(ctx=self.get_context())
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/synchronize.py", line 162, in __init__
    SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/synchronize.py", line 57, in __init__
    sl = self._semlock = _multiprocessing.SemLock(
OSError: [Errno 28] No space left on device
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff1ce10da20>
Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1477, in __del__
    self._shutdown_workers()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1435, in _shutdown_workers
    if self._persistent_workers or self._workers_status[worker_id]:
AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_workers_status'
                                                                               Best trial: 0. Best value: inf:  62%|██████▎   | 15/24 [02:19<01:02,  6.91s/it]Best trial: 0. Best value: inf:  62%|██████▎   | 15/24 [02:19<01:02,  6.91s/it]Best trial: 0. Best value: inf:  67%|██████▋   | 16/24 [02:19<00:40,  5.11s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                               Best trial: 0. Best value: inf:  67%|██████▋   | 16/24 [02:20<00:40,  5.11s/it]                                                                               Best trial: 0. Best value: inf:  67%|██████▋   | 16/24 [02:20<00:40,  5.11s/it]                                                                               Best trial: 0. Best value: inf:  67%|██████▋   | 16/24 [02:20<00:40,  5.11s/it]                                                                               Best trial: 0. Best value: inf:  67%|██████▋   | 16/24 [02:20<00:40,  5.11s/it]                                                                               Best trial: 0. Best value: inf:  67%|██████▋   | 16/24 [02:20<00:40,  5.11s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 590 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
590 K     Trainable params
0         Non-trainable params
590 K     Total params
2.361     Total estimated model params size (MB)
55        Modules in train mode
0         Modules in eval mode
Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1023, in _run_stage
    self._run_sanity_check()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1052, in _run_sanity_check
    val_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py", line 178, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 113, in run
    self.reset()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 231, in reset
    iter(data_fetcher)  # creates the iterator inside the fetcher
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 104, in __iter__
    super().__iter__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 51, in __iter__
    self.iterator = iter(self.combined_loader)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 351, in __iter__
    iter(iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 155, in __iter__
    self._load_current_iterator()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 173, in _load_current_iterator
    self.iterators = [iter(self.iterables[self._iterator_idx])]
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 440, in __iter__
    return self._get_iterator()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 388, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1011, in __init__
    self._worker_result_queue = multiprocessing_context.Queue()  # type: ignore[var-annotated]
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/context.py", line 103, in Queue
    return Queue(maxsize, ctx=self.get_context())
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/queues.py", line 43, in __init__
    self._rlock = ctx.Lock()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/context.py", line 68, in Lock
    return Lock(ctx=self.get_context())
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/synchronize.py", line 162, in __init__
    SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/synchronize.py", line 57, in __init__
    sl = self._semlock = _multiprocessing.SemLock(
OSError: [Errno 28] No space left on device
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff1ce10da20>
Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1477, in __del__
    self._shutdown_workers()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1408, in _shutdown_workers
    if not self._shutdown:
AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_shutdown'
                                                                               Best trial: 0. Best value: inf:  67%|██████▋   | 16/24 [02:20<00:40,  5.11s/it]Best trial: 0. Best value: inf:  67%|██████▋   | 16/24 [02:20<00:40,  5.11s/it]Best trial: 0. Best value: inf:  71%|███████   | 17/24 [02:20<00:26,  3.74s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                               Best trial: 0. Best value: inf:  71%|███████   | 17/24 [02:20<00:26,  3.74s/it]                                                                               Best trial: 0. Best value: inf:  71%|███████   | 17/24 [02:20<00:26,  3.74s/it]                                                                               Best trial: 0. Best value: inf:  71%|███████   | 17/24 [02:20<00:26,  3.74s/it]                                                                               Best trial: 0. Best value: inf:  71%|███████   | 17/24 [02:20<00:26,  3.74s/it]                                                                               Best trial: 0. Best value: inf:  71%|███████   | 17/24 [02:20<00:26,  3.74s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 213 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
213 K     Trainable params
0         Non-trainable params
213 K     Total params
0.853     Total estimated model params size (MB)
55        Modules in train mode
0         Modules in eval mode
Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1023, in _run_stage
    self._run_sanity_check()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1052, in _run_sanity_check
    val_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py", line 178, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 128, in run
    batch, batch_idx, dataloader_idx = next(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 133, in __next__
    batch = super().__next__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 60, in __next__
    batch = next(self.iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 341, in __next__
    out = next(self._iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 142, in __next__
    out = next(self.iterators[0])
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1344, in _next_data
    return self._process_data(data)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1370, in _process_data
    data.reraise()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/_utils.py", line 706, in reraise
    raise exception
RuntimeError: Caught RuntimeError in DataLoader worker process 1.
Original Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 309, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 55, in fetch
    return self.collate_fn(data)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 317, in default_collate
    return collate(batch, collate_fn_map=default_collate_fn_map)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 174, in collate
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 174, in <listcomp>
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 142, in collate
    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 212, in collate_tensor_fn
    storage = elem._typed_storage()._new_shared(numel, device=elem.device)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/storage.py", line 968, in _new_shared
    untyped_storage = torch.UntypedStorage._new_shared(size * self._element_size(), device=device)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/storage.py", line 302, in _new_shared
    return cls._new_using_fd_cpu(size)
RuntimeError: unable to write to file </torch_232425_1197917304_0>: No space left on device (28)

                                                                               Best trial: 0. Best value: inf:  71%|███████   | 17/24 [02:31<00:26,  3.74s/it]Best trial: 0. Best value: inf:  71%|███████   | 17/24 [02:31<00:26,  3.74s/it]Best trial: 0. Best value: inf:  75%|███████▌  | 18/24 [02:31<00:35,  5.97s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                               Best trial: 0. Best value: inf:  75%|███████▌  | 18/24 [02:31<00:35,  5.97s/it]                                                                               Best trial: 0. Best value: inf:  75%|███████▌  | 18/24 [02:31<00:35,  5.97s/it]                                                                               Best trial: 0. Best value: inf:  75%|███████▌  | 18/24 [02:32<00:35,  5.97s/it]                                                                               Best trial: 0. Best value: inf:  75%|███████▌  | 18/24 [02:32<00:35,  5.97s/it]                                                                               Best trial: 0. Best value: inf:  75%|███████▌  | 18/24 [02:32<00:35,  5.97s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 505 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
505 K     Trainable params
0         Non-trainable params
505 K     Total params
2.022     Total estimated model params size (MB)
75        Modules in train mode
0         Modules in eval mode
Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1023, in _run_stage
    self._run_sanity_check()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1052, in _run_sanity_check
    val_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py", line 178, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 128, in run
    batch, batch_idx, dataloader_idx = next(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 133, in __next__
    batch = super().__next__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 60, in __next__
    batch = next(self.iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 341, in __next__
    out = next(self._iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 142, in __next__
    out = next(self.iterators[0])
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1344, in _next_data
    return self._process_data(data)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1370, in _process_data
    data.reraise()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/_utils.py", line 706, in reraise
    raise exception
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 309, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 55, in fetch
    return self.collate_fn(data)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 317, in default_collate
    return collate(batch, collate_fn_map=default_collate_fn_map)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 174, in collate
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 174, in <listcomp>
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 142, in collate
    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 212, in collate_tensor_fn
    storage = elem._typed_storage()._new_shared(numel, device=elem.device)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/storage.py", line 968, in _new_shared
    untyped_storage = torch.UntypedStorage._new_shared(size * self._element_size(), device=device)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/storage.py", line 302, in _new_shared
    return cls._new_using_fd_cpu(size)
RuntimeError: unable to write to file </torch_236300_3324613622_0>: No space left on device (28)

                                                                               Best trial: 0. Best value: inf:  75%|███████▌  | 18/24 [02:42<00:35,  5.97s/it]Best trial: 0. Best value: inf:  75%|███████▌  | 18/24 [02:42<00:35,  5.97s/it]Best trial: 0. Best value: inf:  79%|███████▉  | 19/24 [02:42<00:37,  7.47s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                               Best trial: 0. Best value: inf:  79%|███████▉  | 19/24 [02:42<00:37,  7.47s/it]                                                                               Best trial: 0. Best value: inf:  79%|███████▉  | 19/24 [02:42<00:37,  7.47s/it]                                                                               Best trial: 0. Best value: inf:  79%|███████▉  | 19/24 [02:42<00:37,  7.47s/it]                                                                               Best trial: 0. Best value: inf:  79%|███████▉  | 19/24 [02:42<00:37,  7.47s/it]                                                                               Best trial: 0. Best value: inf:  79%|███████▉  | 19/24 [02:42<00:37,  7.47s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 535 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
535 K     Trainable params
0         Non-trainable params
535 K     Total params
2.140     Total estimated model params size (MB)
55        Modules in train mode
0         Modules in eval mode
Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1023, in _run_stage
    self._run_sanity_check()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1052, in _run_sanity_check
    val_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py", line 178, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 128, in run
    batch, batch_idx, dataloader_idx = next(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 133, in __next__
    batch = super().__next__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 60, in __next__
    batch = next(self.iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 341, in __next__
    out = next(self._iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 142, in __next__
    out = next(self.iterators[0])
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1344, in _next_data
    return self._process_data(data)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1370, in _process_data
    data.reraise()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/_utils.py", line 706, in reraise
    raise exception
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 309, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 55, in fetch
    return self.collate_fn(data)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 317, in default_collate
    return collate(batch, collate_fn_map=default_collate_fn_map)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 174, in collate
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 174, in <listcomp>
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 142, in collate
    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 212, in collate_tensor_fn
    storage = elem._typed_storage()._new_shared(numel, device=elem.device)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/storage.py", line 968, in _new_shared
    untyped_storage = torch.UntypedStorage._new_shared(size * self._element_size(), device=device)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/storage.py", line 302, in _new_shared
    return cls._new_using_fd_cpu(size)
RuntimeError: unable to write to file </torch_241293_3448997769_0>: No space left on device (28)

                                                                               Best trial: 0. Best value: inf:  79%|███████▉  | 19/24 [02:53<00:37,  7.47s/it]Best trial: 0. Best value: inf:  79%|███████▉  | 19/24 [02:53<00:37,  7.47s/it]Best trial: 0. Best value: inf:  83%|████████▎ | 20/24 [02:53<00:33,  8.49s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                               Best trial: 0. Best value: inf:  83%|████████▎ | 20/24 [02:53<00:33,  8.49s/it]                                                                               Best trial: 0. Best value: inf:  83%|████████▎ | 20/24 [02:53<00:33,  8.49s/it]                                                                               Best trial: 0. Best value: inf:  83%|████████▎ | 20/24 [02:53<00:33,  8.49s/it]                                                                               Best trial: 0. Best value: inf:  83%|████████▎ | 20/24 [02:53<00:33,  8.49s/it]                                                                               Best trial: 0. Best value: inf:  83%|████████▎ | 20/24 [02:53<00:33,  8.49s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 370 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
370 K     Trainable params
0         Non-trainable params
370 K     Total params
1.482     Total estimated model params size (MB)
75        Modules in train mode
0         Modules in eval mode
ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1131, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/queues.py", line 114, in get
    raise Empty
_queue.Empty

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1025, in _run_stage
    self.fit_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 205, in run
    self.advance()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 363, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 140, in run
    self.advance(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 212, in advance
    batch, _, __ = next(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 133, in __next__
    batch = super().__next__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 60, in __next__
    batch = next(self.iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 341, in __next__
    out = next(self._iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 78, in __next__
    out[i] = next(self.iterators[i])
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1327, in _next_data
    idx, data = self._get_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1293, in _get_data
    success, data = self._try_get_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1144, in _try_get_data
    raise RuntimeError(f'DataLoader worker (pid(s) {pids_str}) exited unexpectedly') from e
RuntimeError: DataLoader worker (pid(s) 244805, 244883) exited unexpectedly
                                                                               Best trial: 0. Best value: inf:  83%|████████▎ | 20/24 [03:00<00:33,  8.49s/it]Best trial: 0. Best value: inf:  83%|████████▎ | 20/24 [03:00<00:33,  8.49s/it]Best trial: 0. Best value: inf:  88%|████████▊ | 21/24 [03:00<00:23,  7.91s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                               Best trial: 0. Best value: inf:  88%|████████▊ | 21/24 [03:00<00:23,  7.91s/it]                                                                               Best trial: 0. Best value: inf:  88%|████████▊ | 21/24 [03:00<00:23,  7.91s/it]                                                                               Best trial: 0. Best value: inf:  88%|████████▊ | 21/24 [03:00<00:23,  7.91s/it]                                                                               Best trial: 0. Best value: inf:  88%|████████▊ | 21/24 [03:00<00:23,  7.91s/it]                                                                               Best trial: 0. Best value: inf:  88%|████████▊ | 21/24 [03:00<00:23,  7.91s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 666 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
666 K     Trainable params
0         Non-trainable params
666 K     Total params
2.664     Total estimated model params size (MB)
95        Modules in train mode
0         Modules in eval mode
ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1131, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/queues.py", line 122, in get
    return _ForkingPickler.loads(res)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/multiprocessing/reductions.py", line 496, in rebuild_storage_fd
    fd = df.detach()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/resource_sharer.py", line 86, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/connection.py", line 502, in Client
    c = SocketClient(address)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/connection.py", line 630, in SocketClient
    s.connect(address)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1025, in _run_stage
    self.fit_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 205, in run
    self.advance()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 363, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 140, in run
    self.advance(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 212, in advance
    batch, _, __ = next(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 133, in __next__
    batch = super().__next__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 60, in __next__
    batch = next(self.iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 341, in __next__
    out = next(self._iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 78, in __next__
    out[i] = next(self.iterators[i])
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1327, in _next_data
    idx, data = self._get_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1293, in _get_data
    success, data = self._try_get_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1144, in _try_get_data
    raise RuntimeError(f'DataLoader worker (pid(s) {pids_str}) exited unexpectedly') from e
RuntimeError: DataLoader worker (pid(s) 246888) exited unexpectedly
                                                                               Best trial: 0. Best value: inf:  88%|████████▊ | 21/24 [03:02<00:23,  7.91s/it]Best trial: 0. Best value: inf:  88%|████████▊ | 21/24 [03:02<00:23,  7.91s/it]Best trial: 0. Best value: inf:  92%|█████████▏| 22/24 [03:02<00:12,  6.14s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                               Best trial: 0. Best value: inf:  92%|█████████▏| 22/24 [03:02<00:12,  6.14s/it]                                                                               Best trial: 0. Best value: inf:  92%|█████████▏| 22/24 [03:02<00:12,  6.14s/it]                                                                               Best trial: 0. Best value: inf:  92%|█████████▏| 22/24 [03:02<00:12,  6.14s/it]                                                                               Best trial: 0. Best value: inf:  92%|█████████▏| 22/24 [03:02<00:12,  6.14s/it]                                                                               Best trial: 0. Best value: inf:  92%|█████████▏| 22/24 [03:02<00:12,  6.14s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 438 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
438 K     Trainable params
0         Non-trainable params
438 K     Total params
1.756     Total estimated model params size (MB)
75        Modules in train mode
0         Modules in eval mode
Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1025, in _run_stage
    self.fit_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 205, in run
    self.advance()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 363, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 141, in run
    self.on_advance_end(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 295, in on_advance_end
    self.val_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py", line 178, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 142, in run
    return self.on_run_end()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 254, in on_run_end
    self._on_evaluation_epoch_end()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 333, in _on_evaluation_epoch_end
    call._call_callback_hooks(trainer, hook_name)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 218, in _call_callback_hooks
    fn(trainer, trainer.lightning_module, *args, **kwargs)
  File "/data/Pein/Pytorch/Wind-Power-Prediction/utils/callback.py", line 83, in on_validation_epoch_end
    self._update_metrics(trainer, pl_module)
  File "/data/Pein/Pytorch/Wind-Power-Prediction/utils/callback.py", line 119, in _update_metrics
    ) = full_inference(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/utils/inference.py", line 39, in full_inference
    train_rmse, train_custom_acc = invert_and_compute_metrics(train_loader)
  File "/data/Pein/Pytorch/Wind-Power-Prediction/utils/inference.py", line 17, in invert_and_compute_metrics
    for batch in loader:
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1344, in _next_data
    return self._process_data(data)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1370, in _process_data
    data.reraise()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/_utils.py", line 706, in reraise
    raise exception
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 309, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 55, in fetch
    return self.collate_fn(data)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 317, in default_collate
    return collate(batch, collate_fn_map=default_collate_fn_map)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 174, in collate
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 174, in <listcomp>
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 142, in collate
    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 212, in collate_tensor_fn
    storage = elem._typed_storage()._new_shared(numel, device=elem.device)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/storage.py", line 968, in _new_shared
    untyped_storage = torch.UntypedStorage._new_shared(size * self._element_size(), device=device)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/storage.py", line 302, in _new_shared
    return cls._new_using_fd_cpu(size)
RuntimeError: unable to write to file </torch_249510_966933910_2>: No space left on device (28)

ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
                                                                                Best trial: 0. Best value: inf:  92%|█████████▏| 22/24 [03:19<00:12,  6.14s/it]Best trial: 0. Best value: inf:  92%|█████████▏| 22/24 [03:19<00:12,  6.14s/it]Best trial: 0. Best value: inf:  96%|█████████▌| 23/24 [03:19<00:09,  9.65s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                               Best trial: 0. Best value: inf:  96%|█████████▌| 23/24 [03:20<00:09,  9.65s/it]                                                                               Best trial: 0. Best value: inf:  96%|█████████▌| 23/24 [03:20<00:09,  9.65s/it]                                                                               Best trial: 0. Best value: inf:  96%|█████████▌| 23/24 [03:20<00:09,  9.65s/it]                                                                               Best trial: 0. Best value: inf:  96%|█████████▌| 23/24 [03:20<00:09,  9.65s/it]                                                                               Best trial: 0. Best value: inf:  96%|█████████▌| 23/24 [03:20<00:09,  9.65s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 619 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
619 K     Trainable params
0         Non-trainable params
619 K     Total params
2.478     Total estimated model params size (MB)
55        Modules in train mode
0         Modules in eval mode
Metric Loss/val improved. New best score: 1.156
Metric Loss/val improved by 0.128 >= min_delta = 0.0. New best score: 1.029
ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1131, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/queues.py", line 114, in get
    raise Empty
_queue.Empty

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1025, in _run_stage
    self.fit_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 205, in run
    self.advance()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 363, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 141, in run
    self.on_advance_end(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 295, in on_advance_end
    self.val_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py", line 178, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 142, in run
    return self.on_run_end()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 254, in on_run_end
    self._on_evaluation_epoch_end()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 333, in _on_evaluation_epoch_end
    call._call_callback_hooks(trainer, hook_name)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 218, in _call_callback_hooks
    fn(trainer, trainer.lightning_module, *args, **kwargs)
  File "/data/Pein/Pytorch/Wind-Power-Prediction/utils/callback.py", line 83, in on_validation_epoch_end
    self._update_metrics(trainer, pl_module)
  File "/data/Pein/Pytorch/Wind-Power-Prediction/utils/callback.py", line 119, in _update_metrics
    ) = full_inference(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/utils/inference.py", line 39, in full_inference
    train_rmse, train_custom_acc = invert_and_compute_metrics(train_loader)
  File "/data/Pein/Pytorch/Wind-Power-Prediction/utils/inference.py", line 17, in invert_and_compute_metrics
    for batch in loader:
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1327, in _next_data
    idx, data = self._get_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1293, in _get_data
    success, data = self._try_get_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1144, in _try_get_data
    raise RuntimeError(f'DataLoader worker (pid(s) {pids_str}) exited unexpectedly') from e
RuntimeError: DataLoader worker (pid(s) 268519) exited unexpectedly
                                                                               Best trial: 0. Best value: inf:  96%|█████████▌| 23/24 [03:45<00:09,  9.65s/it]Best trial: 0. Best value: inf:  96%|█████████▌| 23/24 [03:45<00:09,  9.65s/it]Best trial: 0. Best value: inf: 100%|██████████| 24/24 [03:45<00:00, 14.38s/it]Best trial: 0. Best value: inf: 100%|██████████| 24/24 [03:45<00:00,  9.39s/it]
[W 2024-08-19 05:19:14,140] The parameter 'use_pos_enc' in trial#11 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:19:14,146] The parameter 'norm_type' in trial#11 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:19:14,164] The parameter 'feat_conv_kernel' in trial#11 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:19:14,183] The parameter 'skip_connection_mode' in trial#11 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:19:14,197] The parameter 'mlp_norm' in trial#11 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=112, n_heads=4, e_layers=4, hidden_d_model=32, seq_layers=2, last_d_model=480, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=8, pos_d_model=20, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.18, use_pos_enc=False, bidirectional=False, norm_type='layer', num_heads=4, fc_layer_type='mlp', conv_out_dim=64, feat_conv_kernel=11, skip_connection_mode='conv_mlp', conv_norm=True, mlp_norm=True, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-conv_mlp-conv-True-mlp-True-use_pos-False-seq_len-8-lr-0.0055-d-112-hid_d-32-last_d-480-tok_d-4-time_d-8-pos_d-20-e_layers-4-tok_conv_k-9-conv_out_d-64-feat_conv_k-11-dropout-0.18-norm_type-layer-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.005537241171601359, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:19:17,202] Trial 11 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 32, 'token_conv_kernel': 9, 'last_d_model': 480, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 8, 'pos_d_model': 20, 'd_model': 112, 'conv_out_dim': 64, 'e_layers': 4, 'learning_rate': 0.005537241171601359, 'dropout': 0.18, 'combine_type': 'add', 'use_pos_enc': False, 'norm_type': 'layer', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 11, 'skip_connection_mode': 'conv_mlp', 'conv_norm': True, 'mlp_norm': True, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:19:17,366] The parameter 'use_pos_enc' in trial#16 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:19:17,370] The parameter 'norm_type' in trial#16 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:19:17,384] The parameter 'feat_conv_kernel' in trial#16 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:19:17,388] The parameter 'skip_connection_mode' in trial#16 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:19:17,399] The parameter 'mlp_norm' in trial#16 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=80, n_heads=4, e_layers=5, hidden_d_model=28, seq_layers=2, last_d_model=288, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=20, pos_d_model=24, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.18, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=4, fc_layer_type='mlp', conv_out_dim=128, feat_conv_kernel=11, skip_connection_mode='conv_mlp', conv_norm=True, mlp_norm=False, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-conv_mlp-conv-True-mlp-False-use_pos-True-seq_len-8-lr-0.0063-d-80-hid_d-28-last_d-288-tok_d-4-time_d-20-pos_d-24-e_layers-5-tok_conv_k-9-conv_out_d-128-feat_conv_k-11-dropout-0.18-norm_type-layer-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.0063485432523715905, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:19:18,442] Trial 16 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 28, 'token_conv_kernel': 9, 'last_d_model': 288, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 20, 'pos_d_model': 24, 'd_model': 80, 'conv_out_dim': 128, 'e_layers': 5, 'learning_rate': 0.0063485432523715905, 'dropout': 0.18, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 11, 'skip_connection_mode': 'conv_mlp', 'conv_norm': True, 'mlp_norm': False, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:19:18,596] The parameter 'use_pos_enc' in trial#17 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:19:18,602] The parameter 'norm_type' in trial#17 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:19:18,617] The parameter 'feat_conv_kernel' in trial#17 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:19:18,622] The parameter 'skip_connection_mode' in trial#17 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:19:18,633] The parameter 'mlp_norm' in trial#17 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=80, n_heads=4, e_layers=5, hidden_d_model=28, seq_layers=2, last_d_model=160, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=20, pos_d_model=24, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=4, fc_layer_type='mlp', conv_out_dim=192, feat_conv_kernel=9, skip_connection_mode='full', conv_norm=True, mlp_norm=True, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-full-conv-True-mlp-True-use_pos-True-seq_len-8-lr-0.0061-d-80-hid_d-28-last_d-160-tok_d-4-time_d-20-pos_d-24-e_layers-5-tok_conv_k-9-conv_out_d-192-feat_conv_k-9-dropout-0.16-norm_type-layer-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.006061878536273563, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:19:25,563] Trial 17 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 28, 'token_conv_kernel': 9, 'last_d_model': 160, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 20, 'pos_d_model': 24, 'd_model': 80, 'conv_out_dim': 192, 'e_layers': 5, 'learning_rate': 0.006061878536273563, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 9, 'skip_connection_mode': 'full', 'conv_norm': True, 'mlp_norm': True, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:19:25,744] The parameter 'use_pos_enc' in trial#24 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:19:25,749] The parameter 'norm_type' in trial#24 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:19:25,761] The parameter 'feat_conv_kernel' in trial#24 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:19:25,765] The parameter 'skip_connection_mode' in trial#24 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:19:25,773] The parameter 'mlp_norm' in trial#24 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=48, n_heads=4, e_layers=5, hidden_d_model=32, seq_layers=2, last_d_model=416, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=28, pos_d_model=16, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=4, fc_layer_type='mlp', conv_out_dim=192, feat_conv_kernel=11, skip_connection_mode='conv_mlp', conv_norm=True, mlp_norm=False, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-conv_mlp-conv-True-mlp-False-use_pos-True-seq_len-8-lr-0.0061-d-48-hid_d-32-last_d-416-tok_d-4-time_d-28-pos_d-16-e_layers-5-tok_conv_k-9-conv_out_d-192-feat_conv_k-11-dropout-0.14-norm_type-batch-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.006098971234968616, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:19:35,422] Trial 24 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 32, 'token_conv_kernel': 9, 'last_d_model': 416, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 28, 'pos_d_model': 16, 'd_model': 48, 'conv_out_dim': 192, 'e_layers': 5, 'learning_rate': 0.006098971234968616, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 11, 'skip_connection_mode': 'conv_mlp', 'conv_norm': True, 'mlp_norm': False, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:19:35,597] The parameter 'use_pos_enc' in trial#37 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:19:35,602] The parameter 'norm_type' in trial#37 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:19:35,619] The parameter 'feat_conv_kernel' in trial#37 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:19:35,625] The parameter 'skip_connection_mode' in trial#37 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:19:35,635] The parameter 'mlp_norm' in trial#37 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=11, input_dim=52, dec_in=52, output_dim=1, d_model=144, n_heads=4, e_layers=5, hidden_d_model=40, seq_layers=2, last_d_model=160, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=16, pos_d_model=12, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=False, bidirectional=False, norm_type='layer', num_heads=4, fc_layer_type='mlp', conv_out_dim=128, feat_conv_kernel=11, skip_connection_mode='full', conv_norm=True, mlp_norm=False, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-full-conv-True-mlp-False-use_pos-False-seq_len-8-lr-0.009-d-144-hid_d-40-last_d-160-tok_d-4-time_d-16-pos_d-12-e_layers-5-tok_conv_k-11-conv_out_d-128-feat_conv_k-11-dropout-0.16-norm_type-layer-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.009009796923410746, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:19:42,098] Trial 37 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 40, 'token_conv_kernel': 11, 'last_d_model': 160, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 16, 'pos_d_model': 12, 'd_model': 144, 'conv_out_dim': 128, 'e_layers': 5, 'learning_rate': 0.009009796923410746, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': False, 'norm_type': 'layer', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 11, 'skip_connection_mode': 'full', 'conv_norm': True, 'mlp_norm': False, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:19:42,345] The parameter 'use_pos_enc' in trial#50 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:19:42,350] The parameter 'norm_type' in trial#50 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:19:42,370] The parameter 'feat_conv_kernel' in trial#50 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:19:42,376] The parameter 'skip_connection_mode' in trial#50 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:19:42,389] The parameter 'mlp_norm' in trial#50 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=112, n_heads=4, e_layers=6, hidden_d_model=36, seq_layers=2, last_d_model=480, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=8, time_d_model=24, pos_d_model=20, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=False, bidirectional=False, norm_type='layer', num_heads=4, fc_layer_type='mlp', conv_out_dim=192, feat_conv_kernel=11, skip_connection_mode='full', conv_norm=True, mlp_norm=False, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-full-conv-True-mlp-False-use_pos-False-seq_len-8-lr-0.0084-d-112-hid_d-36-last_d-480-tok_d-8-time_d-24-pos_d-20-e_layers-6-tok_conv_k-9-conv_out_d-192-feat_conv_k-11-dropout-0.16-norm_type-layer-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.008408232061955765, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:19:53,071] Trial 50 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 36, 'token_conv_kernel': 9, 'last_d_model': 480, 'seq_len': 8, 'token_d_model': 8, 'time_d_model': 24, 'pos_d_model': 20, 'd_model': 112, 'conv_out_dim': 192, 'e_layers': 6, 'learning_rate': 0.008408232061955765, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': False, 'norm_type': 'layer', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 11, 'skip_connection_mode': 'full', 'conv_norm': True, 'mlp_norm': False, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:19:53,281] The parameter 'use_pos_enc' in trial#69 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:19:53,287] The parameter 'norm_type' in trial#69 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:19:53,301] The parameter 'feat_conv_kernel' in trial#69 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:19:53,307] The parameter 'skip_connection_mode' in trial#69 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:19:53,318] The parameter 'mlp_norm' in trial#69 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=48, n_heads=4, e_layers=5, hidden_d_model=32, seq_layers=2, last_d_model=224, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=8, time_d_model=24, pos_d_model=16, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.12, use_pos_enc=False, bidirectional=False, norm_type='batch', num_heads=4, fc_layer_type='mlp', conv_out_dim=192, feat_conv_kernel=11, skip_connection_mode='conv_mlp', conv_norm=True, mlp_norm=True, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-conv_mlp-conv-True-mlp-True-use_pos-False-seq_len-8-lr-0.0058-d-48-hid_d-32-last_d-224-tok_d-8-time_d-24-pos_d-16-e_layers-5-tok_conv_k-9-conv_out_d-192-feat_conv_k-11-dropout-0.12-norm_type-batch-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.005773466981527272, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:20:04,015] Trial 69 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 32, 'token_conv_kernel': 9, 'last_d_model': 224, 'seq_len': 8, 'token_d_model': 8, 'time_d_model': 24, 'pos_d_model': 16, 'd_model': 48, 'conv_out_dim': 192, 'e_layers': 5, 'learning_rate': 0.005773466981527272, 'dropout': 0.12000000000000001, 'combine_type': 'add', 'use_pos_enc': False, 'norm_type': 'batch', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 11, 'skip_connection_mode': 'conv_mlp', 'conv_norm': True, 'mlp_norm': True, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:20:44,634] The parameter 'use_pos_enc' in trial#232 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:44,642] The parameter 'norm_type' in trial#232 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:44,655] The parameter 'feat_conv_kernel' in trial#232 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:44,661] The parameter 'skip_connection_mode' in trial#232 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:44,676] The parameter 'mlp_norm' in trial#232 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=112, n_heads=4, e_layers=3, hidden_d_model=32, seq_layers=2, last_d_model=416, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=28, pos_d_model=20, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=False, bidirectional=False, norm_type='layer', num_heads=4, fc_layer_type='mlp', conv_out_dim=128, feat_conv_kernel=9, skip_connection_mode='full', conv_norm=True, mlp_norm=True, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-full-conv-True-mlp-True-use_pos-False-seq_len-8-lr-0.0081-d-112-hid_d-32-last_d-416-tok_d-4-time_d-28-pos_d-20-e_layers-3-tok_conv_k-9-conv_out_d-128-feat_conv_k-9-dropout-0.14-norm_type-layer-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.008058999796173175, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:20:50,428] Trial 232 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 32, 'token_conv_kernel': 9, 'last_d_model': 416, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 28, 'pos_d_model': 20, 'd_model': 112, 'conv_out_dim': 128, 'e_layers': 3, 'learning_rate': 0.008058999796173175, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': False, 'norm_type': 'layer', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 9, 'skip_connection_mode': 'full', 'conv_norm': True, 'mlp_norm': True, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:20:50,700] The parameter 'use_pos_enc' in trial#256 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:50,704] The parameter 'norm_type' in trial#256 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:50,716] The parameter 'feat_conv_kernel' in trial#256 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:50,721] The parameter 'skip_connection_mode' in trial#256 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:50,730] The parameter 'mlp_norm' in trial#256 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=112, n_heads=4, e_layers=6, hidden_d_model=36, seq_layers=2, last_d_model=288, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=28, pos_d_model=24, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.18, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=4, fc_layer_type='mlp', conv_out_dim=192, feat_conv_kernel=11, skip_connection_mode='full', conv_norm=True, mlp_norm=True, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-full-conv-True-mlp-True-use_pos-True-seq_len-8-lr-0.0068-d-112-hid_d-36-last_d-288-tok_d-4-time_d-28-pos_d-24-e_layers-6-tok_conv_k-9-conv_out_d-192-feat_conv_k-11-dropout-0.18-norm_type-layer-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.006809951377067148, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:21:02,545] Trial 256 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 36, 'token_conv_kernel': 9, 'last_d_model': 288, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 28, 'pos_d_model': 24, 'd_model': 112, 'conv_out_dim': 192, 'e_layers': 6, 'learning_rate': 0.006809951377067148, 'dropout': 0.18, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 11, 'skip_connection_mode': 'full', 'conv_norm': True, 'mlp_norm': True, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:21:02,966] The parameter 'use_pos_enc' in trial#306 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:02,971] The parameter 'norm_type' in trial#306 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:02,987] The parameter 'feat_conv_kernel' in trial#306 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:02,992] The parameter 'skip_connection_mode' in trial#306 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:03,003] The parameter 'mlp_norm' in trial#306 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=144, n_heads=4, e_layers=5, hidden_d_model=28, seq_layers=2, last_d_model=288, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=28, pos_d_model=16, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=False, bidirectional=False, norm_type='layer', num_heads=4, fc_layer_type='mlp', conv_out_dim=128, feat_conv_kernel=9, skip_connection_mode='full', conv_norm=True, mlp_norm=True, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-full-conv-True-mlp-True-use_pos-False-seq_len-8-lr-0.0026-d-144-hid_d-28-last_d-288-tok_d-4-time_d-28-pos_d-16-e_layers-5-tok_conv_k-9-conv_out_d-128-feat_conv_k-9-dropout-0.14-norm_type-layer-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.0026267376601556056, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:21:08,657] Trial 306 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 28, 'token_conv_kernel': 9, 'last_d_model': 288, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 28, 'pos_d_model': 16, 'd_model': 144, 'conv_out_dim': 128, 'e_layers': 5, 'learning_rate': 0.0026267376601556056, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': False, 'norm_type': 'layer', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 9, 'skip_connection_mode': 'full', 'conv_norm': True, 'mlp_norm': True, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:21:08,898] The parameter 'use_pos_enc' in trial#326 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:08,906] The parameter 'norm_type' in trial#326 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:08,922] The parameter 'feat_conv_kernel' in trial#326 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:08,927] The parameter 'skip_connection_mode' in trial#326 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:08,939] The parameter 'mlp_norm' in trial#326 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=144, n_heads=4, e_layers=6, hidden_d_model=32, seq_layers=2, last_d_model=288, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=28, pos_d_model=16, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=False, bidirectional=False, norm_type='batch', num_heads=4, fc_layer_type='mlp', conv_out_dim=128, feat_conv_kernel=9, skip_connection_mode='full', conv_norm=True, mlp_norm=False, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-full-conv-True-mlp-False-use_pos-False-seq_len-8-lr-0.0048-d-144-hid_d-32-last_d-288-tok_d-4-time_d-28-pos_d-16-e_layers-6-tok_conv_k-9-conv_out_d-128-feat_conv_k-9-dropout-0.16-norm_type-batch-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.004817714740717448, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:21:09,337] Trial 326 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 32, 'token_conv_kernel': 9, 'last_d_model': 288, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 28, 'pos_d_model': 16, 'd_model': 144, 'conv_out_dim': 128, 'e_layers': 6, 'learning_rate': 0.004817714740717448, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': False, 'norm_type': 'batch', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 9, 'skip_connection_mode': 'full', 'conv_norm': True, 'mlp_norm': False, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:21:09,611] The parameter 'use_pos_enc' in trial#330 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:09,623] The parameter 'norm_type' in trial#330 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:09,644] The parameter 'feat_conv_kernel' in trial#330 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:09,662] The parameter 'skip_connection_mode' in trial#330 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:09,675] The parameter 'mlp_norm' in trial#330 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=144, n_heads=4, e_layers=3, hidden_d_model=20, seq_layers=2, last_d_model=224, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=20, pos_d_model=16, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=False, bidirectional=False, norm_type='layer', num_heads=4, fc_layer_type='mlp', conv_out_dim=128, feat_conv_kernel=9, skip_connection_mode='conv_mlp', conv_norm=True, mlp_norm=False, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-conv_mlp-conv-True-mlp-False-use_pos-False-seq_len-8-lr-0.0196-d-144-hid_d-20-last_d-224-tok_d-4-time_d-20-pos_d-16-e_layers-3-tok_conv_k-9-conv_out_d-128-feat_conv_k-9-dropout-0.14-norm_type-layer-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.019552425507543946, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:21:15,404] Trial 330 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 20, 'token_conv_kernel': 9, 'last_d_model': 224, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 20, 'pos_d_model': 16, 'd_model': 144, 'conv_out_dim': 128, 'e_layers': 3, 'learning_rate': 0.019552425507543946, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': False, 'norm_type': 'layer', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 9, 'skip_connection_mode': 'conv_mlp', 'conv_norm': True, 'mlp_norm': False, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:21:15,620] The parameter 'use_pos_enc' in trial#358 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:15,627] The parameter 'norm_type' in trial#358 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:15,643] The parameter 'feat_conv_kernel' in trial#358 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:15,652] The parameter 'skip_connection_mode' in trial#358 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:15,663] The parameter 'mlp_norm' in trial#358 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=80, n_heads=4, e_layers=4, hidden_d_model=28, seq_layers=2, last_d_model=288, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=32, pos_d_model=12, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=4, fc_layer_type='mlp', conv_out_dim=192, feat_conv_kernel=9, skip_connection_mode='full', conv_norm=True, mlp_norm=True, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-full-conv-True-mlp-True-use_pos-True-seq_len-8-lr-0.0054-d-80-hid_d-28-last_d-288-tok_d-4-time_d-32-pos_d-12-e_layers-4-tok_conv_k-9-conv_out_d-192-feat_conv_k-9-dropout-0.16-norm_type-layer-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.005369618347805505, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:21:21,177] Trial 358 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 28, 'token_conv_kernel': 9, 'last_d_model': 288, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 32, 'pos_d_model': 12, 'd_model': 80, 'conv_out_dim': 192, 'e_layers': 4, 'learning_rate': 0.005369618347805505, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 9, 'skip_connection_mode': 'full', 'conv_norm': True, 'mlp_norm': True, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:21:21,419] The parameter 'use_pos_enc' in trial#374 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:21,425] The parameter 'norm_type' in trial#374 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:21,442] The parameter 'feat_conv_kernel' in trial#374 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:21,449] The parameter 'skip_connection_mode' in trial#374 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:21,460] The parameter 'mlp_norm' in trial#374 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=112, n_heads=4, e_layers=2, hidden_d_model=16, seq_layers=2, last_d_model=224, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=24, pos_d_model=8, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=False, bidirectional=False, norm_type='layer', num_heads=4, fc_layer_type='mlp', conv_out_dim=192, feat_conv_kernel=11, skip_connection_mode='conv_mlp', conv_norm=True, mlp_norm=False, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-conv_mlp-conv-True-mlp-False-use_pos-False-seq_len-8-lr-0.0097-d-112-hid_d-16-last_d-224-tok_d-4-time_d-24-pos_d-8-e_layers-2-tok_conv_k-9-conv_out_d-192-feat_conv_k-11-dropout-0.16-norm_type-layer-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.009694717237956264, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:21:27,005] Trial 374 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 16, 'token_conv_kernel': 9, 'last_d_model': 224, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 24, 'pos_d_model': 8, 'd_model': 112, 'conv_out_dim': 192, 'e_layers': 2, 'learning_rate': 0.009694717237956264, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': False, 'norm_type': 'layer', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 11, 'skip_connection_mode': 'conv_mlp', 'conv_norm': True, 'mlp_norm': False, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:21:27,225] The parameter 'use_pos_enc' in trial#395 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:27,229] The parameter 'norm_type' in trial#395 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:27,242] The parameter 'feat_conv_kernel' in trial#395 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:27,247] The parameter 'skip_connection_mode' in trial#395 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:27,256] The parameter 'mlp_norm' in trial#395 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=144, n_heads=4, e_layers=3, hidden_d_model=36, seq_layers=2, last_d_model=160, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=20, pos_d_model=16, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.12, use_pos_enc=False, bidirectional=False, norm_type='batch', num_heads=4, fc_layer_type='mlp', conv_out_dim=128, feat_conv_kernel=11, skip_connection_mode='full', conv_norm=True, mlp_norm=True, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-full-conv-True-mlp-True-use_pos-False-seq_len-8-lr-0.0232-d-144-hid_d-36-last_d-160-tok_d-4-time_d-20-pos_d-16-e_layers-3-tok_conv_k-9-conv_out_d-128-feat_conv_k-11-dropout-0.12-norm_type-batch-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.023181271882380804, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:21:32,894] Trial 395 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 36, 'token_conv_kernel': 9, 'last_d_model': 160, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 20, 'pos_d_model': 16, 'd_model': 144, 'conv_out_dim': 128, 'e_layers': 3, 'learning_rate': 0.023181271882380804, 'dropout': 0.12000000000000001, 'combine_type': 'add', 'use_pos_enc': False, 'norm_type': 'batch', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 11, 'skip_connection_mode': 'full', 'conv_norm': True, 'mlp_norm': True, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:21:33,386] The parameter 'use_pos_enc' in trial#423 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:33,390] The parameter 'norm_type' in trial#423 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:33,403] The parameter 'feat_conv_kernel' in trial#423 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:33,408] The parameter 'skip_connection_mode' in trial#423 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:33,418] The parameter 'mlp_norm' in trial#423 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=176, n_heads=4, e_layers=4, hidden_d_model=44, seq_layers=2, last_d_model=160, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=8, time_d_model=28, pos_d_model=12, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=False, bidirectional=False, norm_type='layer', num_heads=4, fc_layer_type='mlp', conv_out_dim=128, feat_conv_kernel=11, skip_connection_mode='conv_mlp', conv_norm=True, mlp_norm=True, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-conv_mlp-conv-True-mlp-True-use_pos-False-seq_len-8-lr-0.0086-d-176-hid_d-44-last_d-160-tok_d-8-time_d-28-pos_d-12-e_layers-4-tok_conv_k-9-conv_out_d-128-feat_conv_k-11-dropout-0.14-norm_type-layer-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.008565280054124427, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:21:33,830] Trial 423 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 44, 'token_conv_kernel': 9, 'last_d_model': 160, 'seq_len': 8, 'token_d_model': 8, 'time_d_model': 28, 'pos_d_model': 12, 'd_model': 176, 'conv_out_dim': 128, 'e_layers': 4, 'learning_rate': 0.008565280054124427, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': False, 'norm_type': 'layer', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 11, 'skip_connection_mode': 'conv_mlp', 'conv_norm': True, 'mlp_norm': True, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:21:34,011] The parameter 'use_pos_enc' in trial#426 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:34,017] The parameter 'norm_type' in trial#426 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:34,034] The parameter 'feat_conv_kernel' in trial#426 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:34,040] The parameter 'skip_connection_mode' in trial#426 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:34,055] The parameter 'mlp_norm' in trial#426 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=176, n_heads=4, e_layers=2, hidden_d_model=24, seq_layers=2, last_d_model=160, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=32, pos_d_model=16, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=4, fc_layer_type='mlp', conv_out_dim=192, feat_conv_kernel=11, skip_connection_mode='full', conv_norm=True, mlp_norm=True, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-full-conv-True-mlp-True-use_pos-True-seq_len-8-lr-0.0317-d-176-hid_d-24-last_d-160-tok_d-4-time_d-32-pos_d-16-e_layers-2-tok_conv_k-9-conv_out_d-192-feat_conv_k-11-dropout-0.14-norm_type-batch-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.03169585832976675, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:21:34,370] Trial 426 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 24, 'token_conv_kernel': 9, 'last_d_model': 160, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 32, 'pos_d_model': 16, 'd_model': 176, 'conv_out_dim': 192, 'e_layers': 2, 'learning_rate': 0.03169585832976675, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 11, 'skip_connection_mode': 'full', 'conv_norm': True, 'mlp_norm': True, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:21:34,613] The parameter 'use_pos_enc' in trial#429 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:34,617] The parameter 'norm_type' in trial#429 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:34,629] The parameter 'feat_conv_kernel' in trial#429 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:34,634] The parameter 'skip_connection_mode' in trial#429 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:34,644] The parameter 'mlp_norm' in trial#429 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=48, n_heads=4, e_layers=2, hidden_d_model=40, seq_layers=2, last_d_model=224, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=28, pos_d_model=12, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=4, fc_layer_type='mlp', conv_out_dim=64, feat_conv_kernel=9, skip_connection_mode='conv_mlp', conv_norm=True, mlp_norm=False, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-conv_mlp-conv-True-mlp-False-use_pos-True-seq_len-8-lr-0.0257-d-48-hid_d-40-last_d-224-tok_d-4-time_d-28-pos_d-12-e_layers-2-tok_conv_k-9-conv_out_d-64-feat_conv_k-9-dropout-0.16-norm_type-layer-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.025715322713064873, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:21:45,553] Trial 429 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 40, 'token_conv_kernel': 9, 'last_d_model': 224, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 28, 'pos_d_model': 12, 'd_model': 48, 'conv_out_dim': 64, 'e_layers': 2, 'learning_rate': 0.025715322713064873, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 9, 'skip_connection_mode': 'conv_mlp', 'conv_norm': True, 'mlp_norm': False, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:21:45,880] The parameter 'use_pos_enc' in trial#459 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:45,886] The parameter 'norm_type' in trial#459 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:45,928] The parameter 'feat_conv_kernel' in trial#459 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:45,934] The parameter 'skip_connection_mode' in trial#459 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:45,947] The parameter 'mlp_norm' in trial#459 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=144, n_heads=4, e_layers=3, hidden_d_model=36, seq_layers=2, last_d_model=96, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=20, pos_d_model=16, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=False, bidirectional=False, norm_type='batch', num_heads=4, fc_layer_type='mlp', conv_out_dim=128, feat_conv_kernel=11, skip_connection_mode='full', conv_norm=True, mlp_norm=False, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-full-conv-True-mlp-False-use_pos-False-seq_len-8-lr-0.009-d-144-hid_d-36-last_d-96-tok_d-4-time_d-20-pos_d-16-e_layers-3-tok_conv_k-9-conv_out_d-128-feat_conv_k-11-dropout-0.14-norm_type-batch-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.008989558495417696, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:21:56,518] Trial 459 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 36, 'token_conv_kernel': 9, 'last_d_model': 96, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 20, 'pos_d_model': 16, 'd_model': 144, 'conv_out_dim': 128, 'e_layers': 3, 'learning_rate': 0.008989558495417696, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': False, 'norm_type': 'batch', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 11, 'skip_connection_mode': 'full', 'conv_norm': True, 'mlp_norm': False, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:21:56,746] The parameter 'use_pos_enc' in trial#482 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:56,751] The parameter 'norm_type' in trial#482 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:56,767] The parameter 'feat_conv_kernel' in trial#482 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:56,773] The parameter 'skip_connection_mode' in trial#482 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:56,783] The parameter 'mlp_norm' in trial#482 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=176, n_heads=4, e_layers=2, hidden_d_model=40, seq_layers=2, last_d_model=288, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=24, pos_d_model=24, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=4, fc_layer_type='mlp', conv_out_dim=128, feat_conv_kernel=11, skip_connection_mode='full', conv_norm=True, mlp_norm=False, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-full-conv-True-mlp-False-use_pos-True-seq_len-8-lr-0.0126-d-176-hid_d-40-last_d-288-tok_d-4-time_d-24-pos_d-24-e_layers-2-tok_conv_k-9-conv_out_d-128-feat_conv_k-11-dropout-0.16-norm_type-batch-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.012577289144316613, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:22:07,377] Trial 482 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 40, 'token_conv_kernel': 9, 'last_d_model': 288, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 24, 'pos_d_model': 24, 'd_model': 176, 'conv_out_dim': 128, 'e_layers': 2, 'learning_rate': 0.012577289144316613, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 11, 'skip_connection_mode': 'full', 'conv_norm': True, 'mlp_norm': False, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:22:07,587] The parameter 'use_pos_enc' in trial#497 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:22:07,592] The parameter 'norm_type' in trial#497 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:22:07,606] The parameter 'feat_conv_kernel' in trial#497 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:22:07,612] The parameter 'skip_connection_mode' in trial#497 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:22:07,621] The parameter 'mlp_norm' in trial#497 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=144, n_heads=4, e_layers=3, hidden_d_model=40, seq_layers=2, last_d_model=32, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=28, pos_d_model=12, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=False, bidirectional=False, norm_type='batch', num_heads=4, fc_layer_type='mlp', conv_out_dim=64, feat_conv_kernel=9, skip_connection_mode='conv_mlp', conv_norm=True, mlp_norm=True, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-conv_mlp-conv-True-mlp-True-use_pos-False-seq_len-8-lr-0.027-d-144-hid_d-40-last_d-32-tok_d-4-time_d-28-pos_d-12-e_layers-3-tok_conv_k-9-conv_out_d-64-feat_conv_k-9-dropout-0.14-norm_type-batch-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.027017973733043026, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:22:13,927] Trial 497 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 40, 'token_conv_kernel': 9, 'last_d_model': 32, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 28, 'pos_d_model': 12, 'd_model': 144, 'conv_out_dim': 64, 'e_layers': 3, 'learning_rate': 0.027017973733043026, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': False, 'norm_type': 'batch', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 9, 'skip_connection_mode': 'conv_mlp', 'conv_norm': True, 'mlp_norm': True, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:22:14,104] The parameter 'use_pos_enc' in trial#503 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:22:14,110] The parameter 'norm_type' in trial#503 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:22:14,129] The parameter 'feat_conv_kernel' in trial#503 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:22:14,135] The parameter 'skip_connection_mode' in trial#503 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:22:14,146] The parameter 'mlp_norm' in trial#503 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=208, n_heads=4, e_layers=4, hidden_d_model=36, seq_layers=2, last_d_model=160, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=28, pos_d_model=16, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=False, bidirectional=False, norm_type='layer', num_heads=4, fc_layer_type='mlp', conv_out_dim=64, feat_conv_kernel=11, skip_connection_mode='full', conv_norm=True, mlp_norm=True, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-full-conv-True-mlp-True-use_pos-False-seq_len-8-lr-0.0099-d-208-hid_d-36-last_d-160-tok_d-4-time_d-28-pos_d-16-e_layers-4-tok_conv_k-9-conv_out_d-64-feat_conv_k-11-dropout-0.14-norm_type-layer-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.00991803561704481, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:22:15,935] Trial 503 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 36, 'token_conv_kernel': 9, 'last_d_model': 160, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 28, 'pos_d_model': 16, 'd_model': 208, 'conv_out_dim': 64, 'e_layers': 4, 'learning_rate': 0.00991803561704481, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': False, 'norm_type': 'layer', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 11, 'skip_connection_mode': 'full', 'conv_norm': True, 'mlp_norm': True, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:22:16,120] The parameter 'use_pos_enc' in trial#507 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:22:16,125] The parameter 'norm_type' in trial#507 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:22:16,145] The parameter 'feat_conv_kernel' in trial#507 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:22:16,152] The parameter 'skip_connection_mode' in trial#507 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:22:16,167] The parameter 'mlp_norm' in trial#507 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=80, n_heads=4, e_layers=3, hidden_d_model=44, seq_layers=2, last_d_model=224, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=24, pos_d_model=20, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=False, bidirectional=False, norm_type='layer', num_heads=4, fc_layer_type='mlp', conv_out_dim=128, feat_conv_kernel=9, skip_connection_mode='full', conv_norm=True, mlp_norm=False, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-full-conv-True-mlp-False-use_pos-False-seq_len-8-lr-0.0174-d-80-hid_d-44-last_d-224-tok_d-4-time_d-24-pos_d-20-e_layers-3-tok_conv_k-9-conv_out_d-128-feat_conv_k-9-dropout-0.14-norm_type-layer-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.017354391506084282, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:22:33,775] Trial 507 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 44, 'token_conv_kernel': 9, 'last_d_model': 224, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 24, 'pos_d_model': 20, 'd_model': 80, 'conv_out_dim': 128, 'e_layers': 3, 'learning_rate': 0.017354391506084282, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': False, 'norm_type': 'layer', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 9, 'skip_connection_mode': 'full', 'conv_norm': True, 'mlp_norm': False, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:22:33,987] The parameter 'use_pos_enc' in trial#515 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:22:33,994] The parameter 'norm_type' in trial#515 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:22:34,011] The parameter 'feat_conv_kernel' in trial#515 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:22:34,017] The parameter 'skip_connection_mode' in trial#515 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:22:34,029] The parameter 'mlp_norm' in trial#515 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=176, n_heads=4, e_layers=2, hidden_d_model=36, seq_layers=2, last_d_model=96, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=28, pos_d_model=24, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.12, use_pos_enc=False, bidirectional=False, norm_type='batch', num_heads=4, fc_layer_type='mlp', conv_out_dim=192, feat_conv_kernel=9, skip_connection_mode='full', conv_norm=True, mlp_norm=True, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-full-conv-True-mlp-True-use_pos-False-seq_len-8-lr-0.0183-d-176-hid_d-36-last_d-96-tok_d-4-time_d-28-pos_d-24-e_layers-2-tok_conv_k-9-conv_out_d-192-feat_conv_k-9-dropout-0.12-norm_type-batch-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.01826075812351569, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:22:59,174] Trial 515 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 36, 'token_conv_kernel': 9, 'last_d_model': 96, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 28, 'pos_d_model': 24, 'd_model': 176, 'conv_out_dim': 192, 'e_layers': 2, 'learning_rate': 0.01826075812351569, 'dropout': 0.12000000000000001, 'combine_type': 'add', 'use_pos_enc': False, 'norm_type': 'batch', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 9, 'skip_connection_mode': 'full', 'conv_norm': True, 'mlp_norm': True, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
Number of finished trials: 519
Best trial:
Value: inf
Params: 
    num_heads: 4
    hidden_d_model: 36
    token_conv_kernel: 9
    last_d_model: 288
    seq_len: 8
    token_d_model: 8
    time_d_model: 12
    pos_d_model: 20
    d_model: 48
    conv_out_dim: 128
    e_layers: 3
    learning_rate: 0.02628870650973661
    dropout: 0.14
    combine_type: add
    use_pos_enc: True
    norm_type: batch
    batch_size: 1024
    train_epochs: 50
    feat_conv_kernel: 9
    skip_connection_mode: full
    conv_norm: True
    mlp_norm: True
    scale_y_type: standard
Top 10 trials saved to /data/Pein/Pytorch/Wind-Power-Prediction/optuna_results/24-08-19-no_time/24-08-19-no_time-farm_66_top10_params.json
Total time taken:  226.0581 seconds,  3.77 minutes
Done!
_size: 1024
    train_epochs: 50
    feat_conv_kernel: 9
    skip_connection_mode: full
    conv_norm: True
    mlp_norm: True
    scale_y_type: standard
Top 10 trials saved to /data/Pein/Pytorch/Wind-Power-Prediction/optuna_results/24-08-19-no_time/24-08-19-no_time-farm_66_top10_params.json
Total time taken:  118.6537 seconds,  1.98 minutes
Done!
