{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np  # noqa\n",
    "import optuna\n",
    "import time\n",
    "import logging  # noqa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from lightgbm import LGBMRegressor, early_stopping, log_evaluation\n",
    "\n",
    "# Enable logging\n",
    "optuna.logging.set_verbosity(optuna.logging.INFO)\n",
    "\n",
    "\n",
    "class TimingCallback:\n",
    "    def __init__(self):\n",
    "        self.start_time = None\n",
    "\n",
    "    def __call__(self, study, trial):\n",
    "        if self.start_time is None:\n",
    "            self.start_time = time.time()\n",
    "        else:\n",
    "            elapsed_time = time.time() - self.start_time\n",
    "            print(f\"Trial {trial.number} finished in {elapsed_time:.2f} seconds.\")\n",
    "            self.start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_dir = \"/data/Pein/Pytorch/Wind-Power-Prediction/data/\"\n",
    "file_name_common = 'farm_92'\n",
    "train_data_selected = pd.read_csv(data_dir + f\"train_{file_name_common}.csv\")\n",
    "test_data_selected = pd.read_csv(data_dir + f\"test_{file_name_common}.csv\")\n",
    "\n",
    "# Define features and target\n",
    "features = [\n",
    "    col\n",
    "    for col in train_data_selected.columns\n",
    "    if col not in [\"time\", \"lead_hour\", \"power\"]\n",
    "]\n",
    "X_train = train_data_selected[features]\n",
    "y_train = train_data_selected[\"power\"]\n",
    "X_test = test_data_selected[features]\n",
    "y_test = test_data_selected[\"power\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (14592, 89), Shape of y_train: (14592,)\n",
      "Shape of X_test: (2880, 89), Shape of y_test: (2880,)\n"
     ]
    }
   ],
   "source": [
    "print(f'Shape of X_train: {X_train.shape}, Shape of y_train: {y_train.shape}')\n",
    "print(f'Shape of X_test: {X_test.shape}, Shape of y_test: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding time features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'time' column to datetime\n",
    "train_data_selected[\"time\"] = pd.to_datetime(train_data_selected[\"time\"])\n",
    "test_data_selected[\"time\"] = pd.to_datetime(test_data_selected[\"time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add time features\n",
    "def add_time_features(df):\n",
    "    df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
    "\n",
    "    # Existing time features\n",
    "    df[\"hour\"] = df[\"time\"].dt.hour\n",
    "    df[\"quarter_hour\"] = df[\"time\"].dt.minute // 15\n",
    "\n",
    "    # New time features\n",
    "    df[\"day\"] = df[\"time\"].dt.day\n",
    "    df[\"day_in_week\"] = df[\"time\"].dt.weekday\n",
    "\n",
    "    # Sine and cosine transformations\n",
    "    df[\"hour_sin\"] = np.sin(2 * np.pi * df[\"hour\"] / 24)\n",
    "    df[\"hour_cos\"] = np.cos(2 * np.pi * df[\"hour\"] / 24)\n",
    "\n",
    "    df[\"quarter_hour_sin\"] = np.sin(2 * np.pi * df[\"quarter_hour\"] / 4)\n",
    "    df[\"quarter_hour_cos\"] = np.cos(2 * np.pi * df[\"quarter_hour\"] / 4)\n",
    "\n",
    "    df[\"day_sin\"] = np.sin(2 * np.pi * df[\"day\"] / 31)\n",
    "    df[\"day_cos\"] = np.cos(2 * np.pi * df[\"day\"] / 31)\n",
    "\n",
    "    df[\"day_in_week_sin\"] = np.sin(2 * np.pi * df[\"day_in_week\"] / 7)\n",
    "    df[\"day_in_week_cos\"] = np.cos(2 * np.pi * df[\"day_in_week\"] / 7)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add time features to both train and test data\n",
    "train_data_selected = add_time_features(train_data_selected)\n",
    "test_data_selected = add_time_features(test_data_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features including the new time features\n",
    "time_features = [\n",
    "    \"hour\",\n",
    "    \"quarter_hour\",\n",
    "    \"day\",\n",
    "    \"day_in_week\",\n",
    "    \"hour_sin\",\n",
    "    \"hour_cos\",\n",
    "    \"quarter_hour_sin\",\n",
    "    \"quarter_hour_cos\",\n",
    "    \"day_sin\",\n",
    "    \"day_cos\",\n",
    "    \"day_in_week_sin\",\n",
    "    \"day_in_week_cos\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = features  + time_features\n",
    "\n",
    "X_train = train_data_selected[all_features]\n",
    "X_test = test_data_selected[all_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(\n",
    "    trial,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test=None,\n",
    "    y_test=None,\n",
    "    use_test_for_validation_flag=False,\n",
    "):\n",
    "    params = {\n",
    "        \"objective\": \"regression\",\n",
    "        \"metric\": \"rmse\",\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 10, 80),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 10, 50),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 500, 1500),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 10, 50),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.3, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"num_threads\": 16,\n",
    "        \"seed\": 42,\n",
    "    }\n",
    "\n",
    "    model = LGBMRegressor(**params)\n",
    "\n",
    "    if use_test_for_validation_flag and X_test is not None and y_test is not None:\n",
    "        X_val_split, y_val_split = X_test, y_test\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=[(X_val_split, y_val_split)],\n",
    "            eval_metric=\"rmse\",\n",
    "            callbacks=[early_stopping(stopping_rounds=15), log_evaluation(period=500)],\n",
    "        )\n",
    "        preds = model.predict(X_val_split)\n",
    "    else:\n",
    "        # Calculate the split index\n",
    "        split_index = int(0.8 * len(X_train))\n",
    "\n",
    "        # Split the data\n",
    "        X_train_split, X_val_split = X_train[:split_index], X_train[split_index:]\n",
    "        y_train_split, y_val_split = y_train[:split_index], y_train[split_index:]\n",
    "        \n",
    "        model.fit(\n",
    "            X_train_split,\n",
    "            y_train_split,\n",
    "            eval_set=[(X_val_split, y_val_split)],\n",
    "            eval_metric=\"rmse\",\n",
    "            callbacks=[early_stopping(stopping_rounds=100), log_evaluation(period=500)],\n",
    "        )\n",
    "        preds = model.predict(X_val_split)\n",
    "\n",
    "    # Calculate RMSE\n",
    "    rmse = root_mean_squared_error(y_val_split, preds)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-10 20:55:10,781] A new study created in memory with name: no-name-01d4eb2a-593a-469f-9c93-88dda283dc4c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002182 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22878\n",
      "[LightGBM] [Info] Number of data points in the train set: 11673, number of used features: 101\n",
      "[LightGBM] [Info] Start training from score 564.183276\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-10 20:55:10,982] Trial 0 finished with value: 360.3341607687539 and parameters: {'num_leaves': 11, 'max_depth': 14, 'learning_rate': 0.063591111825627, 'n_estimators': 1137, 'min_child_samples': 39, 'subsample': 0.5774575092982761, 'colsample_bytree': 0.6723297224232305}. Best is trial 0 with value: 360.3341607687539.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[95]\tvalid_0's rmse: 360.334\n",
      "Best parameters found:  {'num_leaves': 11, 'max_depth': 14, 'learning_rate': 0.063591111825627, 'n_estimators': 1137, 'min_child_samples': 39, 'subsample': 0.5774575092982761, 'colsample_bytree': 0.6723297224232305}\n"
     ]
    }
   ],
   "source": [
    "# Create the study and optimize\n",
    "use_test_for_validation_flag = False\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(\n",
    "    lambda trial: objective(\n",
    "        trial,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        use_test_for_validation_flag=use_test_for_validation_flag,\n",
    "    ),\n",
    "    n_trials=1,\n",
    "    callbacks=[TimingCallback()],\n",
    ")\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = study.best_params\n",
    "print(\"Best parameters found: \", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_params = {\n",
    "#     \"num_leaves\": 70,\n",
    "#     \"max_depth\": 36,\n",
    "#     \"learning_rate\": 0.09304350950671668,\n",
    "#     \"n_estimators\": 1158,\n",
    "#     \"min_child_samples\": 18,\n",
    "#     \"subsample\": 0.579731306036922,\n",
    "#     \"colsample_bytree\": 0.8511910376678277,\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'num_leaves': 11, 'max_depth': 14, 'learning_rate': 0.063591111825627, 'n_estimators': 1137, 'min_child_samples': 39, 'subsample': 0.5774575092982761, 'colsample_bytree': 0.6723297224232305}\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001945 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22879\n",
      "[LightGBM] [Info] Number of data points in the train set: 14592, number of used features: 101\n",
      "[LightGBM] [Info] Start training from score 540.706096\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters found: \", best_params)\n",
    "best_params[\"num_threads\"] = 16\n",
    "\n",
    "# Train the model with the best parameters\n",
    "best_model = LGBMRegressor(**best_params)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Extract feature importances\n",
    "feature_importances = best_model.feature_importances_\n",
    "\n",
    "# Create a DataFrame to hold the features and their importances\n",
    "importance_df = pd.DataFrame(\n",
    "    {\"Feature\": X_train.columns, \"Importance\": feature_importances}\n",
    ")\n",
    "\n",
    "# Sort the DataFrame by importance\n",
    "importance_df = importance_df.sort_values(by=\"Importance\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on the test set: 342.8944\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = root_mean_squared_error(y_test, y_pred)\n",
    "print(f\"RMSE on the test set: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save important_features to csv file\n",
    "if use_test_for_validation_flag:\n",
    "    importance_df.to_csv(\"farm_important_features_use_test_for_validation.csv\", index=False)\n",
    "else:\n",
    "    importance_df.to_csv(\"farm_important_features.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the threshold for feature importance\n",
    "importance_threshold = 0\n",
    "\n",
    "# Select features with importance greater than or equal to the threshold\n",
    "selected_features = importance_df[importance_df[\"Importance\"] >= importance_threshold][\n",
    "    \"Feature\"\n",
    "].tolist()\n",
    "\n",
    "# Separate selected features into original input features and time features\n",
    "selected_input_features = [\n",
    "    feature for feature in selected_features if feature not in time_features\n",
    "]\n",
    "selected_time_features = [\n",
    "    feature for feature in selected_features if feature in time_features\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the filtered features with the power column and time features\n",
    "train_data_final = pd.concat(\n",
    "    [\n",
    "        train_data_selected[\"time\"],\n",
    "        X_train[selected_input_features],\n",
    "        train_data_selected[\"power\"],\n",
    "        train_data_selected[time_features],\n",
    "        train_data_selected[\"lead_hour\"]\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "test_data_final = pd.concat(\n",
    "    [\n",
    "        test_data_selected[\"time\"],\n",
    "        X_test[selected_input_features],\n",
    "        test_data_selected[\"power\"],\n",
    "        test_data_selected[time_features],\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# Ensure the final order of columns\n",
    "final_columns = [\"time\"] + selected_input_features + [\"power\"] + time_features\n",
    "train_data_final = train_data_final[final_columns]\n",
    "test_data_final = test_data_final[final_columns]\n",
    "\n",
    "# Display the new shapes of train_data_final and test_data_final\n",
    "print(\"Shape of train_data_final:\", train_data_final.shape)\n",
    "print(\"Shape of test_data_final:\", test_data_final.shape)\n",
    "print(\"Selected input features:\", selected_input_features)\n",
    "print(f\"Train columns : {train_data_final.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final datasets to CSV\n",
    "file_name_common = 'farm_98_withTime'\n",
    "train_data_final.to_csv(data_dir + f\"train_{file_name_common}.csv\", index=False)\n",
    "test_data_final.to_csv(data_dir + f\"test_{file_name_common}.csv\", index=False)\n",
    "print(\"Final datasets saved to CSV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pein_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
