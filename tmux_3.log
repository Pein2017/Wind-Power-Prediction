[I 2024-08-19 05:19:59,234] Using an existing study with name '24-08-19-no_time-farm_66' instead of creating a new one.
Creating study "24-08-19-no_time-farm_66" with storage "sqlite:////data/Pein/Pytorch/Wind-Power-Prediction/optuna_results/24-08-19-no_time/24-08-19-no_time-farm_66.db?mode=wal"...
Using sampler cma with seed 88136
Using sampler : CmaEsSampler, pruner : <optuna.pruners._median.MedianPruner object at 0x7fdd8a86e860>
  0%|          | 0/24 [00:00<?, ?it/s]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                        0%|          | 0/24 [00:00<?, ?it/s]                                        0%|          | 0/24 [00:00<?, ?it/s]                                        0%|          | 0/24 [00:00<?, ?it/s]                                        0%|          | 0/24 [00:00<?, ?it/s]                                        0%|          | 0/24 [00:00<?, ?it/s]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA A100 80GB PCIe') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 727 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
727 K     Trainable params
0         Non-trainable params
727 K     Total params
2.910     Total estimated model params size (MB)
155       Modules in train mode
0         Modules in eval mode
Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1023, in _run_stage
    self._run_sanity_check()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1052, in _run_sanity_check
    val_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py", line 178, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 128, in run
    batch, batch_idx, dataloader_idx = next(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 133, in __next__
    batch = super().__next__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 60, in __next__
    batch = next(self.iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 341, in __next__
    out = next(self._iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 142, in __next__
    out = next(self.iterators[0])
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1344, in _next_data
    return self._process_data(data)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1370, in _process_data
    data.reraise()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/_utils.py", line 706, in reraise
    raise exception
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 309, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 55, in fetch
    return self.collate_fn(data)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 317, in default_collate
    return collate(batch, collate_fn_map=default_collate_fn_map)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 174, in collate
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 174, in <listcomp>
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 142, in collate
    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 212, in collate_tensor_fn
    storage = elem._typed_storage()._new_shared(numel, device=elem.device)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/storage.py", line 968, in _new_shared
    untyped_storage = torch.UntypedStorage._new_shared(size * self._element_size(), device=device)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/storage.py", line 302, in _new_shared
    return cls._new_using_fd_cpu(size)
RuntimeError: unable to write to file </torch_180839_182869431_0>: No space left on device (28)

                                        0%|          | 0/24 [00:01<?, ?it/s]Best trial: 0. Best value: inf:   0%|          | 0/24 [00:01<?, ?it/s]Best trial: 0. Best value: inf:   4%|▍         | 1/24 [00:01<00:29,  1.29s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                              Best trial: 0. Best value: inf:   4%|▍         | 1/24 [00:11<00:29,  1.29s/it]                                                                              Best trial: 0. Best value: inf:   4%|▍         | 1/24 [00:11<00:29,  1.29s/it]                                                                              Best trial: 0. Best value: inf:   4%|▍         | 1/24 [00:11<00:29,  1.29s/it]                                                                              Best trial: 0. Best value: inf:   4%|▍         | 1/24 [00:11<00:29,  1.29s/it]                                                                              Best trial: 0. Best value: inf:   4%|▍         | 1/24 [00:11<00:29,  1.29s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 360 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
360 K     Trainable params
0         Non-trainable params
360 K     Total params
1.441     Total estimated model params size (MB)
115       Modules in train mode
0         Modules in eval mode
ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1131, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/queues.py", line 114, in get
    raise Empty
_queue.Empty

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1023, in _run_stage
    self._run_sanity_check()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1052, in _run_sanity_check
    val_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py", line 178, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 128, in run
    batch, batch_idx, dataloader_idx = next(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 133, in __next__
    batch = super().__next__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 60, in __next__
    batch = next(self.iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 341, in __next__
    out = next(self._iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 142, in __next__
    out = next(self.iterators[0])
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1327, in _next_data
    idx, data = self._get_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1293, in _get_data
    success, data = self._try_get_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1144, in _try_get_data
    raise RuntimeError(f'DataLoader worker (pid(s) {pids_str}) exited unexpectedly') from e
RuntimeError: DataLoader worker (pid(s) 186912) exited unexpectedly
                                                                              Best trial: 0. Best value: inf:   4%|▍         | 1/24 [00:17<00:29,  1.29s/it]Best trial: 0. Best value: inf:   4%|▍         | 1/24 [00:17<00:29,  1.29s/it]Best trial: 0. Best value: inf:   8%|▊         | 2/24 [00:17<03:43, 10.17s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                              Best trial: 0. Best value: inf:   8%|▊         | 2/24 [00:17<03:43, 10.17s/it]                                                                              Best trial: 0. Best value: inf:   8%|▊         | 2/24 [00:17<03:43, 10.17s/it]                                                                              Best trial: 0. Best value: inf:   8%|▊         | 2/24 [00:17<03:43, 10.17s/it]                                                                              Best trial: 0. Best value: inf:   8%|▊         | 2/24 [00:17<03:43, 10.17s/it]                                                                              Best trial: 0. Best value: inf:   8%|▊         | 2/24 [00:17<03:43, 10.17s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 474 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
474 K     Trainable params
0         Non-trainable params
474 K     Total params
1.898     Total estimated model params size (MB)
155       Modules in train mode
0         Modules in eval mode
ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1023, in _run_stage
    self._run_sanity_check()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1052, in _run_sanity_check
    val_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py", line 178, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 128, in run
    batch, batch_idx, dataloader_idx = next(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 133, in __next__
    batch = super().__next__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 60, in __next__
    batch = next(self.iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 341, in __next__
    out = next(self._iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 142, in __next__
    out = next(self.iterators[0])
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1344, in _next_data
    return self._process_data(data)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1370, in _process_data
    data.reraise()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/_utils.py", line 706, in reraise
    raise exception
RuntimeError: Caught RuntimeError in DataLoader worker process 1.
Original Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 309, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 55, in fetch
    return self.collate_fn(data)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 317, in default_collate
    return collate(batch, collate_fn_map=default_collate_fn_map)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 174, in collate
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 174, in <listcomp>
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 142, in collate
    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 212, in collate_tensor_fn
    storage = elem._typed_storage()._new_shared(numel, device=elem.device)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/storage.py", line 968, in _new_shared
    untyped_storage = torch.UntypedStorage._new_shared(size * self._element_size(), device=device)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/storage.py", line 302, in _new_shared
    return cls._new_using_fd_cpu(size)
RuntimeError: unable to write to file </torch_192061_187706517_0>: No space left on device (28)

                                                                              Best trial: 0. Best value: inf:   8%|▊         | 2/24 [00:23<03:43, 10.17s/it]Best trial: 0. Best value: inf:   8%|▊         | 2/24 [00:23<03:43, 10.17s/it]Best trial: 0. Best value: inf:  12%|█▎        | 3/24 [00:23<02:54,  8.29s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                              Best trial: 0. Best value: inf:  12%|█▎        | 3/24 [00:24<02:54,  8.29s/it]                                                                              Best trial: 0. Best value: inf:  12%|█▎        | 3/24 [00:24<02:54,  8.29s/it]                                                                              Best trial: 0. Best value: inf:  12%|█▎        | 3/24 [00:24<02:54,  8.29s/it]                                                                              Best trial: 0. Best value: inf:  12%|█▎        | 3/24 [00:24<02:54,  8.29s/it]                                                                              Best trial: 0. Best value: inf:  12%|█▎        | 3/24 [00:24<02:54,  8.29s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 718 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
718 K     Trainable params
0         Non-trainable params
718 K     Total params
2.876     Total estimated model params size (MB)
175       Modules in train mode
0         Modules in eval mode
ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1131, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/queues.py", line 114, in get
    raise Empty
_queue.Empty

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1023, in _run_stage
    self._run_sanity_check()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1052, in _run_sanity_check
    val_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py", line 178, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 128, in run
    batch, batch_idx, dataloader_idx = next(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 133, in __next__
    batch = super().__next__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 60, in __next__
    batch = next(self.iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 341, in __next__
    out = next(self._iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 142, in __next__
    out = next(self.iterators[0])
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1327, in _next_data
    idx, data = self._get_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1293, in _get_data
    success, data = self._try_get_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1144, in _try_get_data
    raise RuntimeError(f'DataLoader worker (pid(s) {pids_str}) exited unexpectedly') from e
RuntimeError: DataLoader worker (pid(s) 194510) exited unexpectedly
                                                                              Best trial: 0. Best value: inf:  12%|█▎        | 3/24 [00:29<02:54,  8.29s/it]Best trial: 0. Best value: inf:  12%|█▎        | 3/24 [00:29<02:54,  8.29s/it]Best trial: 0. Best value: inf:  17%|█▋        | 4/24 [00:29<02:29,  7.45s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                              Best trial: 0. Best value: inf:  17%|█▋        | 4/24 [00:30<02:29,  7.45s/it]                                                                              Best trial: 0. Best value: inf:  17%|█▋        | 4/24 [00:30<02:29,  7.45s/it]                                                                              Best trial: 0. Best value: inf:  17%|█▋        | 4/24 [00:30<02:29,  7.45s/it]                                                                              Best trial: 0. Best value: inf:  17%|█▋        | 4/24 [00:30<02:29,  7.45s/it]                                                                              Best trial: 0. Best value: inf:  17%|█▋        | 4/24 [00:30<02:29,  7.45s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 515 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
515 K     Trainable params
0         Non-trainable params
515 K     Total params
2.061     Total estimated model params size (MB)
135       Modules in train mode
0         Modules in eval mode
ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1025, in _run_stage
    self.fit_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 205, in run
    self.advance()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 363, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 140, in run
    self.advance(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 212, in advance
    batch, _, __ = next(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 133, in __next__
    batch = super().__next__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 60, in __next__
    batch = next(self.iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 341, in __next__
    out = next(self._iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 78, in __next__
    out[i] = next(self.iterators[i])
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1344, in _next_data
    return self._process_data(data)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1370, in _process_data
    data.reraise()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/_utils.py", line 706, in reraise
    raise exception
RuntimeError: Caught RuntimeError in DataLoader worker process 3.
Original Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 309, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 55, in fetch
    return self.collate_fn(data)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 317, in default_collate
    return collate(batch, collate_fn_map=default_collate_fn_map)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 174, in collate
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 174, in <listcomp>
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 142, in collate
    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 212, in collate_tensor_fn
    storage = elem._typed_storage()._new_shared(numel, device=elem.device)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/storage.py", line 968, in _new_shared
    untyped_storage = torch.UntypedStorage._new_shared(size * self._element_size(), device=device)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/storage.py", line 302, in _new_shared
    return cls._new_using_fd_cpu(size)
RuntimeError: unable to write to file </torch_197946_1864752924_0>: No space left on device (28)

                                                                              Best trial: 0. Best value: inf:  17%|█▋        | 4/24 [00:31<02:29,  7.45s/it]Best trial: 0. Best value: inf:  17%|█▋        | 4/24 [00:31<02:29,  7.45s/it]Best trial: 0. Best value: inf:  21%|██        | 5/24 [00:31<01:44,  5.47s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                              Best trial: 0. Best value: inf:  21%|██        | 5/24 [00:42<01:44,  5.47s/it]                                                                              Best trial: 0. Best value: inf:  21%|██        | 5/24 [00:42<01:44,  5.47s/it]                                                                              Best trial: 0. Best value: inf:  21%|██        | 5/24 [00:42<01:44,  5.47s/it]                                                                              Best trial: 0. Best value: inf:  21%|██        | 5/24 [00:42<01:44,  5.47s/it]                                                                              Best trial: 0. Best value: inf:  21%|██        | 5/24 [00:42<01:44,  5.47s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 782 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
782 K     Trainable params
0         Non-trainable params
782 K     Total params
3.128     Total estimated model params size (MB)
115       Modules in train mode
0         Modules in eval mode
ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1131, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/queues.py", line 114, in get
    raise Empty
_queue.Empty

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1025, in _run_stage
    self.fit_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 205, in run
    self.advance()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 363, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 140, in run
    self.advance(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 212, in advance
    batch, _, __ = next(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 133, in __next__
    batch = super().__next__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 60, in __next__
    batch = next(self.iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 341, in __next__
    out = next(self._iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 78, in __next__
    out[i] = next(self.iterators[i])
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1327, in _next_data
    idx, data = self._get_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1293, in _get_data
    success, data = self._try_get_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1144, in _try_get_data
    raise RuntimeError(f'DataLoader worker (pid(s) {pids_str}) exited unexpectedly') from e
RuntimeError: DataLoader worker (pid(s) 204141) exited unexpectedly
                                                                              Best trial: 0. Best value: inf:  21%|██        | 5/24 [00:49<01:44,  5.47s/it]Best trial: 0. Best value: inf:  21%|██        | 5/24 [00:49<01:44,  5.47s/it]Best trial: 0. Best value: inf:  25%|██▌       | 6/24 [00:49<02:51,  9.55s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                              Best trial: 0. Best value: inf:  25%|██▌       | 6/24 [00:49<02:51,  9.55s/it]                                                                              Best trial: 0. Best value: inf:  25%|██▌       | 6/24 [00:49<02:51,  9.55s/it]                                                                              Best trial: 0. Best value: inf:  25%|██▌       | 6/24 [00:49<02:51,  9.55s/it]                                                                              Best trial: 0. Best value: inf:  25%|██▌       | 6/24 [00:49<02:51,  9.55s/it]                                                                              Best trial: 0. Best value: inf:  25%|██▌       | 6/24 [00:49<02:51,  9.55s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 339 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
339 K     Trainable params
0         Non-trainable params
339 K     Total params
1.359     Total estimated model params size (MB)
95        Modules in train mode
0         Modules in eval mode
ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1131, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/queues.py", line 114, in get
    raise Empty
_queue.Empty

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1023, in _run_stage
    self._run_sanity_check()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1052, in _run_sanity_check
    val_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py", line 178, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 128, in run
    batch, batch_idx, dataloader_idx = next(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 133, in __next__
    batch = super().__next__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 60, in __next__
    batch = next(self.iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 341, in __next__
    out = next(self._iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 142, in __next__
    out = next(self.iterators[0])
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1327, in _next_data
    idx, data = self._get_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1293, in _get_data
    success, data = self._try_get_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1144, in _try_get_data
    raise RuntimeError(f'DataLoader worker (pid(s) {pids_str}) exited unexpectedly') from e
RuntimeError: DataLoader worker (pid(s) 208272) exited unexpectedly
                                                                              Best trial: 0. Best value: inf:  25%|██▌       | 6/24 [00:55<02:51,  9.55s/it]Best trial: 0. Best value: inf:  25%|██▌       | 6/24 [00:55<02:51,  9.55s/it]Best trial: 0. Best value: inf:  29%|██▉       | 7/24 [00:55<02:21,  8.35s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                              Best trial: 0. Best value: inf:  29%|██▉       | 7/24 [00:55<02:21,  8.35s/it]                                                                              Best trial: 0. Best value: inf:  29%|██▉       | 7/24 [00:55<02:21,  8.35s/it]                                                                              Best trial: 0. Best value: inf:  29%|██▉       | 7/24 [00:55<02:21,  8.35s/it]                                                                              Best trial: 0. Best value: inf:  29%|██▉       | 7/24 [00:55<02:21,  8.35s/it]                                                                              Best trial: 0. Best value: inf:  29%|██▉       | 7/24 [00:55<02:21,  8.35s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 394 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
394 K     Trainable params
0         Non-trainable params
394 K     Total params
1.576     Total estimated model params size (MB)
55        Modules in train mode
0         Modules in eval mode
Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1023, in _run_stage
    self._run_sanity_check()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1052, in _run_sanity_check
    val_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py", line 178, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 113, in run
    self.reset()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 231, in reset
    iter(data_fetcher)  # creates the iterator inside the fetcher
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 104, in __iter__
    super().__iter__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 51, in __iter__
    self.iterator = iter(self.combined_loader)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 351, in __iter__
    iter(iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 155, in __iter__
    self._load_current_iterator()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 173, in _load_current_iterator
    self.iterators = [iter(self.iterables[self._iterator_idx])]
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 440, in __iter__
    return self._get_iterator()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 388, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1020, in __init__
    index_queue = multiprocessing_context.Queue()  # type: ignore[var-annotated]
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/context.py", line 103, in Queue
    return Queue(maxsize, ctx=self.get_context())
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/queues.py", line 48, in __init__
    self._wlock = ctx.Lock()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/context.py", line 68, in Lock
    return Lock(ctx=self.get_context())
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/synchronize.py", line 162, in __init__
    SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/synchronize.py", line 57, in __init__
    sl = self._semlock = _multiprocessing.SemLock(
OSError: [Errno 28] No space left on device
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fde2bd1da20>
Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1477, in __del__
    self._shutdown_workers()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1435, in _shutdown_workers
    if self._persistent_workers or self._workers_status[worker_id]:
AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_workers_status'
                                                                              Best trial: 0. Best value: inf:  29%|██▉       | 7/24 [00:56<02:21,  8.35s/it]Best trial: 0. Best value: inf:  29%|██▉       | 7/24 [00:56<02:21,  8.35s/it]Best trial: 0. Best value: inf:  33%|███▎      | 8/24 [00:56<01:35,  5.97s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                              Best trial: 0. Best value: inf:  33%|███▎      | 8/24 [00:56<01:35,  5.97s/it]                                                                              Best trial: 0. Best value: inf:  33%|███▎      | 8/24 [00:56<01:35,  5.97s/it]                                                                              Best trial: 0. Best value: inf:  33%|███▎      | 8/24 [00:56<01:35,  5.97s/it]                                                                              Best trial: 0. Best value: inf:  33%|███▎      | 8/24 [00:56<01:35,  5.97s/it]                                                                              Best trial: 0. Best value: inf:  33%|███▎      | 8/24 [00:56<01:35,  5.97s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 497 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
497 K     Trainable params
0         Non-trainable params
497 K     Total params
1.990     Total estimated model params size (MB)
75        Modules in train mode
0         Modules in eval mode
Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1023, in _run_stage
    self._run_sanity_check()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1052, in _run_sanity_check
    val_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py", line 178, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 113, in run
    self.reset()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 231, in reset
    iter(data_fetcher)  # creates the iterator inside the fetcher
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 104, in __iter__
    super().__iter__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 51, in __iter__
    self.iterator = iter(self.combined_loader)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 351, in __iter__
    iter(iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 155, in __iter__
    self._load_current_iterator()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 173, in _load_current_iterator
    self.iterators = [iter(self.iterables[self._iterator_idx])]
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 440, in __iter__
    return self._get_iterator()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 388, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1011, in __init__
    self._worker_result_queue = multiprocessing_context.Queue()  # type: ignore[var-annotated]
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/context.py", line 103, in Queue
    return Queue(maxsize, ctx=self.get_context())
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/queues.py", line 43, in __init__
    self._rlock = ctx.Lock()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/context.py", line 68, in Lock
    return Lock(ctx=self.get_context())
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/synchronize.py", line 162, in __init__
    SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/synchronize.py", line 57, in __init__
    sl = self._semlock = _multiprocessing.SemLock(
OSError: [Errno 28] No space left on device
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fde2bd1da20>
Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1477, in __del__
    self._shutdown_workers()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1408, in _shutdown_workers
    if not self._shutdown:
AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_shutdown'
                                                                              Best trial: 0. Best value: inf:  33%|███▎      | 8/24 [00:56<01:35,  5.97s/it]Best trial: 0. Best value: inf:  33%|███▎      | 8/24 [00:56<01:35,  5.97s/it]Best trial: 0. Best value: inf:  38%|███▊      | 9/24 [00:56<01:04,  4.29s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                              Best trial: 0. Best value: inf:  38%|███▊      | 9/24 [00:56<01:04,  4.29s/it]                                                                              Best trial: 0. Best value: inf:  38%|███▊      | 9/24 [00:56<01:04,  4.29s/it]                                                                              Best trial: 0. Best value: inf:  38%|███▊      | 9/24 [00:56<01:04,  4.29s/it]                                                                              Best trial: 0. Best value: inf:  38%|███▊      | 9/24 [00:56<01:04,  4.29s/it]                                                                              Best trial: 0. Best value: inf:  38%|███▊      | 9/24 [00:56<01:04,  4.29s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 620 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
620 K     Trainable params
0         Non-trainable params
620 K     Total params
2.482     Total estimated model params size (MB)
115       Modules in train mode
0         Modules in eval mode
ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1023, in _run_stage
    self._run_sanity_check()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1052, in _run_sanity_check
    val_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py", line 178, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 128, in run
    batch, batch_idx, dataloader_idx = next(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 133, in __next__
    batch = super().__next__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 60, in __next__
    batch = next(self.iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 341, in __next__
    out = next(self._iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 142, in __next__
    out = next(self.iterators[0])
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1344, in _next_data
    return self._process_data(data)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1370, in _process_data
    data.reraise()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/_utils.py", line 706, in reraise
    raise exception
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 309, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 55, in fetch
    return self.collate_fn(data)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 317, in default_collate
    return collate(batch, collate_fn_map=default_collate_fn_map)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 174, in collate
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 174, in <listcomp>
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 142, in collate
    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 212, in collate_tensor_fn
    storage = elem._typed_storage()._new_shared(numel, device=elem.device)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/storage.py", line 968, in _new_shared
    untyped_storage = torch.UntypedStorage._new_shared(size * self._element_size(), device=device)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/storage.py", line 302, in _new_shared
    return cls._new_using_fd_cpu(size)
RuntimeError: unable to write to file </torch_213929_1868094596_0>: No space left on device (28)

                                                                              Best trial: 0. Best value: inf:  38%|███▊      | 9/24 [00:57<01:04,  4.29s/it]Best trial: 0. Best value: inf:  38%|███▊      | 9/24 [00:57<01:04,  4.29s/it]Best trial: 0. Best value: inf:  42%|████▏     | 10/24 [00:57<00:46,  3.35s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                               Best trial: 0. Best value: inf:  42%|████▏     | 10/24 [01:03<00:46,  3.35s/it]                                                                               Best trial: 0. Best value: inf:  42%|████▏     | 10/24 [01:03<00:46,  3.35s/it]                                                                               Best trial: 0. Best value: inf:  42%|████▏     | 10/24 [01:03<00:46,  3.35s/it]                                                                               Best trial: 0. Best value: inf:  42%|████▏     | 10/24 [01:03<00:46,  3.35s/it]                                                                               Best trial: 0. Best value: inf:  42%|████▏     | 10/24 [01:03<00:46,  3.35s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 630 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
630 K     Trainable params
0         Non-trainable params
630 K     Total params
2.522     Total estimated model params size (MB)
115       Modules in train mode
0         Modules in eval mode
ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1023, in _run_stage
    self._run_sanity_check()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1052, in _run_sanity_check
    val_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py", line 178, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 128, in run
    batch, batch_idx, dataloader_idx = next(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 133, in __next__
    batch = super().__next__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 60, in __next__
    batch = next(self.iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 341, in __next__
    out = next(self._iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 142, in __next__
    out = next(self.iterators[0])
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1344, in _next_data
    return self._process_data(data)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1370, in _process_data
    data.reraise()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/_utils.py", line 706, in reraise
    raise exception
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 309, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 55, in fetch
    return self.collate_fn(data)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 317, in default_collate
    return collate(batch, collate_fn_map=default_collate_fn_map)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 174, in collate
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 174, in <listcomp>
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 142, in collate
    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 212, in collate_tensor_fn
    storage = elem._typed_storage()._new_shared(numel, device=elem.device)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/storage.py", line 968, in _new_shared
    untyped_storage = torch.UntypedStorage._new_shared(size * self._element_size(), device=device)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/storage.py", line 302, in _new_shared
    return cls._new_using_fd_cpu(size)
RuntimeError: unable to write to file </torch_216274_892132855_3>: No space left on device (28)

                                                                               Best trial: 0. Best value: inf:  42%|████▏     | 10/24 [01:08<00:46,  3.35s/it]Best trial: 0. Best value: inf:  42%|████▏     | 10/24 [01:08<00:46,  3.35s/it]Best trial: 0. Best value: inf:  46%|████▌     | 11/24 [01:08<01:13,  5.64s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                               Best trial: 0. Best value: inf:  46%|████▌     | 11/24 [01:08<01:13,  5.64s/it]                                                                               Best trial: 0. Best value: inf:  46%|████▌     | 11/24 [01:08<01:13,  5.64s/it]                                                                               Best trial: 0. Best value: inf:  46%|████▌     | 11/24 [01:08<01:13,  5.64s/it]                                                                               Best trial: 0. Best value: inf:  46%|████▌     | 11/24 [01:08<01:13,  5.64s/it]                                                                               Best trial: 0. Best value: inf:  46%|████▌     | 11/24 [01:08<01:13,  5.64s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 542 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
542 K     Trainable params
0         Non-trainable params
542 K     Total params
2.169     Total estimated model params size (MB)
115       Modules in train mode
0         Modules in eval mode
Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1025, in _run_stage
    self.fit_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 205, in run
    self.advance()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 363, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 140, in run
    self.advance(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 212, in advance
    batch, _, __ = next(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 133, in __next__
    batch = super().__next__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 60, in __next__
    batch = next(self.iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 341, in __next__
    out = next(self._iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 78, in __next__
    out[i] = next(self.iterators[i])
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1344, in _next_data
    return self._process_data(data)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1370, in _process_data
    data.reraise()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/_utils.py", line 706, in reraise
    raise exception
RuntimeError: Caught RuntimeError in DataLoader worker process 2.
Original Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 309, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 55, in fetch
    return self.collate_fn(data)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 317, in default_collate
    return collate(batch, collate_fn_map=default_collate_fn_map)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 174, in collate
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 174, in <listcomp>
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 142, in collate
    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 212, in collate_tensor_fn
    storage = elem._typed_storage()._new_shared(numel, device=elem.device)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/storage.py", line 968, in _new_shared
    untyped_storage = torch.UntypedStorage._new_shared(size * self._element_size(), device=device)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/storage.py", line 302, in _new_shared
    return cls._new_using_fd_cpu(size)
RuntimeError: unable to write to file </torch_219743_181461010_0>: No space left on device (28)

                                                                               Best trial: 0. Best value: inf:  46%|████▌     | 11/24 [01:10<01:13,  5.64s/it]Best trial: 0. Best value: inf:  46%|████▌     | 11/24 [01:10<01:13,  5.64s/it]Best trial: 0. Best value: inf:  50%|█████     | 12/24 [01:10<00:52,  4.36s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                               Best trial: 0. Best value: inf:  50%|█████     | 12/24 [01:10<00:52,  4.36s/it]                                                                               Best trial: 0. Best value: inf:  50%|█████     | 12/24 [01:10<00:52,  4.36s/it]                                                                               Best trial: 0. Best value: inf:  50%|█████     | 12/24 [01:10<00:52,  4.36s/it]                                                                               Best trial: 0. Best value: inf:  50%|█████     | 12/24 [01:10<00:52,  4.36s/it]                                                                               Best trial: 0. Best value: inf:  50%|█████     | 12/24 [01:10<00:52,  4.36s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 477 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
477 K     Trainable params
0         Non-trainable params
477 K     Total params
1.908     Total estimated model params size (MB)
95        Modules in train mode
0         Modules in eval mode
ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1025, in _run_stage
    self.fit_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 205, in run
    self.advance()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 363, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 140, in run
    self.advance(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 212, in advance
    batch, _, __ = next(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 133, in __next__
    batch = super().__next__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 60, in __next__
    batch = next(self.iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 341, in __next__
    out = next(self._iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 78, in __next__
    out[i] = next(self.iterators[i])
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1344, in _next_data
    return self._process_data(data)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1370, in _process_data
    data.reraise()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/_utils.py", line 706, in reraise
    raise exception
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 309, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 55, in fetch
    return self.collate_fn(data)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 317, in default_collate
    return collate(batch, collate_fn_map=default_collate_fn_map)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 174, in collate
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 174, in <listcomp>
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 142, in collate
    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 212, in collate_tensor_fn
    storage = elem._typed_storage()._new_shared(numel, device=elem.device)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/storage.py", line 968, in _new_shared
    untyped_storage = torch.UntypedStorage._new_shared(size * self._element_size(), device=device)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/storage.py", line 302, in _new_shared
    return cls._new_using_fd_cpu(size)
RuntimeError: unable to write to file </torch_222099_2794088786_0>: No space left on device (28)

                                                                               Best trial: 0. Best value: inf:  50%|█████     | 12/24 [01:23<00:52,  4.36s/it]Best trial: 0. Best value: inf:  50%|█████     | 12/24 [01:23<00:52,  4.36s/it]Best trial: 0. Best value: inf:  54%|█████▍    | 13/24 [01:23<01:16,  6.93s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                               Best trial: 0. Best value: inf:  54%|█████▍    | 13/24 [01:23<01:16,  6.93s/it]                                                                               Best trial: 0. Best value: inf:  54%|█████▍    | 13/24 [01:23<01:16,  6.93s/it]                                                                               Best trial: 0. Best value: inf:  54%|█████▍    | 13/24 [01:23<01:16,  6.93s/it]                                                                               Best trial: 0. Best value: inf:  54%|█████▍    | 13/24 [01:23<01:16,  6.93s/it]                                                                               Best trial: 0. Best value: inf:  54%|█████▍    | 13/24 [01:23<01:16,  6.93s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 658 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
658 K     Trainable params
0         Non-trainable params
658 K     Total params
2.635     Total estimated model params size (MB)
115       Modules in train mode
0         Modules in eval mode
ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1131, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/queues.py", line 114, in get
    raise Empty
_queue.Empty

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1025, in _run_stage
    self.fit_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 205, in run
    self.advance()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 363, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 140, in run
    self.advance(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 212, in advance
    batch, _, __ = next(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 133, in __next__
    batch = super().__next__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 60, in __next__
    batch = next(self.iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 341, in __next__
    out = next(self._iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 78, in __next__
    out[i] = next(self.iterators[i])
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1327, in _next_data
    idx, data = self._get_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1293, in _get_data
    success, data = self._try_get_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1144, in _try_get_data
    raise RuntimeError(f'DataLoader worker (pid(s) {pids_str}) exited unexpectedly') from e
RuntimeError: DataLoader worker (pid(s) 226587, 226644) exited unexpectedly
                                                                               Best trial: 0. Best value: inf:  54%|█████▍    | 13/24 [01:29<01:16,  6.93s/it]Best trial: 0. Best value: inf:  54%|█████▍    | 13/24 [01:29<01:16,  6.93s/it]Best trial: 0. Best value: inf:  58%|█████▊    | 14/24 [01:29<01:08,  6.85s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                               Best trial: 0. Best value: inf:  58%|█████▊    | 14/24 [01:29<01:08,  6.85s/it]                                                                               Best trial: 0. Best value: inf:  58%|█████▊    | 14/24 [01:29<01:08,  6.85s/it]                                                                               Best trial: 0. Best value: inf:  58%|█████▊    | 14/24 [01:29<01:08,  6.85s/it]                                                                               Best trial: 0. Best value: inf:  58%|█████▊    | 14/24 [01:29<01:08,  6.85s/it]                                                                               Best trial: 0. Best value: inf:  58%|█████▊    | 14/24 [01:29<01:08,  6.85s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 672 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
672 K     Trainable params
0         Non-trainable params
672 K     Total params
2.691     Total estimated model params size (MB)
115       Modules in train mode
0         Modules in eval mode
Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1023, in _run_stage
    self._run_sanity_check()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1052, in _run_sanity_check
    val_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py", line 178, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 113, in run
    self.reset()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 231, in reset
    iter(data_fetcher)  # creates the iterator inside the fetcher
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 104, in __iter__
    super().__iter__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 51, in __iter__
    self.iterator = iter(self.combined_loader)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 351, in __iter__
    iter(iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 155, in __iter__
    self._load_current_iterator()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 173, in _load_current_iterator
    self.iterators = [iter(self.iterables[self._iterator_idx])]
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 440, in __iter__
    return self._get_iterator()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 388, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1011, in __init__
    self._worker_result_queue = multiprocessing_context.Queue()  # type: ignore[var-annotated]
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/context.py", line 103, in Queue
    return Queue(maxsize, ctx=self.get_context())
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/queues.py", line 43, in __init__
    self._rlock = ctx.Lock()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/context.py", line 68, in Lock
    return Lock(ctx=self.get_context())
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/synchronize.py", line 162, in __init__
    SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/synchronize.py", line 57, in __init__
    sl = self._semlock = _multiprocessing.SemLock(
OSError: [Errno 28] No space left on device
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fde2bd1da20>
Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1477, in __del__
    self._shutdown_workers()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1408, in _shutdown_workers
    if not self._shutdown:
AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_shutdown'
                                                                               Best trial: 0. Best value: inf:  58%|█████▊    | 14/24 [01:30<01:08,  6.85s/it]Best trial: 0. Best value: inf:  58%|█████▊    | 14/24 [01:30<01:08,  6.85s/it]Best trial: 0. Best value: inf:  62%|██████▎   | 15/24 [01:30<00:44,  4.96s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                               Best trial: 0. Best value: inf:  62%|██████▎   | 15/24 [01:30<00:44,  4.96s/it]                                                                               Best trial: 0. Best value: inf:  62%|██████▎   | 15/24 [01:30<00:44,  4.96s/it]                                                                               Best trial: 0. Best value: inf:  62%|██████▎   | 15/24 [01:30<00:44,  4.96s/it]                                                                               Best trial: 0. Best value: inf:  62%|██████▎   | 15/24 [01:30<00:44,  4.96s/it]                                                                               Best trial: 0. Best value: inf:  62%|██████▎   | 15/24 [01:30<00:44,  4.96s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 608 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
608 K     Trainable params
0         Non-trainable params
608 K     Total params
2.432     Total estimated model params size (MB)
55        Modules in train mode
0         Modules in eval mode
ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1131, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/queues.py", line 114, in get
    raise Empty
_queue.Empty

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1023, in _run_stage
    self._run_sanity_check()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1052, in _run_sanity_check
    val_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py", line 178, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 128, in run
    batch, batch_idx, dataloader_idx = next(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 133, in __next__
    batch = super().__next__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 60, in __next__
    batch = next(self.iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 341, in __next__
    out = next(self._iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 142, in __next__
    out = next(self.iterators[0])
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1327, in _next_data
    idx, data = self._get_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1293, in _get_data
    success, data = self._try_get_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1144, in _try_get_data
    raise RuntimeError(f'DataLoader worker (pid(s) {pids_str}) exited unexpectedly') from e
RuntimeError: DataLoader worker (pid(s) 230113) exited unexpectedly
                                                                               Best trial: 0. Best value: inf:  62%|██████▎   | 15/24 [01:36<00:44,  4.96s/it]Best trial: 0. Best value: inf:  62%|██████▎   | 15/24 [01:36<00:44,  4.96s/it]Best trial: 0. Best value: inf:  67%|██████▋   | 16/24 [01:36<00:41,  5.25s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                               Best trial: 0. Best value: inf:  67%|██████▋   | 16/24 [01:36<00:41,  5.25s/it]                                                                               Best trial: 0. Best value: inf:  67%|██████▋   | 16/24 [01:36<00:41,  5.25s/it]                                                                               Best trial: 0. Best value: inf:  67%|██████▋   | 16/24 [01:36<00:41,  5.25s/it]                                                                               Best trial: 0. Best value: inf:  67%|██████▋   | 16/24 [01:36<00:41,  5.25s/it]                                                                               Best trial: 0. Best value: inf:  67%|██████▋   | 16/24 [01:36<00:41,  5.25s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 488 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
488 K     Trainable params
0         Non-trainable params
488 K     Total params
1.956     Total estimated model params size (MB)
75        Modules in train mode
0         Modules in eval mode
Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1023, in _run_stage
    self._run_sanity_check()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1052, in _run_sanity_check
    val_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py", line 178, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 113, in run
    self.reset()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 231, in reset
    iter(data_fetcher)  # creates the iterator inside the fetcher
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 104, in __iter__
    super().__iter__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 51, in __iter__
    self.iterator = iter(self.combined_loader)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 351, in __iter__
    iter(iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 155, in __iter__
    self._load_current_iterator()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 173, in _load_current_iterator
    self.iterators = [iter(self.iterables[self._iterator_idx])]
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 440, in __iter__
    return self._get_iterator()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 388, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1011, in __init__
    self._worker_result_queue = multiprocessing_context.Queue()  # type: ignore[var-annotated]
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/context.py", line 103, in Queue
    return Queue(maxsize, ctx=self.get_context())
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/queues.py", line 43, in __init__
    self._rlock = ctx.Lock()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/context.py", line 68, in Lock
    return Lock(ctx=self.get_context())
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/synchronize.py", line 162, in __init__
    SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/synchronize.py", line 57, in __init__
    sl = self._semlock = _multiprocessing.SemLock(
OSError: [Errno 28] No space left on device
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fde2bd1da20>
Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1477, in __del__
    self._shutdown_workers()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1408, in _shutdown_workers
    if not self._shutdown:
AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_shutdown'
                                                                               Best trial: 0. Best value: inf:  67%|██████▋   | 16/24 [01:36<00:41,  5.25s/it]Best trial: 0. Best value: inf:  67%|██████▋   | 16/24 [01:36<00:41,  5.25s/it]Best trial: 0. Best value: inf:  71%|███████   | 17/24 [01:36<00:26,  3.83s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                               Best trial: 0. Best value: inf:  71%|███████   | 17/24 [01:36<00:26,  3.83s/it]                                                                               Best trial: 0. Best value: inf:  71%|███████   | 17/24 [01:36<00:26,  3.83s/it]                                                                               Best trial: 0. Best value: inf:  71%|███████   | 17/24 [01:36<00:26,  3.83s/it]                                                                               Best trial: 0. Best value: inf:  71%|███████   | 17/24 [01:36<00:26,  3.83s/it]                                                                               Best trial: 0. Best value: inf:  71%|███████   | 17/24 [01:37<00:26,  3.83s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 424 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
424 K     Trainable params
0         Non-trainable params
424 K     Total params
1.697     Total estimated model params size (MB)
75        Modules in train mode
0         Modules in eval mode
ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1131, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/queues.py", line 114, in get
    raise Empty
_queue.Empty

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1023, in _run_stage
    self._run_sanity_check()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1052, in _run_sanity_check
    val_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py", line 178, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 128, in run
    batch, batch_idx, dataloader_idx = next(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 133, in __next__
    batch = super().__next__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 60, in __next__
    batch = next(self.iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 341, in __next__
    out = next(self._iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 142, in __next__
    out = next(self.iterators[0])
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1327, in _next_data
    idx, data = self._get_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1293, in _get_data
    success, data = self._try_get_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1144, in _try_get_data
    raise RuntimeError(f'DataLoader worker (pid(s) {pids_str}) exited unexpectedly') from e
RuntimeError: DataLoader worker (pid(s) 233128) exited unexpectedly
                                                                               Best trial: 0. Best value: inf:  71%|███████   | 17/24 [01:42<00:26,  3.83s/it]Best trial: 0. Best value: inf:  71%|███████   | 17/24 [01:42<00:26,  3.83s/it]Best trial: 0. Best value: inf:  75%|███████▌  | 18/24 [01:42<00:27,  4.52s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                               Best trial: 0. Best value: inf:  75%|███████▌  | 18/24 [01:43<00:27,  4.52s/it]                                                                               Best trial: 0. Best value: inf:  75%|███████▌  | 18/24 [01:43<00:27,  4.52s/it]                                                                               Best trial: 0. Best value: inf:  75%|███████▌  | 18/24 [01:43<00:27,  4.52s/it]                                                                               Best trial: 0. Best value: inf:  75%|███████▌  | 18/24 [01:43<00:27,  4.52s/it]                                                                               Best trial: 0. Best value: inf:  75%|███████▌  | 18/24 [01:43<00:27,  4.52s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 824 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
824 K     Trainable params
0         Non-trainable params
824 K     Total params
3.298     Total estimated model params size (MB)
95        Modules in train mode
0         Modules in eval mode
ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1131, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/queues.py", line 114, in get
    raise Empty
_queue.Empty

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1023, in _run_stage
    self._run_sanity_check()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1052, in _run_sanity_check
    val_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py", line 178, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 128, in run
    batch, batch_idx, dataloader_idx = next(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 133, in __next__
    batch = super().__next__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 60, in __next__
    batch = next(self.iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 341, in __next__
    out = next(self._iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 142, in __next__
    out = next(self.iterators[0])
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1327, in _next_data
    idx, data = self._get_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1293, in _get_data
    success, data = self._try_get_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1144, in _try_get_data
    raise RuntimeError(f'DataLoader worker (pid(s) {pids_str}) exited unexpectedly') from e
RuntimeError: DataLoader worker (pid(s) 235073, 235074) exited unexpectedly
                                                                               Best trial: 0. Best value: inf:  75%|███████▌  | 18/24 [01:48<00:27,  4.52s/it]Best trial: 0. Best value: inf:  75%|███████▌  | 18/24 [01:48<00:27,  4.52s/it]Best trial: 0. Best value: inf:  79%|███████▉  | 19/24 [01:48<00:24,  4.91s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                               Best trial: 0. Best value: inf:  79%|███████▉  | 19/24 [01:48<00:24,  4.91s/it]                                                                               Best trial: 0. Best value: inf:  79%|███████▉  | 19/24 [01:48<00:24,  4.91s/it]                                                                               Best trial: 0. Best value: inf:  79%|███████▉  | 19/24 [01:48<00:24,  4.91s/it]                                                                               Best trial: 0. Best value: inf:  79%|███████▉  | 19/24 [01:48<00:24,  4.91s/it]                                                                               Best trial: 0. Best value: inf:  79%|███████▉  | 19/24 [01:48<00:24,  4.91s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 651 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
651 K     Trainable params
0         Non-trainable params
651 K     Total params
2.605     Total estimated model params size (MB)
135       Modules in train mode
0         Modules in eval mode
Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1025, in _run_stage
    self.fit_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 197, in run
    self.setup_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 263, in setup_data
    iter(self._data_fetcher)  # creates the iterator inside the fetcher
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 104, in __iter__
    super().__iter__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 51, in __iter__
    self.iterator = iter(self.combined_loader)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 351, in __iter__
    iter(iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 92, in __iter__
    super().__iter__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 43, in __iter__
    self.iterators = [iter(iterable) for iterable in self.iterables]
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 43, in <listcomp>
    self.iterators = [iter(iterable) for iterable in self.iterables]
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 440, in __iter__
    return self._get_iterator()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 388, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1020, in __init__
    index_queue = multiprocessing_context.Queue()  # type: ignore[var-annotated]
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/context.py", line 103, in Queue
    return Queue(maxsize, ctx=self.get_context())
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/queues.py", line 43, in __init__
    self._rlock = ctx.Lock()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/context.py", line 68, in Lock
    return Lock(ctx=self.get_context())
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/synchronize.py", line 162, in __init__
    SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/synchronize.py", line 57, in __init__
    sl = self._semlock = _multiprocessing.SemLock(
OSError: [Errno 28] No space left on device

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 68, in _call_and_handle_interrupt
    trainer._teardown()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1008, in _teardown
    loop.teardown()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 411, in teardown
    self._data_fetcher.teardown()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 79, in teardown
    self.reset()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 141, in reset
    super().reset()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 75, in reset
    self.length = sized_len(self.combined_loader)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/utilities/data.py", line 51, in sized_len
    length = len(dataloader)  # type: ignore [arg-type]
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 358, in __len__
    raise RuntimeError("Please call `iter(combined_loader)` first.")
RuntimeError: Please call `iter(combined_loader)` first.
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fde2bd1da20>
Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1477, in __del__
    self._shutdown_workers()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1435, in _shutdown_workers
[rank: 0] Received SIGTERM: 15
    if self._persistent_workers or self._workers_status[worker_id]:
AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_workers_status'
                                                                               Best trial: 0. Best value: inf:  79%|███████▉  | 19/24 [01:49<00:24,  4.91s/it]Best trial: 0. Best value: inf:  79%|███████▉  | 19/24 [01:49<00:24,  4.91s/it]Best trial: 0. Best value: inf:  83%|████████▎ | 20/24 [01:49<00:15,  3.80s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                               Best trial: 0. Best value: inf:  83%|████████▎ | 20/24 [01:50<00:15,  3.80s/it]                                                                               Best trial: 0. Best value: inf:  83%|████████▎ | 20/24 [01:50<00:15,  3.80s/it]                                                                               Best trial: 0. Best value: inf:  83%|████████▎ | 20/24 [01:50<00:15,  3.80s/it]                                                                               Best trial: 0. Best value: inf:  83%|████████▎ | 20/24 [01:50<00:15,  3.80s/it]                                                                               Best trial: 0. Best value: inf:  83%|████████▎ | 20/24 [01:50<00:15,  3.80s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 537 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
537 K     Trainable params
0         Non-trainable params
537 K     Total params
2.149     Total estimated model params size (MB)
75        Modules in train mode
0         Modules in eval mode
ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1023, in _run_stage
    self._run_sanity_check()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1052, in _run_sanity_check
    val_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py", line 178, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 128, in run
    batch, batch_idx, dataloader_idx = next(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 133, in __next__
    batch = super().__next__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 60, in __next__
    batch = next(self.iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 341, in __next__
    out = next(self._iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 142, in __next__
    out = next(self.iterators[0])
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1344, in _next_data
    return self._process_data(data)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1370, in _process_data
    data.reraise()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/_utils.py", line 706, in reraise
    raise exception
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 309, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 55, in fetch
    return self.collate_fn(data)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 317, in default_collate
    return collate(batch, collate_fn_map=default_collate_fn_map)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 174, in collate
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 174, in <listcomp>
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 142, in collate
    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 212, in collate_tensor_fn
    storage = elem._typed_storage()._new_shared(numel, device=elem.device)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/storage.py", line 968, in _new_shared
    untyped_storage = torch.UntypedStorage._new_shared(size * self._element_size(), device=device)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/storage.py", line 302, in _new_shared
    return cls._new_using_fd_cpu(size)
RuntimeError: unable to write to file </torch_238929_3199213074_0>: No space left on device (28)

                                                                               Best trial: 0. Best value: inf:  83%|████████▎ | 20/24 [02:01<00:15,  3.80s/it]Best trial: 0. Best value: inf:  83%|████████▎ | 20/24 [02:01<00:15,  3.80s/it]Best trial: 0. Best value: inf:  88%|████████▊ | 21/24 [02:01<00:18,  6.01s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                               Best trial: 0. Best value: inf:  88%|████████▊ | 21/24 [02:01<00:18,  6.01s/it]                                                                               Best trial: 0. Best value: inf:  88%|████████▊ | 21/24 [02:01<00:18,  6.01s/it]                                                                               Best trial: 0. Best value: inf:  88%|████████▊ | 21/24 [02:01<00:18,  6.01s/it]                                                                               Best trial: 0. Best value: inf:  88%|████████▊ | 21/24 [02:01<00:18,  6.01s/it]                                                                               Best trial: 0. Best value: inf:  88%|████████▊ | 21/24 [02:01<00:18,  6.01s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 525 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
525 K     Trainable params
0         Non-trainable params
525 K     Total params
2.103     Total estimated model params size (MB)
95        Modules in train mode
0         Modules in eval mode
ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1131, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/queues.py", line 114, in get
    raise Empty
_queue.Empty

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1023, in _run_stage
    self._run_sanity_check()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1052, in _run_sanity_check
    val_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py", line 178, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 128, in run
    batch, batch_idx, dataloader_idx = next(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 133, in __next__
    batch = super().__next__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 60, in __next__
    batch = next(self.iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 341, in __next__
    out = next(self._iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 142, in __next__
    out = next(self.iterators[0])
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1327, in _next_data
    idx, data = self._get_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1293, in _get_data
    success, data = self._try_get_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1144, in _try_get_data
    raise RuntimeError(f'DataLoader worker (pid(s) {pids_str}) exited unexpectedly') from e
RuntimeError: DataLoader worker (pid(s) 241763) exited unexpectedly
                                                                               Best trial: 0. Best value: inf:  88%|████████▊ | 21/24 [02:06<00:18,  6.01s/it]Best trial: 0. Best value: inf:  88%|████████▊ | 21/24 [02:06<00:18,  6.01s/it]Best trial: 0. Best value: inf:  92%|█████████▏| 22/24 [02:06<00:11,  5.98s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                               Best trial: 0. Best value: inf:  92%|█████████▏| 22/24 [02:07<00:11,  5.98s/it]                                                                               Best trial: 0. Best value: inf:  92%|█████████▏| 22/24 [02:07<00:11,  5.98s/it]                                                                               Best trial: 0. Best value: inf:  92%|█████████▏| 22/24 [02:07<00:11,  5.98s/it]                                                                               Best trial: 0. Best value: inf:  92%|█████████▏| 22/24 [02:07<00:11,  5.98s/it]                                                                               Best trial: 0. Best value: inf:  92%|█████████▏| 22/24 [02:07<00:11,  5.98s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 599 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
599 K     Trainable params
0         Non-trainable params
599 K     Total params
2.397     Total estimated model params size (MB)
95        Modules in train mode
0         Modules in eval mode
ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1025, in _run_stage
    self.fit_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 205, in run
    self.advance()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 363, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 140, in run
    self.advance(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 212, in advance
    batch, _, __ = next(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 133, in __next__
    batch = super().__next__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 60, in __next__
    batch = next(self.iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 341, in __next__
    out = next(self._iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 78, in __next__
    out[i] = next(self.iterators[i])
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1344, in _next_data
    return self._process_data(data)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1370, in _process_data
    data.reraise()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/_utils.py", line 706, in reraise
    raise exception
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 309, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 55, in fetch
    return self.collate_fn(data)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 317, in default_collate
    return collate(batch, collate_fn_map=default_collate_fn_map)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 174, in collate
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 174, in <listcomp>
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 142, in collate
    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 212, in collate_tensor_fn
    storage = elem._typed_storage()._new_shared(numel, device=elem.device)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/storage.py", line 968, in _new_shared
    untyped_storage = torch.UntypedStorage._new_shared(size * self._element_size(), device=device)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/storage.py", line 302, in _new_shared
    return cls._new_using_fd_cpu(size)
RuntimeError: unable to write to file </torch_243718_722569231_0>: No space left on device (28)

                                                                               Best trial: 0. Best value: inf:  92%|█████████▏| 22/24 [02:08<00:11,  5.98s/it]ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 Best trial: 0. Best value: inf:  92%|█████████▏| 22/24 [02:08<00:11,  5.98s/it]Best trial: 0. Best value: inf:  96%|█████████▌| 23/24 [02:08<00:04,  4.60s/it]/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 8] and step=8, but the range is not divisible by `step`. It will be replaced by [4, 4].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [32, 512] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 480].
  warnings.warn(
/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [16, 256] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 240].
  warnings.warn(
                                                                               Best trial: 0. Best value: inf:  96%|█████████▌| 23/24 [02:18<00:04,  4.60s/it]                                                                               Best trial: 0. Best value: inf:  96%|█████████▌| 23/24 [02:18<00:04,  4.60s/it]                                                                               Best trial: 0. Best value: inf:  96%|█████████▌| 23/24 [02:18<00:04,  4.60s/it]                                                                               Best trial: 0. Best value: inf:  96%|█████████▌| 23/24 [02:18<00:04,  4.60s/it]                                                                               Best trial: 0. Best value: inf:  96%|█████████▌| 23/24 [02:18<00:04,  4.60s/it]Seed set to 17
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 671 K  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
671 K     Trainable params
0         Non-trainable params
671 K     Total params
2.684     Total estimated model params size (MB)
55        Modules in train mode
0         Modules in eval mode
ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1131, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/multiprocessing/queues.py", line 114, in get
    raise Empty
_queue.Empty

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 279, in objective
    training_duration = run_training(
  File "/data/Pein/Pytorch/Wind-Power-Prediction/run_scripts/run_optuna.py", line 194, in run_training
    trainer.fit(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1023, in _run_stage
    self._run_sanity_check()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1052, in _run_sanity_check
    val_loop.run()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py", line 178, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 128, in run
    batch, batch_idx, dataloader_idx = next(data_fetcher)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 133, in __next__
    batch = super().__next__()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 60, in __next__
    batch = next(self.iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 341, in __next__
    out = next(self._iterator)
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 142, in __next__
    out = next(self.iterators[0])
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1327, in _next_data
    idx, data = self._get_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1293, in _get_data
    success, data = self._try_get_data()
  File "/root/miniconda3/envs/Pein_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1144, in _try_get_data
    raise RuntimeError(f'DataLoader worker (pid(s) {pids_str}) exited unexpectedly') from e
RuntimeError: DataLoader worker (pid(s) 249451) exited unexpectedly
                                                                               Best trial: 0. Best value: inf:  96%|█████████▌| 23/24 [02:24<00:04,  4.60s/it]Best trial: 0. Best value: inf:  96%|█████████▌| 23/24 [02:24<00:04,  4.60s/it]Best trial: 0. Best value: inf: 100%|██████████| 24/24 [02:24<00:00,  8.06s/it]Best trial: 0. Best value: inf: 100%|██████████| 24/24 [02:24<00:00,  6.02s/it]
[W 2024-08-19 05:19:59,464] The parameter 'use_pos_enc' in trial#85 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:19:59,474] The parameter 'norm_type' in trial#85 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:19:59,503] The parameter 'feat_conv_kernel' in trial#85 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:19:59,508] The parameter 'skip_connection_mode' in trial#85 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:19:59,517] The parameter 'mlp_norm' in trial#85 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=112, n_heads=4, e_layers=7, hidden_d_model=40, seq_layers=2, last_d_model=224, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=28, pos_d_model=20, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.18, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=4, fc_layer_type='mlp', conv_out_dim=128, feat_conv_kernel=11, skip_connection_mode='conv_mlp', conv_norm=True, mlp_norm=False, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-conv_mlp-conv-True-mlp-False-use_pos-True-seq_len-8-lr-0.0051-d-112-hid_d-40-last_d-224-tok_d-4-time_d-28-pos_d-20-e_layers-7-tok_conv_k-9-conv_out_d-128-feat_conv_k-11-dropout-0.18-norm_type-batch-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.0050743095237205755, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:20:00,513] Trial 85 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 40, 'token_conv_kernel': 9, 'last_d_model': 224, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 28, 'pos_d_model': 20, 'd_model': 112, 'conv_out_dim': 128, 'e_layers': 7, 'learning_rate': 0.0050743095237205755, 'dropout': 0.18, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 11, 'skip_connection_mode': 'conv_mlp', 'conv_norm': True, 'mlp_norm': False, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:20:10,719] The parameter 'use_pos_enc' in trial#113 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:10,723] The parameter 'norm_type' in trial#113 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:10,735] The parameter 'feat_conv_kernel' in trial#113 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:10,739] The parameter 'skip_connection_mode' in trial#113 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:10,748] The parameter 'mlp_norm' in trial#113 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=80, n_heads=4, e_layers=5, hidden_d_model=32, seq_layers=2, last_d_model=480, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=28, pos_d_model=24, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.12, use_pos_enc=False, bidirectional=False, norm_type='batch', num_heads=4, fc_layer_type='mlp', conv_out_dim=64, feat_conv_kernel=11, skip_connection_mode='full', conv_norm=True, mlp_norm=True, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-full-conv-True-mlp-True-use_pos-False-seq_len-8-lr-0.0154-d-80-hid_d-32-last_d-480-tok_d-4-time_d-28-pos_d-24-e_layers-5-tok_conv_k-9-conv_out_d-64-feat_conv_k-11-dropout-0.12-norm_type-batch-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.015433284917470112, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:20:16,902] Trial 113 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 32, 'token_conv_kernel': 9, 'last_d_model': 480, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 28, 'pos_d_model': 24, 'd_model': 80, 'conv_out_dim': 64, 'e_layers': 5, 'learning_rate': 0.015433284917470112, 'dropout': 0.12000000000000001, 'combine_type': 'add', 'use_pos_enc': False, 'norm_type': 'batch', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 11, 'skip_connection_mode': 'full', 'conv_norm': True, 'mlp_norm': True, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:20:17,162] The parameter 'use_pos_enc' in trial#143 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:17,170] The parameter 'norm_type' in trial#143 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:17,193] The parameter 'feat_conv_kernel' in trial#143 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:17,202] The parameter 'skip_connection_mode' in trial#143 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:17,214] The parameter 'mlp_norm' in trial#143 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=48, n_heads=4, e_layers=7, hidden_d_model=32, seq_layers=2, last_d_model=480, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=24, pos_d_model=20, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.12, use_pos_enc=False, bidirectional=False, norm_type='layer', num_heads=4, fc_layer_type='mlp', conv_out_dim=128, feat_conv_kernel=11, skip_connection_mode='full', conv_norm=True, mlp_norm=False, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-full-conv-True-mlp-False-use_pos-False-seq_len-8-lr-0.0036-d-48-hid_d-32-last_d-480-tok_d-4-time_d-24-pos_d-20-e_layers-7-tok_conv_k-9-conv_out_d-128-feat_conv_k-11-dropout-0.12-norm_type-layer-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.0035870227674854537, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:20:22,964] Trial 143 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 32, 'token_conv_kernel': 9, 'last_d_model': 480, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 24, 'pos_d_model': 20, 'd_model': 48, 'conv_out_dim': 128, 'e_layers': 7, 'learning_rate': 0.0035870227674854537, 'dropout': 0.12000000000000001, 'combine_type': 'add', 'use_pos_enc': False, 'norm_type': 'layer', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 11, 'skip_connection_mode': 'full', 'conv_norm': True, 'mlp_norm': False, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:20:23,324] The parameter 'use_pos_enc' in trial#157 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:23,329] The parameter 'norm_type' in trial#157 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:23,345] The parameter 'feat_conv_kernel' in trial#157 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:23,350] The parameter 'skip_connection_mode' in trial#157 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:23,360] The parameter 'mlp_norm' in trial#157 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=80, n_heads=4, e_layers=8, hidden_d_model=32, seq_layers=2, last_d_model=416, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=20, pos_d_model=16, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=False, bidirectional=False, norm_type='layer', num_heads=4, fc_layer_type='mlp', conv_out_dim=192, feat_conv_kernel=11, skip_connection_mode='conv_mlp', conv_norm=True, mlp_norm=False, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-conv_mlp-conv-True-mlp-False-use_pos-False-seq_len-8-lr-0.001-d-80-hid_d-32-last_d-416-tok_d-4-time_d-20-pos_d-16-e_layers-8-tok_conv_k-9-conv_out_d-192-feat_conv_k-11-dropout-0.14-norm_type-layer-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.000954254938791751, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:20:29,133] Trial 157 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 32, 'token_conv_kernel': 9, 'last_d_model': 416, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 20, 'pos_d_model': 16, 'd_model': 80, 'conv_out_dim': 192, 'e_layers': 8, 'learning_rate': 0.000954254938791751, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': False, 'norm_type': 'layer', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 11, 'skip_connection_mode': 'conv_mlp', 'conv_norm': True, 'mlp_norm': False, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:20:29,369] The parameter 'use_pos_enc' in trial#173 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:29,379] The parameter 'norm_type' in trial#173 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:29,399] The parameter 'feat_conv_kernel' in trial#173 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:29,404] The parameter 'skip_connection_mode' in trial#173 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:29,414] The parameter 'mlp_norm' in trial#173 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=48, n_heads=4, e_layers=6, hidden_d_model=40, seq_layers=2, last_d_model=480, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=24, pos_d_model=24, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=False, bidirectional=False, norm_type='batch', num_heads=4, fc_layer_type='mlp', conv_out_dim=128, feat_conv_kernel=11, skip_connection_mode='conv_mlp', conv_norm=True, mlp_norm=False, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-conv_mlp-conv-True-mlp-False-use_pos-False-seq_len-8-lr-0.0016-d-48-hid_d-40-last_d-480-tok_d-4-time_d-24-pos_d-24-e_layers-6-tok_conv_k-9-conv_out_d-128-feat_conv_k-11-dropout-0.16-norm_type-batch-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.001598972850319609, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:20:31,087] Trial 173 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 40, 'token_conv_kernel': 9, 'last_d_model': 480, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 24, 'pos_d_model': 24, 'd_model': 48, 'conv_out_dim': 128, 'e_layers': 6, 'learning_rate': 0.001598972850319609, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': False, 'norm_type': 'batch', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 11, 'skip_connection_mode': 'conv_mlp', 'conv_norm': True, 'mlp_norm': False, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:20:41,361] The parameter 'use_pos_enc' in trial#216 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:41,393] The parameter 'norm_type' in trial#216 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:41,411] The parameter 'feat_conv_kernel' in trial#216 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:41,416] The parameter 'skip_connection_mode' in trial#216 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:41,424] The parameter 'mlp_norm' in trial#216 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=144, n_heads=4, e_layers=5, hidden_d_model=44, seq_layers=2, last_d_model=480, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=28, pos_d_model=24, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=False, bidirectional=False, norm_type='layer', num_heads=4, fc_layer_type='mlp', conv_out_dim=128, feat_conv_kernel=11, skip_connection_mode='full', conv_norm=True, mlp_norm=True, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-full-conv-True-mlp-True-use_pos-False-seq_len-8-lr-0.0046-d-144-hid_d-44-last_d-480-tok_d-4-time_d-28-pos_d-24-e_layers-5-tok_conv_k-9-conv_out_d-128-feat_conv_k-11-dropout-0.16-norm_type-layer-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.00459101055816931, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:20:48,549] Trial 216 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 44, 'token_conv_kernel': 9, 'last_d_model': 480, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 28, 'pos_d_model': 24, 'd_model': 144, 'conv_out_dim': 128, 'e_layers': 5, 'learning_rate': 0.00459101055816931, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': False, 'norm_type': 'layer', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 11, 'skip_connection_mode': 'full', 'conv_norm': True, 'mlp_norm': True, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:20:48,785] The parameter 'use_pos_enc' in trial#247 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:48,789] The parameter 'norm_type' in trial#247 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:48,800] The parameter 'feat_conv_kernel' in trial#247 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:48,805] The parameter 'skip_connection_mode' in trial#247 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:48,813] The parameter 'mlp_norm' in trial#247 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=80, n_heads=4, e_layers=4, hidden_d_model=36, seq_layers=2, last_d_model=416, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=24, pos_d_model=24, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.12, use_pos_enc=False, bidirectional=False, norm_type='layer', num_heads=4, fc_layer_type='mlp', conv_out_dim=64, feat_conv_kernel=11, skip_connection_mode='full', conv_norm=True, mlp_norm=True, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-full-conv-True-mlp-True-use_pos-False-seq_len-8-lr-0.0062-d-80-hid_d-36-last_d-416-tok_d-4-time_d-24-pos_d-24-e_layers-4-tok_conv_k-9-conv_out_d-64-feat_conv_k-11-dropout-0.12-norm_type-layer-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.006214114120455091, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:20:54,419] Trial 247 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 36, 'token_conv_kernel': 9, 'last_d_model': 416, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 24, 'pos_d_model': 24, 'd_model': 80, 'conv_out_dim': 64, 'e_layers': 4, 'learning_rate': 0.006214114120455091, 'dropout': 0.12000000000000001, 'combine_type': 'add', 'use_pos_enc': False, 'norm_type': 'layer', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 11, 'skip_connection_mode': 'full', 'conv_norm': True, 'mlp_norm': True, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:20:54,936] The parameter 'use_pos_enc' in trial#276 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:54,941] The parameter 'norm_type' in trial#276 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:54,953] The parameter 'feat_conv_kernel' in trial#276 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:54,958] The parameter 'skip_connection_mode' in trial#276 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:54,968] The parameter 'mlp_norm' in trial#276 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=112, n_heads=4, e_layers=2, hidden_d_model=24, seq_layers=2, last_d_model=352, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=24, pos_d_model=20, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.12, use_pos_enc=False, bidirectional=False, norm_type='batch', num_heads=4, fc_layer_type='mlp', conv_out_dim=128, feat_conv_kernel=9, skip_connection_mode='conv_mlp', conv_norm=True, mlp_norm=False, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-conv_mlp-conv-True-mlp-False-use_pos-False-seq_len-8-lr-0.0036-d-112-hid_d-24-last_d-352-tok_d-4-time_d-24-pos_d-20-e_layers-2-tok_conv_k-9-conv_out_d-128-feat_conv_k-9-dropout-0.12-norm_type-batch-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.0036397598103004147, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:20:55,304] Trial 276 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 24, 'token_conv_kernel': 9, 'last_d_model': 352, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 24, 'pos_d_model': 20, 'd_model': 112, 'conv_out_dim': 128, 'e_layers': 2, 'learning_rate': 0.0036397598103004147, 'dropout': 0.12000000000000001, 'combine_type': 'add', 'use_pos_enc': False, 'norm_type': 'batch', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 9, 'skip_connection_mode': 'conv_mlp', 'conv_norm': True, 'mlp_norm': False, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:20:55,493] The parameter 'use_pos_enc' in trial#279 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:55,500] The parameter 'norm_type' in trial#279 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:55,525] The parameter 'feat_conv_kernel' in trial#279 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:55,536] The parameter 'skip_connection_mode' in trial#279 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:55,549] The parameter 'mlp_norm' in trial#279 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=144, n_heads=4, e_layers=3, hidden_d_model=24, seq_layers=2, last_d_model=288, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=20, pos_d_model=24, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.12, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=4, fc_layer_type='mlp', conv_out_dim=128, feat_conv_kernel=11, skip_connection_mode='conv_mlp', conv_norm=True, mlp_norm=False, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-conv_mlp-conv-True-mlp-False-use_pos-True-seq_len-8-lr-0.0037-d-144-hid_d-24-last_d-288-tok_d-4-time_d-20-pos_d-24-e_layers-3-tok_conv_k-9-conv_out_d-128-feat_conv_k-11-dropout-0.12-norm_type-batch-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.003661926085112974, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:20:55,883] Trial 279 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 24, 'token_conv_kernel': 9, 'last_d_model': 288, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 20, 'pos_d_model': 24, 'd_model': 144, 'conv_out_dim': 128, 'e_layers': 3, 'learning_rate': 0.003661926085112974, 'dropout': 0.12000000000000001, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 11, 'skip_connection_mode': 'conv_mlp', 'conv_norm': True, 'mlp_norm': False, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:20:56,112] The parameter 'use_pos_enc' in trial#282 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:56,117] The parameter 'norm_type' in trial#282 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:56,133] The parameter 'feat_conv_kernel' in trial#282 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:56,137] The parameter 'skip_connection_mode' in trial#282 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:20:56,193] The parameter 'mlp_norm' in trial#282 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=80, n_heads=4, e_layers=5, hidden_d_model=36, seq_layers=2, last_d_model=352, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=24, pos_d_model=16, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.12, use_pos_enc=False, bidirectional=False, norm_type='layer', num_heads=4, fc_layer_type='mlp', conv_out_dim=192, feat_conv_kernel=9, skip_connection_mode='conv_mlp', conv_norm=True, mlp_norm=False, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-conv_mlp-conv-True-mlp-False-use_pos-False-seq_len-8-lr-0.0025-d-80-hid_d-36-last_d-352-tok_d-4-time_d-24-pos_d-16-e_layers-5-tok_conv_k-9-conv_out_d-192-feat_conv_k-9-dropout-0.12-norm_type-layer-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.0025329278484033124, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:20:57,128] Trial 282 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 36, 'token_conv_kernel': 9, 'last_d_model': 352, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 24, 'pos_d_model': 16, 'd_model': 80, 'conv_out_dim': 192, 'e_layers': 5, 'learning_rate': 0.0025329278484033124, 'dropout': 0.12000000000000001, 'combine_type': 'add', 'use_pos_enc': False, 'norm_type': 'layer', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 9, 'skip_connection_mode': 'conv_mlp', 'conv_norm': True, 'mlp_norm': False, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:21:02,352] The parameter 'use_pos_enc' in trial#303 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:02,356] The parameter 'norm_type' in trial#303 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:02,368] The parameter 'feat_conv_kernel' in trial#303 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:02,372] The parameter 'skip_connection_mode' in trial#303 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:02,383] The parameter 'mlp_norm' in trial#303 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=144, n_heads=4, e_layers=5, hidden_d_model=28, seq_layers=2, last_d_model=352, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=16, pos_d_model=16, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=4, fc_layer_type='mlp', conv_out_dim=128, feat_conv_kernel=9, skip_connection_mode='full', conv_norm=True, mlp_norm=False, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-full-conv-True-mlp-False-use_pos-True-seq_len-8-lr-0.0013-d-144-hid_d-28-last_d-352-tok_d-4-time_d-16-pos_d-16-e_layers-5-tok_conv_k-9-conv_out_d-128-feat_conv_k-9-dropout-0.14-norm_type-layer-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.0013496739455088667, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:21:07,938] Trial 303 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 28, 'token_conv_kernel': 9, 'last_d_model': 352, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 16, 'pos_d_model': 16, 'd_model': 144, 'conv_out_dim': 128, 'e_layers': 5, 'learning_rate': 0.0013496739455088667, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 9, 'skip_connection_mode': 'full', 'conv_norm': True, 'mlp_norm': False, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:21:08,177] The parameter 'use_pos_enc' in trial#324 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:08,182] The parameter 'norm_type' in trial#324 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:08,195] The parameter 'feat_conv_kernel' in trial#324 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:08,200] The parameter 'skip_connection_mode' in trial#324 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:08,209] The parameter 'mlp_norm' in trial#324 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=112, n_heads=4, e_layers=5, hidden_d_model=32, seq_layers=2, last_d_model=288, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=28, pos_d_model=16, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=False, bidirectional=False, norm_type='batch', num_heads=4, fc_layer_type='mlp', conv_out_dim=128, feat_conv_kernel=9, skip_connection_mode='conv_mlp', conv_norm=True, mlp_norm=False, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-conv_mlp-conv-True-mlp-False-use_pos-False-seq_len-8-lr-0.0044-d-112-hid_d-32-last_d-288-tok_d-4-time_d-28-pos_d-16-e_layers-5-tok_conv_k-9-conv_out_d-128-feat_conv_k-9-dropout-0.14-norm_type-batch-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.004414920693911619, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:21:09,403] Trial 324 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 32, 'token_conv_kernel': 9, 'last_d_model': 288, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 28, 'pos_d_model': 16, 'd_model': 112, 'conv_out_dim': 128, 'e_layers': 5, 'learning_rate': 0.004414920693911619, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': False, 'norm_type': 'batch', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 9, 'skip_connection_mode': 'conv_mlp', 'conv_norm': True, 'mlp_norm': False, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:21:09,722] The parameter 'use_pos_enc' in trial#331 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:09,728] The parameter 'norm_type' in trial#331 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:09,744] The parameter 'feat_conv_kernel' in trial#331 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:09,749] The parameter 'skip_connection_mode' in trial#331 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:09,760] The parameter 'mlp_norm' in trial#331 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=112, n_heads=4, e_layers=4, hidden_d_model=28, seq_layers=2, last_d_model=288, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=28, pos_d_model=16, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=4, fc_layer_type='mlp', conv_out_dim=128, feat_conv_kernel=9, skip_connection_mode='full', conv_norm=True, mlp_norm=True, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-full-conv-True-mlp-True-use_pos-True-seq_len-8-lr-0.0115-d-112-hid_d-28-last_d-288-tok_d-4-time_d-28-pos_d-16-e_layers-4-tok_conv_k-9-conv_out_d-128-feat_conv_k-9-dropout-0.16-norm_type-layer-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.011480990805674477, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:21:22,257] Trial 331 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 28, 'token_conv_kernel': 9, 'last_d_model': 288, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 28, 'pos_d_model': 16, 'd_model': 112, 'conv_out_dim': 128, 'e_layers': 4, 'learning_rate': 0.011480990805674477, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 9, 'skip_connection_mode': 'full', 'conv_norm': True, 'mlp_norm': True, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:21:22,620] The parameter 'use_pos_enc' in trial#376 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:22,625] The parameter 'norm_type' in trial#376 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:22,637] The parameter 'feat_conv_kernel' in trial#376 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:22,641] The parameter 'skip_connection_mode' in trial#376 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:22,649] The parameter 'mlp_norm' in trial#376 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=112, n_heads=4, e_layers=5, hidden_d_model=28, seq_layers=2, last_d_model=160, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=24, pos_d_model=24, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.12, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=4, fc_layer_type='mlp', conv_out_dim=192, feat_conv_kernel=11, skip_connection_mode='full', conv_norm=True, mlp_norm=True, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-full-conv-True-mlp-True-use_pos-True-seq_len-8-lr-0.0131-d-112-hid_d-28-last_d-160-tok_d-4-time_d-24-pos_d-24-e_layers-5-tok_conv_k-9-conv_out_d-192-feat_conv_k-11-dropout-0.12-norm_type-layer-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.013053251555671797, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:21:28,895] Trial 376 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 28, 'token_conv_kernel': 9, 'last_d_model': 160, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 24, 'pos_d_model': 24, 'd_model': 112, 'conv_out_dim': 192, 'e_layers': 5, 'learning_rate': 0.013053251555671797, 'dropout': 0.12000000000000001, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 11, 'skip_connection_mode': 'full', 'conv_norm': True, 'mlp_norm': True, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:21:29,117] The parameter 'use_pos_enc' in trial#400 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:29,124] The parameter 'norm_type' in trial#400 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:29,140] The parameter 'feat_conv_kernel' in trial#400 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:29,158] The parameter 'skip_connection_mode' in trial#400 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:29,182] The parameter 'mlp_norm' in trial#400 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=112, n_heads=4, e_layers=5, hidden_d_model=28, seq_layers=2, last_d_model=352, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=20, pos_d_model=24, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=False, bidirectional=False, norm_type='layer', num_heads=4, fc_layer_type='mlp', conv_out_dim=192, feat_conv_kernel=9, skip_connection_mode='full', conv_norm=True, mlp_norm=False, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-full-conv-True-mlp-False-use_pos-False-seq_len-8-lr-0.02-d-112-hid_d-28-last_d-352-tok_d-4-time_d-20-pos_d-24-e_layers-5-tok_conv_k-9-conv_out_d-192-feat_conv_k-9-dropout-0.14-norm_type-layer-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.01996216114405622, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:21:29,489] Trial 400 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 28, 'token_conv_kernel': 9, 'last_d_model': 352, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 20, 'pos_d_model': 24, 'd_model': 112, 'conv_out_dim': 192, 'e_layers': 5, 'learning_rate': 0.01996216114405622, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': False, 'norm_type': 'layer', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 9, 'skip_connection_mode': 'full', 'conv_norm': True, 'mlp_norm': False, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:21:29,804] The parameter 'use_pos_enc' in trial#406 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:29,822] The parameter 'norm_type' in trial#406 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:29,846] The parameter 'feat_conv_kernel' in trial#406 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:29,851] The parameter 'skip_connection_mode' in trial#406 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:29,861] The parameter 'mlp_norm' in trial#406 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=176, n_heads=4, e_layers=2, hidden_d_model=28, seq_layers=2, last_d_model=224, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=24, pos_d_model=16, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=False, bidirectional=False, norm_type='batch', num_heads=4, fc_layer_type='mlp', conv_out_dim=192, feat_conv_kernel=9, skip_connection_mode='full', conv_norm=True, mlp_norm=False, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-full-conv-True-mlp-False-use_pos-False-seq_len-8-lr-0.0091-d-176-hid_d-28-last_d-224-tok_d-4-time_d-24-pos_d-16-e_layers-2-tok_conv_k-9-conv_out_d-192-feat_conv_k-9-dropout-0.14-norm_type-batch-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.009147043439509043, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:21:35,397] Trial 406 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 28, 'token_conv_kernel': 9, 'last_d_model': 224, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 24, 'pos_d_model': 16, 'd_model': 176, 'conv_out_dim': 192, 'e_layers': 2, 'learning_rate': 0.009147043439509043, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': False, 'norm_type': 'batch', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 9, 'skip_connection_mode': 'full', 'conv_norm': True, 'mlp_norm': False, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:21:35,608] The parameter 'use_pos_enc' in trial#433 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:35,613] The parameter 'norm_type' in trial#433 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:35,627] The parameter 'feat_conv_kernel' in trial#433 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:35,632] The parameter 'skip_connection_mode' in trial#433 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:35,642] The parameter 'mlp_norm' in trial#433 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=144, n_heads=4, e_layers=3, hidden_d_model=32, seq_layers=2, last_d_model=160, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=24, pos_d_model=8, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=False, bidirectional=False, norm_type='batch', num_heads=4, fc_layer_type='mlp', conv_out_dim=128, feat_conv_kernel=11, skip_connection_mode='full', conv_norm=True, mlp_norm=True, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-full-conv-True-mlp-True-use_pos-False-seq_len-8-lr-0.0146-d-144-hid_d-32-last_d-160-tok_d-4-time_d-24-pos_d-8-e_layers-3-tok_conv_k-9-conv_out_d-128-feat_conv_k-11-dropout-0.16-norm_type-batch-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.01462004091902344, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:21:35,944] Trial 433 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 32, 'token_conv_kernel': 9, 'last_d_model': 160, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 24, 'pos_d_model': 8, 'd_model': 144, 'conv_out_dim': 128, 'e_layers': 3, 'learning_rate': 0.01462004091902344, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': False, 'norm_type': 'batch', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 11, 'skip_connection_mode': 'full', 'conv_norm': True, 'mlp_norm': True, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:21:36,210] The parameter 'use_pos_enc' in trial#435 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:36,215] The parameter 'norm_type' in trial#435 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:36,228] The parameter 'feat_conv_kernel' in trial#435 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:36,234] The parameter 'skip_connection_mode' in trial#435 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:36,246] The parameter 'mlp_norm' in trial#435 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=112, n_heads=4, e_layers=3, hidden_d_model=28, seq_layers=2, last_d_model=288, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=20, pos_d_model=8, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=4, fc_layer_type='mlp', conv_out_dim=128, feat_conv_kernel=9, skip_connection_mode='full', conv_norm=True, mlp_norm=False, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-full-conv-True-mlp-False-use_pos-True-seq_len-8-lr-0.0086-d-112-hid_d-28-last_d-288-tok_d-4-time_d-20-pos_d-8-e_layers-3-tok_conv_k-9-conv_out_d-128-feat_conv_k-9-dropout-0.14-norm_type-batch-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.008642176314188099, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:21:42,058] Trial 435 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 28, 'token_conv_kernel': 9, 'last_d_model': 288, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 20, 'pos_d_model': 8, 'd_model': 112, 'conv_out_dim': 128, 'e_layers': 3, 'learning_rate': 0.008642176314188099, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 9, 'skip_connection_mode': 'full', 'conv_norm': True, 'mlp_norm': False, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:21:42,445] The parameter 'use_pos_enc' in trial#450 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:42,451] The parameter 'norm_type' in trial#450 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:42,470] The parameter 'feat_conv_kernel' in trial#450 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:42,480] The parameter 'skip_connection_mode' in trial#450 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:42,491] The parameter 'mlp_norm' in trial#450 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=144, n_heads=4, e_layers=4, hidden_d_model=36, seq_layers=2, last_d_model=160, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=8, time_d_model=24, pos_d_model=12, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=4, fc_layer_type='mlp', conv_out_dim=128, feat_conv_kernel=9, skip_connection_mode='full', conv_norm=True, mlp_norm=False, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-full-conv-True-mlp-False-use_pos-True-seq_len-8-lr-0.0218-d-144-hid_d-36-last_d-160-tok_d-8-time_d-24-pos_d-12-e_layers-4-tok_conv_k-9-conv_out_d-128-feat_conv_k-9-dropout-0.14-norm_type-layer-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.021847366356658116, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:21:47,858] Trial 450 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 36, 'token_conv_kernel': 9, 'last_d_model': 160, 'seq_len': 8, 'token_d_model': 8, 'time_d_model': 24, 'pos_d_model': 12, 'd_model': 144, 'conv_out_dim': 128, 'e_layers': 4, 'learning_rate': 0.021847366356658116, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 9, 'skip_connection_mode': 'full', 'conv_norm': True, 'mlp_norm': False, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:21:48,089] The parameter 'use_pos_enc' in trial#466 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:48,095] The parameter 'norm_type' in trial#466 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:48,115] The parameter 'feat_conv_kernel' in trial#466 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:48,122] The parameter 'skip_connection_mode' in trial#466 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:48,135] The parameter 'mlp_norm' in trial#466 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=112, n_heads=4, e_layers=6, hidden_d_model=40, seq_layers=2, last_d_model=288, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=8, time_d_model=24, pos_d_model=20, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.12, use_pos_enc=False, bidirectional=False, norm_type='batch', num_heads=4, fc_layer_type='mlp', conv_out_dim=64, feat_conv_kernel=9, skip_connection_mode='full', conv_norm=True, mlp_norm=False, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-full-conv-True-mlp-False-use_pos-False-seq_len-8-lr-0.0116-d-112-hid_d-40-last_d-288-tok_d-8-time_d-24-pos_d-20-e_layers-6-tok_conv_k-9-conv_out_d-64-feat_conv_k-9-dropout-0.12-norm_type-batch-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.011596319838899848, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:21:49,101] Trial 466 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 40, 'token_conv_kernel': 9, 'last_d_model': 288, 'seq_len': 8, 'token_d_model': 8, 'time_d_model': 24, 'pos_d_model': 20, 'd_model': 112, 'conv_out_dim': 64, 'e_layers': 6, 'learning_rate': 0.011596319838899848, 'dropout': 0.12000000000000001, 'combine_type': 'add', 'use_pos_enc': False, 'norm_type': 'batch', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 9, 'skip_connection_mode': 'full', 'conv_norm': True, 'mlp_norm': False, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:21:49,355] The parameter 'use_pos_enc' in trial#471 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:49,361] The parameter 'norm_type' in trial#471 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:49,379] The parameter 'feat_conv_kernel' in trial#471 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:49,384] The parameter 'skip_connection_mode' in trial#471 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:21:49,394] The parameter 'mlp_norm' in trial#471 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=144, n_heads=4, e_layers=3, hidden_d_model=40, seq_layers=2, last_d_model=224, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=28, pos_d_model=20, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=False, bidirectional=False, norm_type='layer', num_heads=4, fc_layer_type='mlp', conv_out_dim=128, feat_conv_kernel=9, skip_connection_mode='conv_mlp', conv_norm=True, mlp_norm=True, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-conv_mlp-conv-True-mlp-True-use_pos-False-seq_len-8-lr-0.0227-d-144-hid_d-40-last_d-224-tok_d-4-time_d-28-pos_d-20-e_layers-3-tok_conv_k-9-conv_out_d-128-feat_conv_k-9-dropout-0.14-norm_type-layer-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.022707102122935915, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:22:00,259] Trial 471 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 40, 'token_conv_kernel': 9, 'last_d_model': 224, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 28, 'pos_d_model': 20, 'd_model': 144, 'conv_out_dim': 128, 'e_layers': 3, 'learning_rate': 0.022707102122935915, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': False, 'norm_type': 'layer', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 9, 'skip_connection_mode': 'conv_mlp', 'conv_norm': True, 'mlp_norm': True, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:22:00,470] The parameter 'use_pos_enc' in trial#486 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:22:00,476] The parameter 'norm_type' in trial#486 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:22:00,493] The parameter 'feat_conv_kernel' in trial#486 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:22:00,499] The parameter 'skip_connection_mode' in trial#486 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:22:00,511] The parameter 'mlp_norm' in trial#486 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=112, n_heads=4, e_layers=4, hidden_d_model=36, seq_layers=2, last_d_model=224, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=24, pos_d_model=24, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=False, bidirectional=False, norm_type='layer', num_heads=4, fc_layer_type='mlp', conv_out_dim=128, feat_conv_kernel=11, skip_connection_mode='full', conv_norm=True, mlp_norm=True, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-full-conv-True-mlp-True-use_pos-False-seq_len-8-lr-0.0142-d-112-hid_d-36-last_d-224-tok_d-4-time_d-24-pos_d-24-e_layers-4-tok_conv_k-9-conv_out_d-128-feat_conv_k-11-dropout-0.16-norm_type-layer-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.014168483822787337, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:22:06,137] Trial 486 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 36, 'token_conv_kernel': 9, 'last_d_model': 224, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 24, 'pos_d_model': 24, 'd_model': 112, 'conv_out_dim': 128, 'e_layers': 4, 'learning_rate': 0.014168483822787337, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': False, 'norm_type': 'layer', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 11, 'skip_connection_mode': 'full', 'conv_norm': True, 'mlp_norm': True, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:22:06,417] The parameter 'use_pos_enc' in trial#494 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:22:06,423] The parameter 'norm_type' in trial#494 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:22:06,441] The parameter 'feat_conv_kernel' in trial#494 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:22:06,448] The parameter 'skip_connection_mode' in trial#494 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:22:06,461] The parameter 'mlp_norm' in trial#494 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=144, n_heads=4, e_layers=4, hidden_d_model=32, seq_layers=2, last_d_model=288, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=20, pos_d_model=28, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=4, fc_layer_type='mlp', conv_out_dim=128, feat_conv_kernel=11, skip_connection_mode='conv_mlp', conv_norm=True, mlp_norm=False, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-conv_mlp-conv-True-mlp-False-use_pos-True-seq_len-8-lr-0.0158-d-144-hid_d-32-last_d-288-tok_d-4-time_d-20-pos_d-28-e_layers-4-tok_conv_k-9-conv_out_d-128-feat_conv_k-11-dropout-0.16-norm_type-layer-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.01583473659300382, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:22:07,537] Trial 494 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 32, 'token_conv_kernel': 9, 'last_d_model': 288, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 20, 'pos_d_model': 28, 'd_model': 144, 'conv_out_dim': 128, 'e_layers': 4, 'learning_rate': 0.01583473659300382, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 11, 'skip_connection_mode': 'conv_mlp', 'conv_norm': True, 'mlp_norm': False, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
[W 2024-08-19 05:22:17,773] The parameter 'use_pos_enc' in trial#498 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:22:17,780] The parameter 'norm_type' in trial#498 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:22:17,799] The parameter 'feat_conv_kernel' in trial#498 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:22:17,806] The parameter 'skip_connection_mode' in trial#498 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-19 05:22:17,817] The parameter 'mlp_norm' in trial#498 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'batch_size', 'train_epochs', 'feat_conv_kernel', 'skip_connection_mode', 'conv_norm', 'mlp_norm', 'scale_y_type'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data/Pein/Pytorch/Wind-Power-Prediction/train_log/24-08-19-no_time', res_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/res_output/24-08-19-no_time', final_best_metrics_log_path='/data/Pein/Pytorch/Wind-Power-Prediction/final_best_metric/24-08-19-no_time.log', optuna_study_dir='/data/Pein/Pytorch/Wind-Power-Prediction/optuna_study/24-08-19-no_time', wandb_output_dir='/data/Pein/Pytorch/Wind-Power-Prediction/wandb_output/optuna/24-08-19-no_time'), data_paths=Namespace(data_root_dir='/data/Pein/Pytorch/Wind-Power-Prediction/new_data/', train_path='4-train_66_withTime.csv', test_path='4-test_66_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard', random_split=False), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=52, dec_in=52, output_dim=1, d_model=208, n_heads=4, e_layers=2, hidden_d_model=28, seq_layers=2, last_d_model=160, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=4, time_d_model=28, pos_d_model=24, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=False, bidirectional=False, norm_type='layer', num_heads=4, fc_layer_type='mlp', conv_out_dim=192, feat_conv_kernel=11, skip_connection_mode='full', conv_norm=True, mlp_norm=False, norm_after_dict=None), exp_settings='scale_y-standard-skip_mode-full-conv-True-mlp-False-use_pos-False-seq_len-8-lr-0.0242-d-208-hid_d-28-last_d-160-tok_d-4-time_d-28-pos_d-24-e_layers-2-tok_conv_k-9-conv_out_d-192-feat_conv_k-11-dropout-0.14-norm_type-layer-num_heads-4-bs-1024', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.024157260127528692, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


******************************
Data shapes:
Train X shape: (11673, 52)
Train y shape: (11673, 1)
Val X shape: (2919, 52)
Val y shape: (2919, 1)
Test X shape: (2880, 52)
Test y shape: (2880, 1)
Train X mark shape: (11673, 12)
Val X mark shape: (2919, 12)
Test X mark shape: (2880, 12)
******************************
Trial failed due to an error:
[I 2024-08-19 05:22:23,686] Trial 498 finished with value: inf and parameters: {'num_heads': 4, 'hidden_d_model': 28, 'token_conv_kernel': 9, 'last_d_model': 160, 'seq_len': 8, 'token_d_model': 4, 'time_d_model': 28, 'pos_d_model': 24, 'd_model': 208, 'conv_out_dim': 192, 'e_layers': 2, 'learning_rate': 0.024157260127528692, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': False, 'norm_type': 'layer', 'batch_size': 1024, 'train_epochs': 50, 'feat_conv_kernel': 11, 'skip_connection_mode': 'full', 'conv_norm': True, 'mlp_norm': False, 'scale_y_type': 'standard'}. Best is trial 0 with value: inf.
Number of finished trials: 510
Best trial:
Value: inf
Params: 
    num_heads: 4
    hidden_d_model: 36
    token_conv_kernel: 9
    last_d_model: 288
    seq_len: 8
    token_d_model: 8
    time_d_model: 12
    pos_d_model: 20
    d_model: 48
    conv_out_dim: 128
    e_layers: 3
    learning_rate: 0.02628870650973661
    dropout: 0.14
    combine_type: add
    use_pos_enc: True
    norm_type: batch
    batch_size: 1024
    train_epochs: 50
    feat_conv_kernel: 9
    skip_connection_mode: full
    conv_norm: True
    mlp_norm: True
    scale_y_type: standard
Top 10 trials saved to /data/Pein/Pytorch/Wind-Power-Prediction/optuna_results/24-08-19-no_time/24-08-19-no_time-farm_66_top10_params.json
Total time taken:  145.1110 seconds,  2.42 minutes
Done!
