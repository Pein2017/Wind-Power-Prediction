[I 2024-08-13 23:38:15,659] A new study created in RDB with name: 24-08-13-mlp_v3-search-farm_89
Creating study "24-08-13-mlp_v3-search-farm_89" with storage "sqlite:////data3/lsf/Pein/Power-Prediction/optuna_results/24-08-13-mlp_v3-search/24-08-13-mlp_v3-search-farm_89.db?mode=wal"...
  0%|          | 0/100 [00:00<?, ?it/s]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA RTX 6000 Ada Generation') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 9.9 M  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
9.9 M     Trainable params
0         Non-trainable params
9.9 M     Total params
39.410    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.057
Metric Loss/val improved by 0.018 >= min_delta = 0.0. New best score: 1.038
Metric Loss/val improved by 0.008 >= min_delta = 0.0. New best score: 1.031
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.031. Signaling Trainer to stop.
                                         0%|          | 0/100 [01:33<?, ?it/s]Best trial: 0. Best value: 4.65444:   0%|          | 0/100 [01:33<?, ?it/s]Best trial: 0. Best value: 4.65444:   1%|          | 1/100 [01:33<2:33:54, 93.27s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                     Best trial: 0. Best value: 4.65444:   1%|          | 1/100 [01:34<2:33:54, 93.27s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 11.4 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
11.4 M    Trainable params
0         Non-trainable params
11.4 M    Total params
45.442    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.030
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.030. Signaling Trainer to stop.
                                                                                     Best trial: 0. Best value: 4.65444:   1%|          | 1/100 [02:53<2:33:54, 93.27s/it]Best trial: 1. Best value: 0.995314:   1%|          | 1/100 [02:53<2:33:54, 93.27s/it]Best trial: 1. Best value: 0.995314:   2%|▏         | 2/100 [02:53<2:20:12, 85.85s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                      Best trial: 1. Best value: 0.995314:   2%|▏         | 2/100 [02:54<2:20:12, 85.85s/it]Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=11, input_dim=84, dec_in=84, output_dim=1, d_model=192, n_heads=4, e_layers=4, hidden_d_model=240, seq_layers=2, last_d_model=640, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=32, time_d_model=128, pos_d_model=96, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.1, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=256), exp_settings='seq_len-8-lr-0.0181-d-192-hid_d-240-last_d-640-tok_d-32-time_d-128-pos_d-96-e_layers-4-tok_conv_k-11-dropout-0.1-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.01807005084640161, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-13 23:39:48,975] Trial 0 finished with value: 4.654439401626587 and parameters: {'num_heads': 36, 'hidden_d_model': 240, 'token_conv_kernel': 11, 'last_d_model': 640, 'seq_len': 8, 'token_d_model': 32, 'time_d_model': 128, 'pos_d_model': 96, 'd_model': 192, 'conv_out_dim': 256, 'e_layers': 4, 'learning_rate': 0.01807005084640161, 'dropout': 0.1, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 0 with value: 4.654439401626587.
[W 2024-08-13 23:39:49,737] The parameter 'norm_type' in trial#2 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=320, n_heads=4, e_layers=6, hidden_d_model=160, seq_layers=2, last_d_model=608, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=48, time_d_model=64, pos_d_model=48, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.2, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=160), exp_settings='seq_len-12-lr-0.0037-d-320-hid_d-160-last_d-608-tok_d-48-time_d-64-pos_d-48-e_layers-6-tok_conv_k-9-dropout-0.2-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.003715491504600098, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-13 23:41:09,623] Trial 2 finished with value: 1.491926145553589 and parameters: {'num_heads': 36, 'hidden_d_model': 160, 'token_conv_kernel': 9, 'last_d_model': 608, 'seq_len': 12, 'token_d_model': 48, 'time_d_model': 64, 'pos_d_model': 48, 'd_model': 320, 'conv_out_dim': 160, 'e_layers': 6, 'learning_rate': 0.003715491504600098, 'dropout': 0.2, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 1 with value: 0.9953140735626221.
[W 2024-08-13 23:41:10,645] The parameter 'norm_type' in trial#4 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 9.1 M  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
9.1 M     Trainable params
0         Non-trainable params
9.1 M     Total params
36.432    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.070
Metric Loss/val improved by 0.033 >= min_delta = 0.0. New best score: 1.037
Metric Loss/val improved by 0.007 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                      Best trial: 1. Best value: 0.995314:   2%|▏         | 2/100 [05:22<2:20:12, 85.85s/it]Best trial: 4. Best value: 0.969465:   2%|▏         | 2/100 [05:22<2:20:12, 85.85s/it]Best trial: 4. Best value: 0.969465:   3%|▎         | 3/100 [05:22<3:05:23, 114.68s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                       Best trial: 4. Best value: 0.969465:   3%|▎         | 3/100 [05:23<3:05:23, 114.68s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 12.6 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
12.6 M    Trainable params
0         Non-trainable params
12.6 M    Total params
50.239    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.037
Metric Loss/val improved by 0.003 >= min_delta = 0.0. New best score: 1.034
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.033
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.032
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.031
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                       Best trial: 4. Best value: 0.969465:   3%|▎         | 3/100 [08:08<3:05:23, 114.68s/it]Best trial: 4. Best value: 0.969465:   3%|▎         | 3/100 [08:08<3:05:23, 114.68s/it]Best trial: 4. Best value: 0.969465:   4%|▍         | 4/100 [08:08<3:35:45, 134.85s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                       Best trial: 4. Best value: 0.969465:   4%|▍         | 4/100 [08:09<3:35:45, 134.85s/it]dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=8, input_dim=84, dec_in=84, output_dim=1, d_model=224, n_heads=4, e_layers=8, hidden_d_model=208, seq_layers=2, last_d_model=768, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=48, time_d_model=80, pos_d_model=64, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.18, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=68, fc_layer_type='mha', conv_out_dim=160), exp_settings='seq_len-12-lr-0.0134-d-224-hid_d-208-last_d-768-tok_d-48-time_d-80-pos_d-64-e_layers-8-tok_conv_k-8-dropout-0.18-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-68-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.013387525706631962, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-13 23:43:38,604] Trial 4 finished with value: 0.9694645166397096 and parameters: {'num_heads': 68, 'hidden_d_model': 208, 'token_conv_kernel': 8, 'last_d_model': 768, 'seq_len': 12, 'token_d_model': 48, 'time_d_model': 80, 'pos_d_model': 64, 'd_model': 224, 'conv_out_dim': 160, 'e_layers': 8, 'learning_rate': 0.013387525706631962, 'dropout': 0.18, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-13 23:43:39,552] The parameter 'norm_type' in trial#6 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=8, input_dim=84, dec_in=84, output_dim=1, d_model=256, n_heads=4, e_layers=4, hidden_d_model=176, seq_layers=2, last_d_model=1280, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=48, time_d_model=96, pos_d_model=64, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.2, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=68, fc_layer_type='mha', conv_out_dim=288), exp_settings='seq_len-12-lr-0.0057-d-256-hid_d-176-last_d-1280-tok_d-48-time_d-96-pos_d-64-e_layers-4-tok_conv_k-8-dropout-0.2-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-68-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.005660408056130758, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-13 23:46:24,366] Trial 6 finished with value: 1.108008623123169 and parameters: {'num_heads': 68, 'hidden_d_model': 176, 'token_conv_kernel': 8, 'last_d_model': 1280, 'seq_len': 12, 'token_d_model': 48, 'time_d_model': 96, 'pos_d_model': 64, 'd_model': 256, 'conv_out_dim': 288, 'e_layers': 4, 'learning_rate': 0.005660408056130758, 'dropout': 0.2, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-13 23:46:25,321] The parameter 'norm_type' in trial#8 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 20.8 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
20.8 M    Trainable params
0         Non-trainable params
20.8 M    Total params
83.330    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.049
Metric Loss/val improved by 0.012 >= min_delta = 0.0. New best score: 1.037
Metric Loss/val improved by 0.004 >= min_delta = 0.0. New best score: 1.033
Metric Loss/val improved by 0.003 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                       Best trial: 4. Best value: 0.969465:   4%|▍         | 4/100 [11:07<3:35:45, 134.85s/it]Best trial: 4. Best value: 0.969465:   4%|▍         | 4/100 [11:07<3:35:45, 134.85s/it]Best trial: 4. Best value: 0.969465:   5%|▌         | 5/100 [11:07<3:58:24, 150.58s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                       Best trial: 4. Best value: 0.969465:   5%|▌         | 5/100 [11:07<3:58:24, 150.58s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 24.0 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
24.0 M    Trainable params
0         Non-trainable params
24.0 M    Total params
95.898    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.079
Metric Loss/val improved by 0.030 >= min_delta = 0.0. New best score: 1.049
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 1.048
Metric Loss/val improved by 0.018 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                       Best trial: 4. Best value: 0.969465:   5%|▌         | 5/100 [13:25<3:58:24, 150.58s/it]Best trial: 4. Best value: 0.969465:   5%|▌         | 5/100 [13:25<3:58:24, 150.58s/it]Best trial: 4. Best value: 0.969465:   6%|▌         | 6/100 [13:25<3:49:25, 146.44s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                       Best trial: 4. Best value: 0.969465:   6%|▌         | 6/100 [13:25<3:49:25, 146.44s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=352, n_heads=4, e_layers=6, hidden_d_model=176, seq_layers=2, last_d_model=736, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=80, pos_d_model=64, combine_type='add', seq_len=10, pred_len=1, min_y_value=0.0, dropout=0.2, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=256), exp_settings='seq_len-10-lr-0.0124-d-352-hid_d-176-last_d-736-tok_d-64-time_d-80-pos_d-64-e_layers-6-tok_conv_k-10-dropout-0.2-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.012409937163260273, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-13 23:49:22,839] Trial 8 finished with value: 0.9967625617980957 and parameters: {'num_heads': 36, 'hidden_d_model': 176, 'token_conv_kernel': 10, 'last_d_model': 736, 'seq_len': 10, 'token_d_model': 64, 'time_d_model': 80, 'pos_d_model': 64, 'd_model': 352, 'conv_out_dim': 256, 'e_layers': 6, 'learning_rate': 0.012409937163260273, 'dropout': 0.2, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-13 23:49:23,130] The parameter 'norm_type' in trial#10 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=8, input_dim=84, dec_in=84, output_dim=1, d_model=480, n_heads=4, e_layers=4, hidden_d_model=208, seq_layers=2, last_d_model=992, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=96, pos_d_model=80, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.22, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=68, fc_layer_type='mha', conv_out_dim=352), exp_settings='seq_len-8-lr-0.0103-d-480-hid_d-208-last_d-992-tok_d-64-time_d-96-pos_d-80-e_layers-4-tok_conv_k-8-dropout-0.22-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-68-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.010268781481864353, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-13 23:51:41,261] Trial 10 finished with value: 0.9965314626693726 and parameters: {'num_heads': 68, 'hidden_d_model': 208, 'token_conv_kernel': 8, 'last_d_model': 992, 'seq_len': 8, 'token_d_model': 64, 'time_d_model': 96, 'pos_d_model': 80, 'd_model': 480, 'conv_out_dim': 352, 'e_layers': 4, 'learning_rate': 0.010268781481864353, 'dropout': 0.22, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-13 23:51:41,510] The parameter 'norm_type' in trial#12 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 14.2 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
14.2 M    Trainable params
0         Non-trainable params
14.2 M    Total params
56.708    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.031
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.031. Signaling Trainer to stop.
                                                                                       Best trial: 4. Best value: 0.969465:   6%|▌         | 6/100 [14:40<3:49:25, 146.44s/it]Best trial: 4. Best value: 0.969465:   6%|▌         | 6/100 [14:40<3:49:25, 146.44s/it]Best trial: 4. Best value: 0.969465:   7%|▋         | 7/100 [14:40<3:10:50, 123.12s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                       Best trial: 4. Best value: 0.969465:   7%|▋         | 7/100 [14:41<3:10:50, 123.12s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 9.3 M  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
9.3 M     Trainable params
0         Non-trainable params
9.3 M     Total params
37.275    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.110
Metric Loss/val improved by 0.024 >= min_delta = 0.0. New best score: 1.086
Metric Loss/val improved by 0.020 >= min_delta = 0.0. New best score: 1.066
Metric Loss/val improved by 0.034 >= min_delta = 0.0. New best score: 1.032
Metric Loss/val improved by 0.003 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                       Best trial: 4. Best value: 0.969465:   7%|▋         | 7/100 [16:10<3:10:50, 123.12s/it]Best trial: 4. Best value: 0.969465:   7%|▋         | 7/100 [16:10<3:10:50, 123.12s/it]Best trial: 4. Best value: 0.969465:   8%|▊         | 8/100 [16:10<2:52:29, 112.50s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                       Best trial: 4. Best value: 0.969465:   8%|▊         | 8/100 [16:10<2:52:29, 112.50s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=192, n_heads=4, e_layers=4, hidden_d_model=256, seq_layers=2, last_d_model=1440, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=48, time_d_model=96, pos_d_model=112, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.18, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=320), exp_settings='seq_len-12-lr-0.0026-d-192-hid_d-256-last_d-1440-tok_d-48-time_d-96-pos_d-112-e_layers-4-tok_conv_k-9-dropout-0.18-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0026293698934636944, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-13 23:52:56,362] Trial 12 finished with value: 1.871958422660828 and parameters: {'num_heads': 36, 'hidden_d_model': 256, 'token_conv_kernel': 9, 'last_d_model': 1440, 'seq_len': 12, 'token_d_model': 48, 'time_d_model': 96, 'pos_d_model': 112, 'd_model': 192, 'conv_out_dim': 320, 'e_layers': 4, 'learning_rate': 0.0026293698934636944, 'dropout': 0.18, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-13 23:52:56,842] The parameter 'norm_type' in trial#14 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=288, n_heads=4, e_layers=6, hidden_d_model=224, seq_layers=2, last_d_model=768, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=48, time_d_model=96, pos_d_model=64, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.22, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=68, fc_layer_type='mha', conv_out_dim=128), exp_settings='seq_len-12-lr-0.041-d-288-hid_d-224-last_d-768-tok_d-48-time_d-96-pos_d-64-e_layers-6-tok_conv_k-9-dropout-0.22-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-68-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0409793483129947, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-13 23:54:26,102] Trial 14 finished with value: 0.9975953578948975 and parameters: {'num_heads': 68, 'hidden_d_model': 224, 'token_conv_kernel': 9, 'last_d_model': 768, 'seq_len': 12, 'token_d_model': 48, 'time_d_model': 96, 'pos_d_model': 64, 'd_model': 288, 'conv_out_dim': 128, 'e_layers': 6, 'learning_rate': 0.0409793483129947, 'dropout': 0.22, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-13 23:54:26,355] The parameter 'norm_type' in trial#16 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 17.2 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
17.2 M    Trainable params
0         Non-trainable params
17.2 M    Total params
68.749    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.240
Metric Loss/val improved by 0.201 >= min_delta = 0.0. New best score: 1.040
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.039
Metric Loss/val improved by 0.008 >= min_delta = 0.0. New best score: 1.031
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                       Best trial: 4. Best value: 0.969465:   8%|▊         | 8/100 [18:45<2:52:29, 112.50s/it]Best trial: 4. Best value: 0.969465:   8%|▊         | 8/100 [18:45<2:52:29, 112.50s/it]Best trial: 4. Best value: 0.969465:   9%|▉         | 9/100 [18:45<3:10:54, 125.87s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                       Best trial: 4. Best value: 0.969465:   9%|▉         | 9/100 [18:46<3:10:54, 125.87s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 11.7 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
11.7 M    Trainable params
0         Non-trainable params
11.7 M    Total params
46.850    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                       Best trial: 4. Best value: 0.969465:   9%|▉         | 9/100 [20:16<3:10:54, 125.87s/it]Best trial: 4. Best value: 0.969465:   9%|▉         | 9/100 [20:16<3:10:54, 125.87s/it]Best trial: 4. Best value: 0.969465:  10%|█         | 10/100 [20:16<2:52:23, 114.93s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                        Best trial: 4. Best value: 0.969465:  10%|█         | 10/100 [20:16<2:52:23, 114.93s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=6, input_dim=84, dec_in=84, output_dim=1, d_model=320, n_heads=4, e_layers=8, hidden_d_model=176, seq_layers=2, last_d_model=992, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=48, time_d_model=80, pos_d_model=112, combine_type='add', seq_len=10, pred_len=1, min_y_value=0.0, dropout=0.2, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=68, fc_layer_type='mha', conv_out_dim=384), exp_settings='seq_len-10-lr-0.0183-d-320-hid_d-176-last_d-992-tok_d-48-time_d-80-pos_d-112-e_layers-8-tok_conv_k-6-dropout-0.2-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-68-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.01825057951887291, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-13 23:57:01,385] Trial 16 finished with value: 1.2286834478378297 and parameters: {'num_heads': 68, 'hidden_d_model': 176, 'token_conv_kernel': 6, 'last_d_model': 992, 'seq_len': 10, 'token_d_model': 48, 'time_d_model': 80, 'pos_d_model': 112, 'd_model': 320, 'conv_out_dim': 384, 'e_layers': 8, 'learning_rate': 0.01825057951887291, 'dropout': 0.2, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-13 23:57:01,747] The parameter 'norm_type' in trial#18 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=6, input_dim=84, dec_in=84, output_dim=1, d_model=384, n_heads=4, e_layers=6, hidden_d_model=192, seq_layers=2, last_d_model=1024, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=48, time_d_model=96, pos_d_model=80, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.18, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=160), exp_settings='seq_len-12-lr-0.0038-d-384-hid_d-192-last_d-1024-tok_d-48-time_d-96-pos_d-80-e_layers-6-tok_conv_k-6-dropout-0.18-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0038368231004372606, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-13 23:58:31,806] Trial 18 finished with value: 0.9926766991615296 and parameters: {'num_heads': 36, 'hidden_d_model': 192, 'token_conv_kernel': 6, 'last_d_model': 1024, 'seq_len': 12, 'token_d_model': 48, 'time_d_model': 96, 'pos_d_model': 80, 'd_model': 384, 'conv_out_dim': 160, 'e_layers': 6, 'learning_rate': 0.0038368231004372606, 'dropout': 0.18, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-13 23:58:32,101] The parameter 'norm_type' in trial#19 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 17.2 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
17.2 M    Trainable params
0         Non-trainable params
17.2 M    Total params
68.614    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.774
Metric Loss/val improved by 0.719 >= min_delta = 0.0. New best score: 1.055
Metric Loss/val improved by 0.013 >= min_delta = 0.0. New best score: 1.042
Metric Loss/val improved by 0.014 >= min_delta = 0.0. New best score: 1.028
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.028. Signaling Trainer to stop.
                                                                                        Best trial: 4. Best value: 0.969465:  10%|█         | 10/100 [22:32<2:52:23, 114.93s/it]Best trial: 4. Best value: 0.969465:  10%|█         | 10/100 [22:32<2:52:23, 114.93s/it]Best trial: 4. Best value: 0.969465:  11%|█         | 11/100 [22:32<3:00:18, 121.56s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                        Best trial: 4. Best value: 0.969465:  11%|█         | 11/100 [22:32<3:00:18, 121.56s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 12.8 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
12.8 M    Trainable params
0         Non-trainable params
12.8 M    Total params
51.383    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.047
Metric Loss/val improved by 0.016 >= min_delta = 0.0. New best score: 1.032
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                        Best trial: 4. Best value: 0.969465:  11%|█         | 11/100 [25:06<3:00:18, 121.56s/it]Best trial: 4. Best value: 0.969465:  11%|█         | 11/100 [25:06<3:00:18, 121.56s/it]Best trial: 4. Best value: 0.969465:  12%|█▏        | 12/100 [25:06<3:12:40, 131.37s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                        Best trial: 4. Best value: 0.969465:  12%|█▏        | 12/100 [25:06<3:12:40, 131.37s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=416, n_heads=4, e_layers=4, hidden_d_model=208, seq_layers=2, last_d_model=768, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=48, time_d_model=96, pos_d_model=96, combine_type='add', seq_len=14, pred_len=1, min_y_value=0.0, dropout=0.26, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=288), exp_settings='seq_len-14-lr-0.0253-d-416-hid_d-208-last_d-768-tok_d-48-time_d-96-pos_d-96-e_layers-4-tok_conv_k-9-dropout-0.26-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.025309654696349364, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 00:00:48,395] Trial 19 finished with value: 0.9967672824859619 and parameters: {'num_heads': 36, 'hidden_d_model': 208, 'token_conv_kernel': 9, 'last_d_model': 768, 'seq_len': 14, 'token_d_model': 48, 'time_d_model': 96, 'pos_d_model': 96, 'd_model': 416, 'conv_out_dim': 288, 'e_layers': 4, 'learning_rate': 0.025309654696349364, 'dropout': 0.26, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 00:00:48,683] The parameter 'norm_type' in trial#22 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=8, input_dim=84, dec_in=84, output_dim=1, d_model=320, n_heads=4, e_layers=6, hidden_d_model=192, seq_layers=2, last_d_model=1184, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=96, pos_d_model=112, combine_type='add', seq_len=10, pred_len=1, min_y_value=0.0, dropout=0.18, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=68, fc_layer_type='mha', conv_out_dim=160), exp_settings='seq_len-10-lr-0.0061-d-320-hid_d-192-last_d-1184-tok_d-64-time_d-96-pos_d-112-e_layers-6-tok_conv_k-8-dropout-0.18-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-68-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.006085889833208438, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 00:03:22,214] Trial 22 finished with value: 0.9980596899986268 and parameters: {'num_heads': 68, 'hidden_d_model': 192, 'token_conv_kernel': 8, 'last_d_model': 1184, 'seq_len': 10, 'token_d_model': 64, 'time_d_model': 96, 'pos_d_model': 112, 'd_model': 320, 'conv_out_dim': 160, 'e_layers': 6, 'learning_rate': 0.006085889833208438, 'dropout': 0.18, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 00:03:22,504] The parameter 'norm_type' in trial#24 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 16.9 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
16.9 M    Trainable params
0         Non-trainable params
16.9 M    Total params
67.701    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.034
Metric Loss/val improved by 0.004 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                        Best trial: 4. Best value: 0.969465:  12%|█▏        | 12/100 [28:27<3:12:40, 131.37s/it]Best trial: 4. Best value: 0.969465:  12%|█▏        | 12/100 [28:27<3:12:40, 131.37s/it]Best trial: 4. Best value: 0.969465:  13%|█▎        | 13/100 [28:27<3:41:15, 152.59s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                        Best trial: 4. Best value: 0.969465:  13%|█▎        | 13/100 [28:28<3:41:15, 152.59s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 20.1 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
20.1 M    Trainable params
0         Non-trainable params
20.1 M    Total params
80.523    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.130
Metric Loss/val improved by 0.086 >= min_delta = 0.0. New best score: 1.044
Metric Loss/val improved by 0.012 >= min_delta = 0.0. New best score: 1.032
Metric Loss/val improved by 0.003 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                        Best trial: 4. Best value: 0.969465:  13%|█▎        | 13/100 [31:13<3:41:15, 152.59s/it]Best trial: 4. Best value: 0.969465:  13%|█▎        | 13/100 [31:13<3:41:15, 152.59s/it]Best trial: 4. Best value: 0.969465:  14%|█▍        | 14/100 [31:13<3:44:09, 156.39s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                        Best trial: 4. Best value: 0.969465:  14%|█▍        | 14/100 [31:13<3:44:09, 156.39s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=352, n_heads=4, e_layers=8, hidden_d_model=192, seq_layers=2, last_d_model=448, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=48, time_d_model=128, pos_d_model=48, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.18, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=224), exp_settings='seq_len-12-lr-0.0058-d-352-hid_d-192-last_d-448-tok_d-48-time_d-128-pos_d-48-e_layers-8-tok_conv_k-9-dropout-0.18-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.005798118139299892, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 00:06:43,634] Trial 24 finished with value: 0.9972689151763917 and parameters: {'num_heads': 36, 'hidden_d_model': 192, 'token_conv_kernel': 9, 'last_d_model': 448, 'seq_len': 12, 'token_d_model': 48, 'time_d_model': 128, 'pos_d_model': 48, 'd_model': 352, 'conv_out_dim': 224, 'e_layers': 8, 'learning_rate': 0.005798118139299892, 'dropout': 0.18, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 00:06:43,997] The parameter 'norm_type' in trial#27 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=8, input_dim=84, dec_in=84, output_dim=1, d_model=320, n_heads=4, e_layers=8, hidden_d_model=192, seq_layers=2, last_d_model=896, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=64, pos_d_model=80, combine_type='add', seq_len=14, pred_len=1, min_y_value=0.0, dropout=0.2, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=288), exp_settings='seq_len-14-lr-0.009-d-320-hid_d-192-last_d-896-tok_d-64-time_d-64-pos_d-80-e_layers-8-tok_conv_k-8-dropout-0.2-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.008989262660485324, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 00:09:28,818] Trial 27 finished with value: 0.9960351467132569 and parameters: {'num_heads': 36, 'hidden_d_model': 192, 'token_conv_kernel': 8, 'last_d_model': 896, 'seq_len': 14, 'token_d_model': 64, 'time_d_model': 64, 'pos_d_model': 80, 'd_model': 320, 'conv_out_dim': 288, 'e_layers': 8, 'learning_rate': 0.008989262660485324, 'dropout': 0.2, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 00:09:29,376] The parameter 'norm_type' in trial#29 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 10.2 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
10.2 M    Trainable params
0         Non-trainable params
10.2 M    Total params
40.641    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 10.000
Metric Loss/val improved by 6.843 >= min_delta = 0.0. New best score: 3.157
Metric Loss/val improved by 2.082 >= min_delta = 0.0. New best score: 1.075
Metric Loss/val improved by 0.046 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                        Best trial: 4. Best value: 0.969465:  14%|█▍        | 14/100 [33:12<3:44:09, 156.39s/it]Best trial: 4. Best value: 0.969465:  14%|█▍        | 14/100 [33:12<3:44:09, 156.39s/it]Best trial: 4. Best value: 0.969465:  15%|█▌        | 15/100 [33:12<3:25:43, 145.21s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                        Best trial: 4. Best value: 0.969465:  15%|█▌        | 15/100 [33:12<3:25:43, 145.21s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 5.4 M  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
5.4 M     Trainable params
0         Non-trainable params
5.4 M     Total params
21.496    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.035
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 1.033
Metric Loss/val improved by 0.004 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                        Best trial: 4. Best value: 0.969465:  15%|█▌        | 15/100 [35:20<3:25:43, 145.21s/it]Best trial: 4. Best value: 0.969465:  15%|█▌        | 15/100 [35:20<3:25:43, 145.21s/it]Best trial: 4. Best value: 0.969465:  16%|█▌        | 16/100 [35:20<3:16:17, 140.20s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                        Best trial: 4. Best value: 0.969465:  16%|█▌        | 16/100 [35:21<3:16:17, 140.20s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=7, input_dim=84, dec_in=84, output_dim=1, d_model=320, n_heads=4, e_layers=6, hidden_d_model=176, seq_layers=2, last_d_model=640, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=48, time_d_model=112, pos_d_model=96, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.18, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=160), exp_settings='seq_len-12-lr-0.0533-d-320-hid_d-176-last_d-640-tok_d-48-time_d-112-pos_d-96-e_layers-6-tok_conv_k-7-dropout-0.18-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.05334932174790658, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 00:11:28,119] Trial 29 finished with value: 0.9964828968048096 and parameters: {'num_heads': 36, 'hidden_d_model': 176, 'token_conv_kernel': 7, 'last_d_model': 640, 'seq_len': 12, 'token_d_model': 48, 'time_d_model': 112, 'pos_d_model': 96, 'd_model': 320, 'conv_out_dim': 160, 'e_layers': 6, 'learning_rate': 0.05334932174790658, 'dropout': 0.18, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 00:11:28,388] The parameter 'norm_type' in trial#31 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=6, input_dim=84, dec_in=84, output_dim=1, d_model=224, n_heads=4, e_layers=4, hidden_d_model=192, seq_layers=2, last_d_model=704, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=80, pos_d_model=80, combine_type='add', seq_len=16, pred_len=1, min_y_value=0.0, dropout=0.18, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=96), exp_settings='seq_len-16-lr-0.0094-d-224-hid_d-192-last_d-704-tok_d-64-time_d-80-pos_d-80-e_layers-4-tok_conv_k-6-dropout-0.18-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.009365300942440234, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 00:13:36,693] Trial 31 finished with value: 0.9965043544769288 and parameters: {'num_heads': 36, 'hidden_d_model': 192, 'token_conv_kernel': 6, 'last_d_model': 704, 'seq_len': 16, 'token_d_model': 64, 'time_d_model': 80, 'pos_d_model': 80, 'd_model': 224, 'conv_out_dim': 96, 'e_layers': 4, 'learning_rate': 0.009365300942440234, 'dropout': 0.18, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 00:13:36,959] The parameter 'norm_type' in trial#33 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 17.4 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
17.4 M    Trainable params
0         Non-trainable params
17.4 M    Total params
69.755    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 10.000
Metric Loss/val improved by 8.952 >= min_delta = 0.0. New best score: 1.048
Metric Loss/val improved by 0.015 >= min_delta = 0.0. New best score: 1.034
Metric Loss/val improved by 0.004 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                        Best trial: 4. Best value: 0.969465:  16%|█▌        | 16/100 [37:31<3:16:17, 140.20s/it]Best trial: 4. Best value: 0.969465:  16%|█▌        | 16/100 [37:31<3:16:17, 140.20s/it]Best trial: 4. Best value: 0.969465:  17%|█▋        | 17/100 [37:31<3:09:59, 137.35s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                        Best trial: 4. Best value: 0.969465:  17%|█▋        | 17/100 [37:32<3:09:59, 137.35s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 14.7 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
14.7 M    Trainable params
0         Non-trainable params
14.7 M    Total params
58.899    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.619
Metric Loss/val improved by 0.365 >= min_delta = 0.0. New best score: 1.254
Metric Loss/val improved by 0.142 >= min_delta = 0.0. New best score: 1.112
Metric Loss/val improved by 0.083 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                        Best trial: 4. Best value: 0.969465:  17%|█▋        | 17/100 [39:39<3:09:59, 137.35s/it]Best trial: 4. Best value: 0.969465:  17%|█▋        | 17/100 [39:39<3:09:59, 137.35s/it]Best trial: 4. Best value: 0.969465:  18%|█▊        | 18/100 [39:39<3:03:53, 134.56s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                        Best trial: 4. Best value: 0.969465:  18%|█▊        | 18/100 [39:40<3:03:53, 134.56s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=7, input_dim=84, dec_in=84, output_dim=1, d_model=480, n_heads=4, e_layers=6, hidden_d_model=192, seq_layers=2, last_d_model=1280, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=48, time_d_model=112, pos_d_model=80, combine_type='add', seq_len=14, pred_len=1, min_y_value=0.0, dropout=0.18, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=68, fc_layer_type='mha', conv_out_dim=192), exp_settings='seq_len-14-lr-0.0185-d-480-hid_d-192-last_d-1280-tok_d-48-time_d-112-pos_d-80-e_layers-6-tok_conv_k-7-dropout-0.18-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-68-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.01850047516102123, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 00:15:47,389] Trial 33 finished with value: 0.9958489775657655 and parameters: {'num_heads': 68, 'hidden_d_model': 192, 'token_conv_kernel': 7, 'last_d_model': 1280, 'seq_len': 14, 'token_d_model': 48, 'time_d_model': 112, 'pos_d_model': 80, 'd_model': 480, 'conv_out_dim': 192, 'e_layers': 6, 'learning_rate': 0.01850047516102123, 'dropout': 0.18, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 00:15:48,058] The parameter 'norm_type' in trial#35 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=8, input_dim=84, dec_in=84, output_dim=1, d_model=384, n_heads=4, e_layers=6, hidden_d_model=176, seq_layers=2, last_d_model=640, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=96, pos_d_model=64, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.18, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=160), exp_settings='seq_len-12-lr-0.0463-d-384-hid_d-176-last_d-640-tok_d-64-time_d-96-pos_d-64-e_layers-6-tok_conv_k-8-dropout-0.18-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.04634575028196202, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 00:17:55,448] Trial 35 finished with value: 0.9964472532272339 and parameters: {'num_heads': 36, 'hidden_d_model': 176, 'token_conv_kernel': 8, 'last_d_model': 640, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 96, 'pos_d_model': 64, 'd_model': 384, 'conv_out_dim': 160, 'e_layers': 6, 'learning_rate': 0.04634575028196202, 'dropout': 0.18, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 00:17:55,716] The parameter 'norm_type' in trial#36 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 16.3 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
16.3 M    Trainable params
0         Non-trainable params
16.3 M    Total params
65.231    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                        Best trial: 4. Best value: 0.969465:  18%|█▊        | 18/100 [41:35<3:03:53, 134.56s/it]Best trial: 4. Best value: 0.969465:  18%|█▊        | 18/100 [41:35<3:03:53, 134.56s/it]Best trial: 4. Best value: 0.969465:  19%|█▉        | 19/100 [41:35<2:53:58, 128.88s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                        Best trial: 4. Best value: 0.969465:  19%|█▉        | 19/100 [41:35<2:53:58, 128.88s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 26.2 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
26.2 M    Trainable params
0         Non-trainable params
26.2 M    Total params
104.846   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 10.000
Metric Loss/val improved by 8.562 >= min_delta = 0.0. New best score: 1.438
Metric Loss/val improved by 0.402 >= min_delta = 0.0. New best score: 1.036
Metric Loss/val improved by 0.006 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 1.028
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.028. Signaling Trainer to stop.
                                                                                        Best trial: 4. Best value: 0.969465:  19%|█▉        | 19/100 [44:03<2:53:58, 128.88s/it]Best trial: 4. Best value: 0.969465:  19%|█▉        | 19/100 [44:03<2:53:58, 128.88s/it]Best trial: 4. Best value: 0.969465:  20%|██        | 20/100 [44:03<2:59:20, 134.51s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                        Best trial: 4. Best value: 0.969465:  20%|██        | 20/100 [44:03<2:59:20, 134.51s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=7, input_dim=84, dec_in=84, output_dim=1, d_model=480, n_heads=4, e_layers=6, hidden_d_model=176, seq_layers=2, last_d_model=1024, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=48, time_d_model=112, pos_d_model=80, combine_type='add', seq_len=14, pred_len=1, min_y_value=0.0, dropout=0.24000000000000002, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=68, fc_layer_type='mha', conv_out_dim=160), exp_settings='seq_len-14-lr-0.0056-d-480-hid_d-176-last_d-1024-tok_d-48-time_d-112-pos_d-80-e_layers-6-tok_conv_k-7-dropout-0.24000000000000002-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-68-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.005603774943166356, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 00:19:51,094] Trial 36 finished with value: 1.001753044128418 and parameters: {'num_heads': 68, 'hidden_d_model': 176, 'token_conv_kernel': 7, 'last_d_model': 1024, 'seq_len': 14, 'token_d_model': 48, 'time_d_model': 112, 'pos_d_model': 80, 'd_model': 480, 'conv_out_dim': 160, 'e_layers': 6, 'learning_rate': 0.005603774943166356, 'dropout': 0.24000000000000002, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 00:19:51,406] The parameter 'norm_type' in trial#39 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=8, input_dim=84, dec_in=84, output_dim=1, d_model=480, n_heads=4, e_layers=6, hidden_d_model=192, seq_layers=2, last_d_model=1120, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=80, pos_d_model=80, combine_type='add', seq_len=14, pred_len=1, min_y_value=0.0, dropout=0.26, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=320), exp_settings='seq_len-14-lr-0.031-d-480-hid_d-192-last_d-1120-tok_d-64-time_d-80-pos_d-80-e_layers-6-tok_conv_k-8-dropout-0.26-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.03099463310724135, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 00:22:18,727] Trial 39 finished with value: 2.4116066932678226 and parameters: {'num_heads': 36, 'hidden_d_model': 192, 'token_conv_kernel': 8, 'last_d_model': 1120, 'seq_len': 14, 'token_d_model': 64, 'time_d_model': 80, 'pos_d_model': 80, 'd_model': 480, 'conv_out_dim': 320, 'e_layers': 6, 'learning_rate': 0.03099463310724135, 'dropout': 0.26, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 00:22:19,003] The parameter 'norm_type' in trial#40 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 17.4 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
17.4 M    Trainable params
0         Non-trainable params
17.4 M    Total params
69.601    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.109
Metric Loss/val improved by 0.063 >= min_delta = 0.0. New best score: 1.047
Metric Loss/val improved by 0.017 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                        Best trial: 4. Best value: 0.969465:  20%|██        | 20/100 [46:17<2:59:20, 134.51s/it]Best trial: 4. Best value: 0.969465:  20%|██        | 20/100 [46:17<2:59:20, 134.51s/it]Best trial: 4. Best value: 0.969465:  21%|██        | 21/100 [46:17<2:56:58, 134.41s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                        Best trial: 4. Best value: 0.969465:  21%|██        | 21/100 [46:17<2:56:58, 134.41s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 11.2 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
11.2 M    Trainable params
0         Non-trainable params
11.2 M    Total params
44.898    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.065
Metric Loss/val improved by 0.020 >= min_delta = 0.0. New best score: 1.045
Metric Loss/val improved by 0.011 >= min_delta = 0.0. New best score: 1.034
Metric Loss/val improved by 0.004 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                        Best trial: 4. Best value: 0.969465:  21%|██        | 21/100 [48:42<2:56:58, 134.41s/it]Best trial: 4. Best value: 0.969465:  21%|██        | 21/100 [48:42<2:56:58, 134.41s/it]Best trial: 4. Best value: 0.969465:  22%|██▏       | 22/100 [48:42<2:59:00, 137.69s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                        Best trial: 4. Best value: 0.969465:  22%|██▏       | 22/100 [48:42<2:59:00, 137.69s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=288, n_heads=4, e_layers=6, hidden_d_model=176, seq_layers=2, last_d_model=384, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=128, pos_d_model=32, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.22, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=4, fc_layer_type='mha', conv_out_dim=256), exp_settings='seq_len-12-lr-0.0154-d-288-hid_d-176-last_d-384-tok_d-64-time_d-128-pos_d-32-e_layers-6-tok_conv_k-9-dropout-0.22-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-4-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.015422254332020317, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 00:24:32,907] Trial 40 finished with value: 0.9960134983062745 and parameters: {'num_heads': 4, 'hidden_d_model': 176, 'token_conv_kernel': 9, 'last_d_model': 384, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 128, 'pos_d_model': 32, 'd_model': 288, 'conv_out_dim': 256, 'e_layers': 6, 'learning_rate': 0.015422254332020317, 'dropout': 0.22, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 00:24:33,191] The parameter 'norm_type' in trial#42 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=8, input_dim=84, dec_in=84, output_dim=1, d_model=256, n_heads=4, e_layers=4, hidden_d_model=208, seq_layers=2, last_d_model=224, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=112, pos_d_model=96, combine_type='add', seq_len=14, pred_len=1, min_y_value=0.0, dropout=0.2, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=192), exp_settings='seq_len-14-lr-0.0128-d-256-hid_d-208-last_d-224-tok_d-64-time_d-112-pos_d-96-e_layers-4-tok_conv_k-8-dropout-0.2-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.012833273060858802, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 00:26:58,263] Trial 42 finished with value: 0.9958356857299805 and parameters: {'num_heads': 36, 'hidden_d_model': 208, 'token_conv_kernel': 8, 'last_d_model': 224, 'seq_len': 14, 'token_d_model': 64, 'time_d_model': 112, 'pos_d_model': 96, 'd_model': 256, 'conv_out_dim': 192, 'e_layers': 4, 'learning_rate': 0.012833273060858802, 'dropout': 0.2, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 00:26:58,544] The parameter 'norm_type' in trial#44 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 17.7 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
17.7 M    Trainable params
0         Non-trainable params
17.7 M    Total params
70.798    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.068
Metric Loss/val improved by 0.030 >= min_delta = 0.0. New best score: 1.038
Metric Loss/val improved by 0.008 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                        Best trial: 4. Best value: 0.969465:  22%|██▏       | 22/100 [50:42<2:59:00, 137.69s/it]Best trial: 4. Best value: 0.969465:  22%|██▏       | 22/100 [50:42<2:59:00, 137.69s/it]Best trial: 4. Best value: 0.969465:  23%|██▎       | 23/100 [50:42<2:49:55, 132.41s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                        Best trial: 4. Best value: 0.969465:  23%|██▎       | 23/100 [50:42<2:49:55, 132.41s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 14.0 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
14.0 M    Trainable params
0         Non-trainable params
14.0 M    Total params
55.978    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.030
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                        Best trial: 4. Best value: 0.969465:  23%|██▎       | 23/100 [53:40<2:49:55, 132.41s/it]Best trial: 4. Best value: 0.969465:  23%|██▎       | 23/100 [53:40<2:49:55, 132.41s/it]Best trial: 4. Best value: 0.969465:  24%|██▍       | 24/100 [53:40<3:05:09, 146.17s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                        Best trial: 4. Best value: 0.969465:  24%|██▍       | 24/100 [53:41<3:05:09, 146.17s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=8, input_dim=84, dec_in=84, output_dim=1, d_model=384, n_heads=4, e_layers=6, hidden_d_model=176, seq_layers=2, last_d_model=512, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=128, pos_d_model=80, combine_type='add', seq_len=14, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=4, fc_layer_type='mha', conv_out_dim=224), exp_settings='seq_len-14-lr-0.0145-d-384-hid_d-176-last_d-512-tok_d-64-time_d-128-pos_d-80-e_layers-6-tok_conv_k-8-dropout-0.14-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-4-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.014473356534446917, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 00:28:58,336] Trial 44 finished with value: 2.230116748809815 and parameters: {'num_heads': 4, 'hidden_d_model': 176, 'token_conv_kernel': 8, 'last_d_model': 512, 'seq_len': 14, 'token_d_model': 64, 'time_d_model': 128, 'pos_d_model': 80, 'd_model': 384, 'conv_out_dim': 224, 'e_layers': 6, 'learning_rate': 0.014473356534446917, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 00:28:58,605] The parameter 'norm_type' in trial#45 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=6, input_dim=84, dec_in=84, output_dim=1, d_model=384, n_heads=4, e_layers=8, hidden_d_model=176, seq_layers=2, last_d_model=832, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=48, time_d_model=96, pos_d_model=64, combine_type='add', seq_len=14, pred_len=1, min_y_value=0.0, dropout=0.2, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=160), exp_settings='seq_len-14-lr-0.0098-d-384-hid_d-176-last_d-832-tok_d-48-time_d-96-pos_d-64-e_layers-8-tok_conv_k-6-dropout-0.2-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.00977151961767304, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 00:31:56,625] Trial 45 finished with value: 0.9951676487922669 and parameters: {'num_heads': 36, 'hidden_d_model': 176, 'token_conv_kernel': 6, 'last_d_model': 832, 'seq_len': 14, 'token_d_model': 48, 'time_d_model': 96, 'pos_d_model': 64, 'd_model': 384, 'conv_out_dim': 160, 'e_layers': 8, 'learning_rate': 0.00977151961767304, 'dropout': 0.2, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 00:31:56,880] The parameter 'norm_type' in trial#48 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 14.5 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
14.5 M    Trainable params
0         Non-trainable params
14.5 M    Total params
57.928    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.090
Metric Loss/val improved by 0.055 >= min_delta = 0.0. New best score: 1.035
Metric Loss/val improved by 0.006 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                        Best trial: 4. Best value: 0.969465:  24%|██▍       | 24/100 [56:01<3:05:09, 146.17s/it]Best trial: 4. Best value: 0.969465:  24%|██▍       | 24/100 [56:01<3:05:09, 146.17s/it]Best trial: 4. Best value: 0.969465:  25%|██▌       | 25/100 [56:01<3:00:48, 144.64s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                        Best trial: 4. Best value: 0.969465:  25%|██▌       | 25/100 [56:02<3:00:48, 144.64s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 13.5 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
13.5 M    Trainable params
0         Non-trainable params
13.5 M    Total params
53.999    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 2.259
Metric Loss/val improved by 1.167 >= min_delta = 0.0. New best score: 1.092
Metric Loss/val improved by 0.060 >= min_delta = 0.0. New best score: 1.032
Metric Loss/val improved by 0.003 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                        Best trial: 4. Best value: 0.969465:  25%|██▌       | 25/100 [58:05<3:00:48, 144.64s/it]Best trial: 4. Best value: 0.969465:  25%|██▌       | 25/100 [58:05<3:00:48, 144.64s/it]Best trial: 4. Best value: 0.969465:  26%|██▌       | 26/100 [58:05<2:50:40, 138.39s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                        Best trial: 4. Best value: 0.969465:  26%|██▌       | 26/100 [58:06<2:50:40, 138.39s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=6, input_dim=84, dec_in=84, output_dim=1, d_model=320, n_heads=4, e_layers=6, hidden_d_model=192, seq_layers=2, last_d_model=1504, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=128, pos_d_model=64, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.2, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=256), exp_settings='seq_len-12-lr-0.0154-d-320-hid_d-192-last_d-1504-tok_d-64-time_d-128-pos_d-64-e_layers-6-tok_conv_k-6-dropout-0.2-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.015376351154850669, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 00:34:17,685] Trial 48 finished with value: 0.9964164495468141 and parameters: {'num_heads': 36, 'hidden_d_model': 192, 'token_conv_kernel': 6, 'last_d_model': 1504, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 128, 'pos_d_model': 64, 'd_model': 320, 'conv_out_dim': 256, 'e_layers': 6, 'learning_rate': 0.015376351154850669, 'dropout': 0.2, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 00:34:17,963] The parameter 'norm_type' in trial#49 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=7, input_dim=84, dec_in=84, output_dim=1, d_model=480, n_heads=4, e_layers=4, hidden_d_model=176, seq_layers=2, last_d_model=832, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=48, time_d_model=112, pos_d_model=64, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.2, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=4, fc_layer_type='mha', conv_out_dim=192), exp_settings='seq_len-12-lr-0.0228-d-480-hid_d-176-last_d-832-tok_d-48-time_d-112-pos_d-64-e_layers-4-tok_conv_k-7-dropout-0.2-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-4-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.022801993098794434, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 00:36:21,499] Trial 49 finished with value: 0.9968778610229493 and parameters: {'num_heads': 4, 'hidden_d_model': 176, 'token_conv_kernel': 7, 'last_d_model': 832, 'seq_len': 12, 'token_d_model': 48, 'time_d_model': 112, 'pos_d_model': 64, 'd_model': 480, 'conv_out_dim': 192, 'e_layers': 4, 'learning_rate': 0.022801993098794434, 'dropout': 0.2, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 00:36:21,789] The parameter 'norm_type' in trial#51 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 11.5 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
11.5 M    Trainable params
0         Non-trainable params
11.5 M    Total params
46.082    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.040
Metric Loss/val improved by 0.011 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                        Best trial: 4. Best value: 0.969465:  26%|██▌       | 26/100 [1:01:12<2:50:40, 138.39s/it]Best trial: 4. Best value: 0.969465:  26%|██▌       | 26/100 [1:01:12<2:50:40, 138.39s/it]Best trial: 4. Best value: 0.969465:  27%|██▋       | 27/100 [1:01:12<3:05:51, 152.76s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                          Best trial: 4. Best value: 0.969465:  27%|██▋       | 27/100 [1:01:12<3:05:51, 152.76s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 17.6 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
17.6 M    Trainable params
0         Non-trainable params
17.6 M    Total params
70.363    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                          Best trial: 4. Best value: 0.969465:  27%|██▋       | 27/100 [1:03:36<3:05:51, 152.76s/it]Best trial: 4. Best value: 0.969465:  27%|██▋       | 27/100 [1:03:36<3:05:51, 152.76s/it]Best trial: 4. Best value: 0.969465:  28%|██▊       | 28/100 [1:03:36<3:00:22, 150.31s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                          Best trial: 4. Best value: 0.969465:  28%|██▊       | 28/100 [1:03:37<3:00:22, 150.31s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=7, input_dim=84, dec_in=84, output_dim=1, d_model=352, n_heads=4, e_layers=6, hidden_d_model=208, seq_layers=2, last_d_model=928, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=80, pos_d_model=48, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=68, fc_layer_type='mha', conv_out_dim=128), exp_settings='seq_len-12-lr-0.0078-d-352-hid_d-208-last_d-928-tok_d-64-time_d-80-pos_d-48-e_layers-6-tok_conv_k-7-dropout-0.16-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-68-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.007803325334634404, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 00:39:27,772] Trial 51 finished with value: 1.011366617679596 and parameters: {'num_heads': 68, 'hidden_d_model': 208, 'token_conv_kernel': 7, 'last_d_model': 928, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 80, 'pos_d_model': 48, 'd_model': 352, 'conv_out_dim': 128, 'e_layers': 6, 'learning_rate': 0.007803325334634404, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 00:39:28,045] The parameter 'norm_type' in trial#53 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=8, input_dim=84, dec_in=84, output_dim=1, d_model=416, n_heads=4, e_layers=6, hidden_d_model=208, seq_layers=2, last_d_model=1024, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=48, time_d_model=80, pos_d_model=64, combine_type='add', seq_len=10, pred_len=1, min_y_value=0.0, dropout=0.18, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=68, fc_layer_type='mha', conv_out_dim=256), exp_settings='seq_len-10-lr-0.0089-d-416-hid_d-208-last_d-1024-tok_d-48-time_d-80-pos_d-64-e_layers-6-tok_conv_k-8-dropout-0.18-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-68-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.008939201286273358, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 00:41:52,373] Trial 53 finished with value: 0.9971089363098145 and parameters: {'num_heads': 68, 'hidden_d_model': 208, 'token_conv_kernel': 8, 'last_d_model': 1024, 'seq_len': 10, 'token_d_model': 48, 'time_d_model': 80, 'pos_d_model': 64, 'd_model': 416, 'conv_out_dim': 256, 'e_layers': 6, 'learning_rate': 0.008939201286273358, 'dropout': 0.18, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 00:41:52,722] The parameter 'norm_type' in trial#55 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 16.3 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
16.3 M    Trainable params
0         Non-trainable params
16.3 M    Total params
65.167    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.045
Metric Loss/val improved by 0.014 >= min_delta = 0.0. New best score: 1.031
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                          Best trial: 4. Best value: 0.969465:  28%|██▊       | 28/100 [1:06:04<3:00:22, 150.31s/it]Best trial: 4. Best value: 0.969465:  28%|██▊       | 28/100 [1:06:04<3:00:22, 150.31s/it]Best trial: 4. Best value: 0.969465:  29%|██▉       | 29/100 [1:06:04<2:56:52, 149.48s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                          Best trial: 4. Best value: 0.969465:  29%|██▉       | 29/100 [1:06:04<2:56:52, 149.48s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 14.8 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
14.8 M    Trainable params
0         Non-trainable params
14.8 M    Total params
59.358    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                          Best trial: 4. Best value: 0.969465:  29%|██▉       | 29/100 [1:07:58<2:56:52, 149.48s/it]Best trial: 4. Best value: 0.969465:  29%|██▉       | 29/100 [1:07:58<2:56:52, 149.48s/it]Best trial: 4. Best value: 0.969465:  30%|███       | 30/100 [1:07:58<2:41:55, 138.79s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                          Best trial: 4. Best value: 0.969465:  30%|███       | 30/100 [1:07:58<2:41:55, 138.79s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=7, input_dim=84, dec_in=84, output_dim=1, d_model=352, n_heads=4, e_layers=8, hidden_d_model=208, seq_layers=2, last_d_model=1440, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=64, pos_d_model=96, combine_type='add', seq_len=10, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=192), exp_settings='seq_len-10-lr-0.0092-d-352-hid_d-208-last_d-1440-tok_d-64-time_d-64-pos_d-96-e_layers-8-tok_conv_k-7-dropout-0.14-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.009249282564991742, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 00:44:19,908] Trial 55 finished with value: 0.9962574243545532 and parameters: {'num_heads': 36, 'hidden_d_model': 208, 'token_conv_kernel': 7, 'last_d_model': 1440, 'seq_len': 10, 'token_d_model': 64, 'time_d_model': 64, 'pos_d_model': 96, 'd_model': 352, 'conv_out_dim': 192, 'e_layers': 8, 'learning_rate': 0.009249282564991742, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 00:44:20,179] The parameter 'norm_type' in trial#57 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=7, input_dim=84, dec_in=84, output_dim=1, d_model=352, n_heads=4, e_layers=8, hidden_d_model=160, seq_layers=2, last_d_model=960, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=80, pos_d_model=80, combine_type='add', seq_len=10, pred_len=1, min_y_value=0.0, dropout=0.18, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=160), exp_settings='seq_len-10-lr-0.0055-d-352-hid_d-160-last_d-960-tok_d-64-time_d-80-pos_d-80-e_layers-8-tok_conv_k-7-dropout-0.18-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.005534986435681183, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 00:46:13,776] Trial 57 finished with value: 1.0055178165435792 and parameters: {'num_heads': 36, 'hidden_d_model': 160, 'token_conv_kernel': 7, 'last_d_model': 960, 'seq_len': 10, 'token_d_model': 64, 'time_d_model': 80, 'pos_d_model': 80, 'd_model': 352, 'conv_out_dim': 160, 'e_layers': 8, 'learning_rate': 0.005534986435681183, 'dropout': 0.18, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 00:46:14,032] The parameter 'norm_type' in trial#58 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 25.0 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
25.0 M    Trainable params
0         Non-trainable params
25.0 M    Total params
100.190   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.048
Metric Loss/val improved by 0.017 >= min_delta = 0.0. New best score: 1.031
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.031
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                          Best trial: 4. Best value: 0.969465:  30%|███       | 30/100 [1:11:47<2:41:55, 138.79s/it]Best trial: 4. Best value: 0.969465:  30%|███       | 30/100 [1:11:47<2:41:55, 138.79s/it]Best trial: 4. Best value: 0.969465:  31%|███       | 31/100 [1:11:47<3:10:44, 165.87s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                          Best trial: 4. Best value: 0.969465:  31%|███       | 31/100 [1:11:47<3:10:44, 165.87s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 25.6 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
25.6 M    Trainable params
0         Non-trainable params
25.6 M    Total params
102.377   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.046
Metric Loss/val improved by 0.013 >= min_delta = 0.0. New best score: 1.032
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.031
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                          Best trial: 4. Best value: 0.969465:  31%|███       | 31/100 [1:15:37<3:10:44, 165.87s/it]Best trial: 4. Best value: 0.969465:  31%|███       | 31/100 [1:15:37<3:10:44, 165.87s/it]Best trial: 4. Best value: 0.969465:  32%|███▏      | 32/100 [1:15:37<3:29:54, 185.21s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                          Best trial: 4. Best value: 0.969465:  32%|███▏      | 32/100 [1:15:37<3:29:54, 185.21s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=7, input_dim=84, dec_in=84, output_dim=1, d_model=480, n_heads=4, e_layers=8, hidden_d_model=176, seq_layers=2, last_d_model=160, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=80, pos_d_model=64, combine_type='add', seq_len=14, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=256), exp_settings='seq_len-14-lr-0.0049-d-480-hid_d-176-last_d-160-tok_d-64-time_d-80-pos_d-64-e_layers-8-tok_conv_k-7-dropout-0.16-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.004865690448001845, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 00:50:02,811] Trial 58 finished with value: 0.9993835449218751 and parameters: {'num_heads': 36, 'hidden_d_model': 176, 'token_conv_kernel': 7, 'last_d_model': 160, 'seq_len': 14, 'token_d_model': 64, 'time_d_model': 80, 'pos_d_model': 64, 'd_model': 480, 'conv_out_dim': 256, 'e_layers': 8, 'learning_rate': 0.004865690448001845, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 00:50:03,082] The parameter 'norm_type' in trial#61 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=7, input_dim=84, dec_in=84, output_dim=1, d_model=416, n_heads=4, e_layers=8, hidden_d_model=208, seq_layers=2, last_d_model=800, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=32, pos_d_model=112, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.18, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=352), exp_settings='seq_len-12-lr-0.0073-d-416-hid_d-208-last_d-800-tok_d-64-time_d-32-pos_d-112-e_layers-8-tok_conv_k-7-dropout-0.18-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.007277546259701513, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 00:53:53,147] Trial 61 finished with value: 3.0526556968688965 and parameters: {'num_heads': 36, 'hidden_d_model': 208, 'token_conv_kernel': 7, 'last_d_model': 800, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 32, 'pos_d_model': 112, 'd_model': 416, 'conv_out_dim': 352, 'e_layers': 8, 'learning_rate': 0.007277546259701513, 'dropout': 0.18, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 00:53:53,472] The parameter 'norm_type' in trial#63 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 13.8 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
13.8 M    Trainable params
0         Non-trainable params
13.8 M    Total params
55.318    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.040
Metric Loss/val improved by 0.010 >= min_delta = 0.0. New best score: 1.031
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                          Best trial: 4. Best value: 0.969465:  32%|███▏      | 32/100 [1:18:30<3:29:54, 185.21s/it]Best trial: 4. Best value: 0.969465:  32%|███▏      | 32/100 [1:18:30<3:29:54, 185.21s/it]Best trial: 4. Best value: 0.969465:  33%|███▎      | 33/100 [1:18:30<3:22:47, 181.60s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                          Best trial: 4. Best value: 0.969465:  33%|███▎      | 33/100 [1:18:30<3:22:47, 181.60s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 27.9 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
27.9 M    Trainable params
0         Non-trainable params
27.9 M    Total params
111.670   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 7.910
Metric Loss/val improved by 6.814 >= min_delta = 0.0. New best score: 1.096
Metric Loss/val improved by 0.066 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                          Best trial: 4. Best value: 0.969465:  33%|███▎      | 33/100 [1:21:25<3:22:47, 181.60s/it]Best trial: 4. Best value: 0.969465:  33%|███▎      | 33/100 [1:21:25<3:22:47, 181.60s/it]Best trial: 4. Best value: 0.969465:  34%|███▍      | 34/100 [1:21:25<3:17:40, 179.71s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                          Best trial: 4. Best value: 0.969465:  34%|███▍      | 34/100 [1:21:26<3:17:40, 179.71s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=8, input_dim=84, dec_in=84, output_dim=1, d_model=352, n_heads=4, e_layers=6, hidden_d_model=224, seq_layers=2, last_d_model=1056, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=96, pos_d_model=112, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=160), exp_settings='seq_len-12-lr-0.0255-d-352-hid_d-224-last_d-1056-tok_d-64-time_d-96-pos_d-112-e_layers-6-tok_conv_k-8-dropout-0.16-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.02553067776959523, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 00:56:46,340] Trial 63 finished with value: 1.0042396545410157 and parameters: {'num_heads': 36, 'hidden_d_model': 224, 'token_conv_kernel': 8, 'last_d_model': 1056, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 96, 'pos_d_model': 112, 'd_model': 352, 'conv_out_dim': 160, 'e_layers': 6, 'learning_rate': 0.02553067776959523, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 00:56:46,606] The parameter 'norm_type' in trial#65 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=384, n_heads=4, e_layers=8, hidden_d_model=208, seq_layers=2, last_d_model=576, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=80, pos_d_model=112, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=68, fc_layer_type='mha', conv_out_dim=352), exp_settings='seq_len-12-lr-0.0451-d-384-hid_d-208-last_d-576-tok_d-64-time_d-80-pos_d-112-e_layers-8-tok_conv_k-9-dropout-0.16-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-68-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.04509451300277886, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 00:59:41,621] Trial 65 finished with value: 0.9964639067649841 and parameters: {'num_heads': 68, 'hidden_d_model': 208, 'token_conv_kernel': 9, 'last_d_model': 576, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 80, 'pos_d_model': 112, 'd_model': 384, 'conv_out_dim': 352, 'e_layers': 8, 'learning_rate': 0.04509451300277886, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 00:59:41,878] The parameter 'norm_type' in trial#67 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 20.4 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
20.4 M    Trainable params
0         Non-trainable params
20.4 M    Total params
81.588    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.086
Metric Loss/val improved by 0.026 >= min_delta = 0.0. New best score: 1.060
Metric Loss/val improved by 0.030 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                          Best trial: 4. Best value: 0.969465:  34%|███▍      | 34/100 [1:23:46<3:17:40, 179.71s/it]Best trial: 4. Best value: 0.969465:  34%|███▍      | 34/100 [1:23:46<3:17:40, 179.71s/it]Best trial: 4. Best value: 0.969465:  35%|███▌      | 35/100 [1:23:46<3:01:48, 167.83s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                          Best trial: 4. Best value: 0.969465:  35%|███▌      | 35/100 [1:23:46<3:01:48, 167.83s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 21.0 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
21.0 M    Trainable params
0         Non-trainable params
21.0 M    Total params
83.951    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.046
Metric Loss/val improved by 0.014 >= min_delta = 0.0. New best score: 1.033
Metric Loss/val improved by 0.003 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                          Best trial: 4. Best value: 0.969465:  35%|███▌      | 35/100 [1:27:25<3:01:48, 167.83s/it]Best trial: 4. Best value: 0.969465:  35%|███▌      | 35/100 [1:27:25<3:01:48, 167.83s/it]Best trial: 4. Best value: 0.969465:  36%|███▌      | 36/100 [1:27:25<3:15:40, 183.44s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                          Best trial: 4. Best value: 0.969465:  36%|███▌      | 36/100 [1:27:26<3:15:40, 183.44s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=8, input_dim=84, dec_in=84, output_dim=1, d_model=288, n_heads=4, e_layers=8, hidden_d_model=192, seq_layers=2, last_d_model=704, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=96, pos_d_model=96, combine_type='add', seq_len=10, pred_len=1, min_y_value=0.0, dropout=0.2, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=320), exp_settings='seq_len-10-lr-0.0114-d-288-hid_d-192-last_d-704-tok_d-64-time_d-96-pos_d-96-e_layers-8-tok_conv_k-8-dropout-0.2-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.011404648159175907, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 01:02:01,729] Trial 67 finished with value: 0.9968162536621095 and parameters: {'num_heads': 36, 'hidden_d_model': 192, 'token_conv_kernel': 8, 'last_d_model': 704, 'seq_len': 10, 'token_d_model': 64, 'time_d_model': 96, 'pos_d_model': 96, 'd_model': 288, 'conv_out_dim': 320, 'e_layers': 8, 'learning_rate': 0.011404648159175907, 'dropout': 0.2, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 01:02:01,999] The parameter 'norm_type' in trial#69 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=7, input_dim=84, dec_in=84, output_dim=1, d_model=448, n_heads=4, e_layers=8, hidden_d_model=192, seq_layers=2, last_d_model=736, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=48, time_d_model=80, pos_d_model=96, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.2, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=68, fc_layer_type='mha', conv_out_dim=256), exp_settings='seq_len-12-lr-0.0141-d-448-hid_d-192-last_d-736-tok_d-48-time_d-80-pos_d-96-e_layers-8-tok_conv_k-7-dropout-0.2-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-68-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.014103037849671936, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 01:05:41,589] Trial 69 finished with value: 1.0034934282302856 and parameters: {'num_heads': 68, 'hidden_d_model': 192, 'token_conv_kernel': 7, 'last_d_model': 736, 'seq_len': 12, 'token_d_model': 48, 'time_d_model': 80, 'pos_d_model': 96, 'd_model': 448, 'conv_out_dim': 256, 'e_layers': 8, 'learning_rate': 0.014103037849671936, 'dropout': 0.2, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 01:05:41,861] The parameter 'norm_type' in trial#72 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 23.9 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
23.9 M    Trainable params
0         Non-trainable params
23.9 M    Total params
95.534    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.033
Metric Loss/val improved by 0.004 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                          Best trial: 4. Best value: 0.969465:  36%|███▌      | 36/100 [1:30:07<3:15:40, 183.44s/it]Best trial: 4. Best value: 0.969465:  36%|███▌      | 36/100 [1:30:07<3:15:40, 183.44s/it]Best trial: 4. Best value: 0.969465:  37%|███▋      | 37/100 [1:30:07<3:05:35, 176.75s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                          Best trial: 4. Best value: 0.969465:  37%|███▋      | 37/100 [1:30:07<3:05:35, 176.75s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 14.4 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
14.4 M    Trainable params
0         Non-trainable params
14.4 M    Total params
57.501    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.046
Metric Loss/val improved by 0.012 >= min_delta = 0.0. New best score: 1.034
Metric Loss/val improved by 0.004 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.028
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.028. Signaling Trainer to stop.
                                                                                          Best trial: 4. Best value: 0.969465:  37%|███▋      | 37/100 [1:32:57<3:05:35, 176.75s/it]Best trial: 4. Best value: 0.969465:  37%|███▋      | 37/100 [1:32:57<3:05:35, 176.75s/it]Best trial: 4. Best value: 0.969465:  38%|███▊      | 38/100 [1:32:57<3:00:38, 174.82s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                          Best trial: 4. Best value: 0.969465:  38%|███▊      | 38/100 [1:32:57<3:00:38, 174.82s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=384, n_heads=4, e_layers=6, hidden_d_model=192, seq_layers=2, last_d_model=544, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=96, pos_d_model=96, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.22, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=320), exp_settings='seq_len-12-lr-0.0069-d-384-hid_d-192-last_d-544-tok_d-64-time_d-96-pos_d-96-e_layers-6-tok_conv_k-9-dropout-0.22-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.006908864399534137, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 01:08:22,750] Trial 72 finished with value: 0.9965164661407471 and parameters: {'num_heads': 36, 'hidden_d_model': 192, 'token_conv_kernel': 9, 'last_d_model': 544, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 96, 'pos_d_model': 96, 'd_model': 384, 'conv_out_dim': 320, 'e_layers': 6, 'learning_rate': 0.006908864399534137, 'dropout': 0.22, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 01:08:23,046] The parameter 'norm_type' in trial#74 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=8, input_dim=84, dec_in=84, output_dim=1, d_model=288, n_heads=4, e_layers=8, hidden_d_model=208, seq_layers=2, last_d_model=128, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=64, pos_d_model=128, combine_type='add', seq_len=14, pred_len=1, min_y_value=0.0, dropout=0.12000000000000001, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=192), exp_settings='seq_len-14-lr-0.0132-d-288-hid_d-208-last_d-128-tok_d-64-time_d-64-pos_d-128-e_layers-8-tok_conv_k-8-dropout-0.12000000000000001-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.013246859754995854, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 01:11:13,052] Trial 74 finished with value: 0.9951321840286256 and parameters: {'num_heads': 36, 'hidden_d_model': 208, 'token_conv_kernel': 8, 'last_d_model': 128, 'seq_len': 14, 'token_d_model': 64, 'time_d_model': 64, 'pos_d_model': 128, 'd_model': 288, 'conv_out_dim': 192, 'e_layers': 8, 'learning_rate': 0.013246859754995854, 'dropout': 0.12000000000000001, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 01:11:13,323] The parameter 'norm_type' in trial#76 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 14.4 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
14.4 M    Trainable params
0         Non-trainable params
14.4 M    Total params
57.469    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 2.910
Metric Loss/val improved by 1.218 >= min_delta = 0.0. New best score: 1.692
Metric Loss/val improved by 0.642 >= min_delta = 0.0. New best score: 1.050
Metric Loss/val improved by 0.012 >= min_delta = 0.0. New best score: 1.039
Metric Loss/val improved by 0.009 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                          Best trial: 4. Best value: 0.969465:  38%|███▊      | 38/100 [1:34:59<3:00:38, 174.82s/it]Best trial: 4. Best value: 0.969465:  38%|███▊      | 38/100 [1:34:59<3:00:38, 174.82s/it]Best trial: 4. Best value: 0.969465:  39%|███▉      | 39/100 [1:34:59<2:41:39, 159.00s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                          Best trial: 4. Best value: 0.969465:  39%|███▉      | 39/100 [1:34:59<2:41:39, 159.00s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 25.0 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
25.0 M    Trainable params
0         Non-trainable params
25.0 M    Total params
100.090   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.454
Metric Loss/val improved by 0.404 >= min_delta = 0.0. New best score: 1.050
Metric Loss/val improved by 0.020 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                          Best trial: 4. Best value: 0.969465:  39%|███▉      | 39/100 [1:38:11<2:41:39, 159.00s/it]Best trial: 4. Best value: 0.969465:  39%|███▉      | 39/100 [1:38:11<2:41:39, 159.00s/it]Best trial: 4. Best value: 0.969465:  40%|████      | 40/100 [1:38:11<2:48:58, 168.98s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                          Best trial: 4. Best value: 0.969465:  40%|████      | 40/100 [1:38:11<2:48:58, 168.98s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=8, input_dim=84, dec_in=84, output_dim=1, d_model=288, n_heads=4, e_layers=8, hidden_d_model=208, seq_layers=2, last_d_model=448, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=48, pos_d_model=80, combine_type='add', seq_len=10, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=192), exp_settings='seq_len-10-lr-0.0177-d-288-hid_d-208-last_d-448-tok_d-64-time_d-48-pos_d-80-e_layers-8-tok_conv_k-8-dropout-0.16-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.017745497797296728, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 01:13:15,161] Trial 76 finished with value: 0.9961125373840332 and parameters: {'num_heads': 36, 'hidden_d_model': 208, 'token_conv_kernel': 8, 'last_d_model': 448, 'seq_len': 10, 'token_d_model': 64, 'time_d_model': 48, 'pos_d_model': 80, 'd_model': 288, 'conv_out_dim': 192, 'e_layers': 8, 'learning_rate': 0.017745497797296728, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 01:13:15,468] The parameter 'norm_type' in trial#78 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=416, n_heads=4, e_layers=6, hidden_d_model=208, seq_layers=2, last_d_model=256, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=96, pos_d_model=112, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.18, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=320), exp_settings='seq_len-12-lr-0.0674-d-416-hid_d-208-last_d-256-tok_d-64-time_d-96-pos_d-112-e_layers-6-tok_conv_k-9-dropout-0.18-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.06741374992909975, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 01:16:27,403] Trial 78 finished with value: 0.9957632064819337 and parameters: {'num_heads': 36, 'hidden_d_model': 208, 'token_conv_kernel': 9, 'last_d_model': 256, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 96, 'pos_d_model': 112, 'd_model': 416, 'conv_out_dim': 320, 'e_layers': 6, 'learning_rate': 0.06741374992909975, 'dropout': 0.18, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 01:16:27,653] The parameter 'norm_type' in trial#80 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 25.6 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
25.6 M    Trainable params
0         Non-trainable params
25.6 M    Total params
102.450   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.860
Metric Loss/val improved by 0.125 >= min_delta = 0.0. New best score: 1.735
Metric Loss/val improved by 0.701 >= min_delta = 0.0. New best score: 1.034
Metric Loss/val improved by 0.005 >= min_delta = 0.0. New best score: 1.028
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.028
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.028. Signaling Trainer to stop.
                                                                                          Best trial: 4. Best value: 0.969465:  40%|████      | 40/100 [1:40:47<2:48:58, 168.98s/it]Best trial: 4. Best value: 0.969465:  40%|████      | 40/100 [1:40:47<2:48:58, 168.98s/it]Best trial: 4. Best value: 0.969465:  41%|████      | 41/100 [1:40:47<2:42:11, 164.94s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                          Best trial: 4. Best value: 0.969465:  41%|████      | 41/100 [1:40:47<2:42:11, 164.94s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 25.4 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
25.4 M    Trainable params
0         Non-trainable params
25.4 M    Total params
101.554   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 10.000
Metric Loss/val improved by 8.933 >= min_delta = 0.0. New best score: 1.067
Metric Loss/val improved by 0.038 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                          Best trial: 4. Best value: 0.969465:  41%|████      | 41/100 [1:42:36<2:42:11, 164.94s/it]Best trial: 4. Best value: 0.969465:  41%|████      | 41/100 [1:42:36<2:42:11, 164.94s/it]Best trial: 4. Best value: 0.969465:  42%|████▏     | 42/100 [1:42:36<2:23:15, 148.20s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                          Best trial: 4. Best value: 0.969465:  42%|████▏     | 42/100 [1:42:36<2:23:15, 148.20s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=512, n_heads=4, e_layers=6, hidden_d_model=208, seq_layers=2, last_d_model=1152, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=48, time_d_model=64, pos_d_model=128, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.2, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=320), exp_settings='seq_len-8-lr-0.0284-d-512-hid_d-208-last_d-1152-tok_d-48-time_d-64-pos_d-128-e_layers-6-tok_conv_k-9-dropout-0.2-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.028357206244563945, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 01:19:02,928] Trial 80 finished with value: 0.9958406805992126 and parameters: {'num_heads': 36, 'hidden_d_model': 208, 'token_conv_kernel': 9, 'last_d_model': 1152, 'seq_len': 8, 'token_d_model': 48, 'time_d_model': 64, 'pos_d_model': 128, 'd_model': 512, 'conv_out_dim': 320, 'e_layers': 6, 'learning_rate': 0.028357206244563945, 'dropout': 0.2, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 01:19:03,194] The parameter 'norm_type' in trial#82 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=11, input_dim=84, dec_in=84, output_dim=1, d_model=384, n_heads=4, e_layers=6, hidden_d_model=176, seq_layers=2, last_d_model=416, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=96, pos_d_model=96, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.22, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=288), exp_settings='seq_len-12-lr-0.0999-d-384-hid_d-176-last_d-416-tok_d-64-time_d-96-pos_d-96-e_layers-6-tok_conv_k-11-dropout-0.22-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.09986817523344828, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 01:20:52,072] Trial 82 finished with value: 1.012440013885498 and parameters: {'num_heads': 36, 'hidden_d_model': 176, 'token_conv_kernel': 11, 'last_d_model': 416, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 96, 'pos_d_model': 96, 'd_model': 384, 'conv_out_dim': 288, 'e_layers': 6, 'learning_rate': 0.09986817523344828, 'dropout': 0.22, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 01:20:52,321] The parameter 'norm_type' in trial#83 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 22.7 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
22.7 M    Trainable params
0         Non-trainable params
22.7 M    Total params
90.965    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 10.000
Metric Loss/val improved by 5.715 >= min_delta = 0.0. New best score: 4.285
Metric Loss/val improved by 2.458 >= min_delta = 0.0. New best score: 1.827
Metric Loss/val improved by 0.767 >= min_delta = 0.0. New best score: 1.060
Metric Loss/val improved by 0.030 >= min_delta = 0.0. New best score: 1.031
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                          Best trial: 4. Best value: 0.969465:  42%|████▏     | 42/100 [1:45:35<2:23:15, 148.20s/it]Best trial: 4. Best value: 0.969465:  42%|████▏     | 42/100 [1:45:35<2:23:15, 148.20s/it]Best trial: 4. Best value: 0.969465:  43%|████▎     | 43/100 [1:45:35<2:29:42, 157.58s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                          Best trial: 4. Best value: 0.969465:  43%|████▎     | 43/100 [1:45:36<2:29:42, 157.58s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 20.2 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
20.2 M    Trainable params
0         Non-trainable params
20.2 M    Total params
80.616    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 10.000
Metric Loss/val improved by 8.914 >= min_delta = 0.0. New best score: 1.086
Metric Loss/val improved by 0.057 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                          Best trial: 4. Best value: 0.969465:  43%|████▎     | 43/100 [1:48:42<2:29:42, 157.58s/it]Best trial: 4. Best value: 0.969465:  43%|████▎     | 43/100 [1:48:42<2:29:42, 157.58s/it]Best trial: 4. Best value: 0.969465:  44%|████▍     | 44/100 [1:48:42<2:35:15, 166.35s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                          Best trial: 4. Best value: 0.969465:  44%|████▍     | 44/100 [1:48:42<2:35:15, 166.35s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=448, n_heads=4, e_layers=8, hidden_d_model=192, seq_layers=2, last_d_model=736, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=48, time_d_model=64, pos_d_model=80, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.18, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=224), exp_settings='seq_len-12-lr-0.0201-d-448-hid_d-192-last_d-736-tok_d-48-time_d-64-pos_d-80-e_layers-8-tok_conv_k-10-dropout-0.18-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.02011479643457205, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 01:23:51,542] Trial 83 finished with value: 0.991835641860962 and parameters: {'num_heads': 36, 'hidden_d_model': 192, 'token_conv_kernel': 10, 'last_d_model': 736, 'seq_len': 12, 'token_d_model': 48, 'time_d_model': 64, 'pos_d_model': 80, 'd_model': 448, 'conv_out_dim': 224, 'e_layers': 8, 'learning_rate': 0.02011479643457205, 'dropout': 0.18, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 01:23:51,820] The parameter 'norm_type' in trial#85 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=8, input_dim=84, dec_in=84, output_dim=1, d_model=384, n_heads=4, e_layers=8, hidden_d_model=208, seq_layers=2, last_d_model=896, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=80, pos_d_model=96, combine_type='add', seq_len=10, pred_len=1, min_y_value=0.0, dropout=0.12000000000000001, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=224), exp_settings='seq_len-10-lr-0.0506-d-384-hid_d-208-last_d-896-tok_d-64-time_d-80-pos_d-96-e_layers-8-tok_conv_k-8-dropout-0.12000000000000001-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.050582295290580094, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 01:26:58,360] Trial 85 finished with value: 0.9962357044219972 and parameters: {'num_heads': 36, 'hidden_d_model': 208, 'token_conv_kernel': 8, 'last_d_model': 896, 'seq_len': 10, 'token_d_model': 64, 'time_d_model': 80, 'pos_d_model': 96, 'd_model': 384, 'conv_out_dim': 224, 'e_layers': 8, 'learning_rate': 0.050582295290580094, 'dropout': 0.12000000000000001, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 01:26:58,650] The parameter 'norm_type' in trial#87 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 17.4 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
17.4 M    Trainable params
0         Non-trainable params
17.4 M    Total params
69.756    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.037
Metric Loss/val improved by 0.003 >= min_delta = 0.0. New best score: 1.034
Metric Loss/val improved by 0.004 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.030
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.030. Signaling Trainer to stop.
                                                                                          Best trial: 4. Best value: 0.969465:  44%|████▍     | 44/100 [1:51:20<2:35:15, 166.35s/it]Best trial: 4. Best value: 0.969465:  44%|████▍     | 44/100 [1:51:20<2:35:15, 166.35s/it]Best trial: 4. Best value: 0.969465:  45%|████▌     | 45/100 [1:51:20<2:30:11, 163.85s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                          Best trial: 4. Best value: 0.969465:  45%|████▌     | 45/100 [1:51:20<2:30:11, 163.85s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 23.4 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
23.4 M    Trainable params
0         Non-trainable params
23.4 M    Total params
93.794    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 2.960
Metric Loss/val improved by 1.888 >= min_delta = 0.0. New best score: 1.072
Metric Loss/val improved by 0.043 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                          Best trial: 4. Best value: 0.969465:  45%|████▌     | 45/100 [1:53:51<2:30:11, 163.85s/it]Best trial: 4. Best value: 0.969465:  45%|████▌     | 45/100 [1:53:51<2:30:11, 163.85s/it]Best trial: 4. Best value: 0.969465:  46%|████▌     | 46/100 [1:53:51<2:23:50, 159.83s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                          Best trial: 4. Best value: 0.969465:  46%|████▌     | 46/100 [1:53:51<2:23:50, 159.83s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=352, n_heads=4, e_layers=6, hidden_d_model=160, seq_layers=2, last_d_model=960, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=48, time_d_model=80, pos_d_model=96, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=68, fc_layer_type='mha', conv_out_dim=256), exp_settings='seq_len-12-lr-0.0058-d-352-hid_d-160-last_d-960-tok_d-48-time_d-80-pos_d-96-e_layers-6-tok_conv_k-10-dropout-0.14-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-68-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.005769405236596223, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 01:29:36,359] Trial 87 finished with value: 1.0048317313194275 and parameters: {'num_heads': 68, 'hidden_d_model': 160, 'token_conv_kernel': 10, 'last_d_model': 960, 'seq_len': 12, 'token_d_model': 48, 'time_d_model': 80, 'pos_d_model': 96, 'd_model': 352, 'conv_out_dim': 256, 'e_layers': 6, 'learning_rate': 0.005769405236596223, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 01:29:36,624] The parameter 'norm_type' in trial#89 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=11, input_dim=84, dec_in=84, output_dim=1, d_model=384, n_heads=4, e_layers=8, hidden_d_model=176, seq_layers=2, last_d_model=704, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=48, time_d_model=96, pos_d_model=80, combine_type='add', seq_len=10, pred_len=1, min_y_value=0.0, dropout=0.12000000000000001, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=288), exp_settings='seq_len-10-lr-0.0211-d-384-hid_d-176-last_d-704-tok_d-48-time_d-96-pos_d-80-e_layers-8-tok_conv_k-11-dropout-0.12000000000000001-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.021133113081930915, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 01:32:06,825] Trial 89 finished with value: 0.9947619915008545 and parameters: {'num_heads': 36, 'hidden_d_model': 176, 'token_conv_kernel': 11, 'last_d_model': 704, 'seq_len': 10, 'token_d_model': 48, 'time_d_model': 96, 'pos_d_model': 80, 'd_model': 384, 'conv_out_dim': 288, 'e_layers': 8, 'learning_rate': 0.021133113081930915, 'dropout': 0.12000000000000001, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 01:32:07,112] The parameter 'norm_type' in trial#91 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 27.6 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
27.6 M    Trainable params
0         Non-trainable params
27.6 M    Total params
110.244   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.032
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                          Best trial: 4. Best value: 0.969465:  46%|████▌     | 46/100 [1:57:24<2:23:50, 159.83s/it]Best trial: 4. Best value: 0.969465:  46%|████▌     | 46/100 [1:57:24<2:23:50, 159.83s/it]Best trial: 4. Best value: 0.969465:  47%|████▋     | 47/100 [1:57:24<2:35:22, 175.90s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                          Best trial: 4. Best value: 0.969465:  47%|████▋     | 47/100 [1:57:24<2:35:22, 175.90s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 19.0 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
19.0 M    Trainable params
0         Non-trainable params
19.0 M    Total params
76.034    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.234
Metric Loss/val improved by 0.111 >= min_delta = 0.0. New best score: 1.123
Metric Loss/val improved by 0.062 >= min_delta = 0.0. New best score: 1.061
Metric Loss/val improved by 0.031 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                          Best trial: 4. Best value: 0.969465:  47%|████▋     | 47/100 [1:59:45<2:35:22, 175.90s/it]Best trial: 93. Best value: 0.969243:  47%|████▋     | 47/100 [1:59:45<2:35:22, 175.90s/it]Best trial: 93. Best value: 0.969243:  48%|████▊     | 48/100 [1:59:45<2:23:17, 165.34s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  48%|████▊     | 48/100 [1:59:45<2:23:17, 165.34s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=480, n_heads=4, e_layers=8, hidden_d_model=192, seq_layers=2, last_d_model=768, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=48, time_d_model=80, pos_d_model=112, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.2, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=4, fc_layer_type='mha', conv_out_dim=288), exp_settings='seq_len-12-lr-0.0051-d-480-hid_d-192-last_d-768-tok_d-48-time_d-80-pos_d-112-e_layers-8-tok_conv_k-10-dropout-0.2-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-4-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.005102598750494479, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 01:35:40,233] Trial 91 finished with value: 1.249164867401123 and parameters: {'num_heads': 4, 'hidden_d_model': 192, 'token_conv_kernel': 10, 'last_d_model': 768, 'seq_len': 12, 'token_d_model': 48, 'time_d_model': 80, 'pos_d_model': 112, 'd_model': 480, 'conv_out_dim': 288, 'e_layers': 8, 'learning_rate': 0.005102598750494479, 'dropout': 0.2, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 01:35:40,490] The parameter 'norm_type' in trial#93 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=7, input_dim=84, dec_in=84, output_dim=1, d_model=384, n_heads=4, e_layers=6, hidden_d_model=176, seq_layers=2, last_d_model=672, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=96, pos_d_model=64, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.18, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=288), exp_settings='seq_len-12-lr-0.0175-d-384-hid_d-176-last_d-672-tok_d-64-time_d-96-pos_d-64-e_layers-6-tok_conv_k-7-dropout-0.18-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.017511033836363155, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 01:38:00,917] Trial 93 finished with value: 0.969242835044861 and parameters: {'num_heads': 36, 'hidden_d_model': 176, 'token_conv_kernel': 7, 'last_d_model': 672, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 96, 'pos_d_model': 64, 'd_model': 384, 'conv_out_dim': 288, 'e_layers': 6, 'learning_rate': 0.017511033836363155, 'dropout': 0.18, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 01:38:01,179] The parameter 'norm_type' in trial#95 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 18.5 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
18.5 M    Trainable params
0         Non-trainable params
18.5 M    Total params
73.969    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.272
Metric Loss/val improved by 0.163 >= min_delta = 0.0. New best score: 1.109
Metric Loss/val improved by 0.020 >= min_delta = 0.0. New best score: 1.089
Metric Loss/val improved by 0.060 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                           Best trial: 93. Best value: 0.969243:  48%|████▊     | 48/100 [2:02:08<2:23:17, 165.34s/it]Best trial: 93. Best value: 0.969243:  48%|████▊     | 48/100 [2:02:08<2:23:17, 165.34s/it]Best trial: 93. Best value: 0.969243:  49%|████▉     | 49/100 [2:02:08<2:14:53, 158.69s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  49%|████▉     | 49/100 [2:02:08<2:14:53, 158.69s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 23.0 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
23.0 M    Trainable params
0         Non-trainable params
23.0 M    Total params
91.878    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.296
Metric Loss/val improved by 0.159 >= min_delta = 0.0. New best score: 1.136
Metric Loss/val improved by 0.041 >= min_delta = 0.0. New best score: 1.095
Metric Loss/val improved by 0.059 >= min_delta = 0.0. New best score: 1.037
Metric Loss/val improved by 0.003 >= min_delta = 0.0. New best score: 1.034
Metric Loss/val improved by 0.004 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                           Best trial: 93. Best value: 0.969243:  49%|████▉     | 49/100 [2:05:36<2:14:53, 158.69s/it]Best trial: 93. Best value: 0.969243:  49%|████▉     | 49/100 [2:05:36<2:14:53, 158.69s/it]Best trial: 93. Best value: 0.969243:  50%|█████     | 50/100 [2:05:36<2:24:32, 173.46s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  50%|█████     | 50/100 [2:05:36<2:24:32, 173.46s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=384, n_heads=4, e_layers=8, hidden_d_model=208, seq_layers=2, last_d_model=768, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=48, time_d_model=80, pos_d_model=80, combine_type='add', seq_len=10, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=224), exp_settings='seq_len-10-lr-0.0189-d-384-hid_d-208-last_d-768-tok_d-48-time_d-80-pos_d-80-e_layers-8-tok_conv_k-9-dropout-0.16-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.018917742918599573, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 01:40:24,098] Trial 95 finished with value: 0.9965196967124939 and parameters: {'num_heads': 36, 'hidden_d_model': 208, 'token_conv_kernel': 9, 'last_d_model': 768, 'seq_len': 10, 'token_d_model': 48, 'time_d_model': 80, 'pos_d_model': 80, 'd_model': 384, 'conv_out_dim': 224, 'e_layers': 8, 'learning_rate': 0.018917742918599573, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 01:40:24,347] The parameter 'norm_type' in trial#96 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=448, n_heads=4, e_layers=8, hidden_d_model=192, seq_layers=2, last_d_model=352, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=48, time_d_model=80, pos_d_model=96, combine_type='add', seq_len=10, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=256), exp_settings='seq_len-10-lr-0.0157-d-448-hid_d-192-last_d-352-tok_d-48-time_d-80-pos_d-96-e_layers-8-tok_conv_k-9-dropout-0.14-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.01570363292659199, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 01:43:52,011] Trial 96 finished with value: 0.9960042715072632 and parameters: {'num_heads': 36, 'hidden_d_model': 192, 'token_conv_kernel': 9, 'last_d_model': 352, 'seq_len': 10, 'token_d_model': 48, 'time_d_model': 80, 'pos_d_model': 96, 'd_model': 448, 'conv_out_dim': 256, 'e_layers': 8, 'learning_rate': 0.01570363292659199, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 01:43:52,310] The parameter 'norm_type' in trial#99 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 23.1 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
23.1 M    Trainable params
0         Non-trainable params
23.1 M    Total params
92.342    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.115
Metric Loss/val improved by 0.084 >= min_delta = 0.0. New best score: 1.032
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                           Best trial: 93. Best value: 0.969243:  50%|█████     | 50/100 [2:09:01<2:24:32, 173.46s/it]Best trial: 93. Best value: 0.969243:  50%|█████     | 50/100 [2:09:01<2:24:32, 173.46s/it]Best trial: 93. Best value: 0.969243:  51%|█████     | 51/100 [2:09:01<2:29:25, 182.98s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  51%|█████     | 51/100 [2:09:01<2:29:25, 182.98s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 14.0 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
14.0 M    Trainable params
0         Non-trainable params
14.0 M    Total params
55.938    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.050
Metric Loss/val improved by 0.006 >= min_delta = 0.0. New best score: 1.044
Metric Loss/val improved by 0.014 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                           Best trial: 93. Best value: 0.969243:  51%|█████     | 51/100 [2:12:08<2:29:25, 182.98s/it]Best trial: 93. Best value: 0.969243:  51%|█████     | 51/100 [2:12:08<2:29:25, 182.98s/it]Best trial: 93. Best value: 0.969243:  52%|█████▏    | 52/100 [2:12:08<2:27:26, 184.30s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  52%|█████▏    | 52/100 [2:12:09<2:27:26, 184.30s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=384, n_heads=4, e_layers=8, hidden_d_model=192, seq_layers=2, last_d_model=992, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=96, pos_d_model=80, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=256), exp_settings='seq_len-12-lr-0.0074-d-384-hid_d-192-last_d-992-tok_d-64-time_d-96-pos_d-80-e_layers-8-tok_conv_k-9-dropout-0.14-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.00739789689025421, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 01:47:17,194] Trial 99 finished with value: 0.9955877065658569 and parameters: {'num_heads': 36, 'hidden_d_model': 192, 'token_conv_kernel': 9, 'last_d_model': 992, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 96, 'pos_d_model': 80, 'd_model': 384, 'conv_out_dim': 256, 'e_layers': 8, 'learning_rate': 0.00739789689025421, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 01:47:17,474] The parameter 'norm_type' in trial#101 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=8, input_dim=84, dec_in=84, output_dim=1, d_model=384, n_heads=4, e_layers=4, hidden_d_model=176, seq_layers=2, last_d_model=896, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=48, time_d_model=96, pos_d_model=64, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.2, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=256), exp_settings='seq_len-12-lr-0.0134-d-384-hid_d-176-last_d-896-tok_d-48-time_d-96-pos_d-64-e_layers-4-tok_conv_k-8-dropout-0.2-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.013430249032693681, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 01:50:24,577] Trial 101 finished with value: 0.996368670463562 and parameters: {'num_heads': 36, 'hidden_d_model': 176, 'token_conv_kernel': 8, 'last_d_model': 896, 'seq_len': 12, 'token_d_model': 48, 'time_d_model': 96, 'pos_d_model': 64, 'd_model': 384, 'conv_out_dim': 256, 'e_layers': 4, 'learning_rate': 0.013430249032693681, 'dropout': 0.2, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 01:50:24,847] The parameter 'norm_type' in trial#104 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 33.5 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
33.5 M    Trainable params
0         Non-trainable params
33.5 M    Total params
133.844   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.033
Metric Loss/val improved by 0.005 >= min_delta = 0.0. New best score: 1.028
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.028. Signaling Trainer to stop.
                                                                                           Best trial: 93. Best value: 0.969243:  52%|█████▏    | 52/100 [2:15:27<2:27:26, 184.30s/it]Best trial: 93. Best value: 0.969243:  52%|█████▏    | 52/100 [2:15:27<2:27:26, 184.30s/it]Best trial: 93. Best value: 0.969243:  53%|█████▎    | 53/100 [2:15:27<2:27:38, 188.48s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  53%|█████▎    | 53/100 [2:15:27<2:27:38, 188.48s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 25.4 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
25.4 M    Trainable params
0         Non-trainable params
25.4 M    Total params
101.690   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.044
Metric Loss/val improved by 0.017 >= min_delta = 0.0. New best score: 1.027
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.027. Signaling Trainer to stop.
                                                                                           Best trial: 93. Best value: 0.969243:  53%|█████▎    | 53/100 [2:17:56<2:27:38, 188.48s/it]Best trial: 93. Best value: 0.969243:  53%|█████▎    | 53/100 [2:17:56<2:27:38, 188.48s/it]Best trial: 93. Best value: 0.969243:  54%|█████▍    | 54/100 [2:17:56<2:15:31, 176.77s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  54%|█████▍    | 54/100 [2:17:56<2:15:31, 176.77s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=480, n_heads=4, e_layers=8, hidden_d_model=160, seq_layers=2, last_d_model=800, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=112, pos_d_model=96, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=320), exp_settings='seq_len-12-lr-0.0053-d-480-hid_d-160-last_d-800-tok_d-64-time_d-112-pos_d-96-e_layers-8-tok_conv_k-10-dropout-0.16-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.005268564981702652, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 01:53:42,831] Trial 104 finished with value: 0.9921608924865724 and parameters: {'num_heads': 36, 'hidden_d_model': 160, 'token_conv_kernel': 10, 'last_d_model': 800, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 112, 'pos_d_model': 96, 'd_model': 480, 'conv_out_dim': 320, 'e_layers': 8, 'learning_rate': 0.005268564981702652, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 01:53:43,112] The parameter 'norm_type' in trial#105 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=384, n_heads=4, e_layers=6, hidden_d_model=176, seq_layers=2, last_d_model=512, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=96, pos_d_model=64, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=352), exp_settings='seq_len-8-lr-0.0058-d-384-hid_d-176-last_d-512-tok_d-64-time_d-96-pos_d-64-e_layers-6-tok_conv_k-9-dropout-0.14-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.005756704020662506, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 01:56:12,263] Trial 105 finished with value: 0.995950973033905 and parameters: {'num_heads': 36, 'hidden_d_model': 176, 'token_conv_kernel': 9, 'last_d_model': 512, 'seq_len': 8, 'token_d_model': 64, 'time_d_model': 96, 'pos_d_model': 64, 'd_model': 384, 'conv_out_dim': 352, 'e_layers': 6, 'learning_rate': 0.005756704020662506, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 01:56:12,552] The parameter 'norm_type' in trial#107 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 21.5 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
21.5 M    Trainable params
0         Non-trainable params
21.5 M    Total params
86.180    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 10.000
Metric Loss/val improved by 2.946 >= min_delta = 0.0. New best score: 7.054
Metric Loss/val improved by 4.857 >= min_delta = 0.0. New best score: 2.197
Metric Loss/val improved by 1.166 >= min_delta = 0.0. New best score: 1.031
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                           Best trial: 93. Best value: 0.969243:  54%|█████▍    | 54/100 [2:20:51<2:15:31, 176.77s/it]Best trial: 93. Best value: 0.969243:  54%|█████▍    | 54/100 [2:20:51<2:15:31, 176.77s/it]Best trial: 93. Best value: 0.969243:  55%|█████▌    | 55/100 [2:20:51<2:12:07, 176.17s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  55%|█████▌    | 55/100 [2:20:51<2:12:07, 176.17s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 19.8 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
19.8 M    Trainable params
0         Non-trainable params
19.8 M    Total params
79.245    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 8.670
Metric Loss/val improved by 7.574 >= min_delta = 0.0. New best score: 1.096
Metric Loss/val improved by 0.067 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                           Best trial: 93. Best value: 0.969243:  55%|█████▌    | 55/100 [2:22:47<2:12:07, 176.17s/it]Best trial: 93. Best value: 0.969243:  55%|█████▌    | 55/100 [2:22:47<2:12:07, 176.17s/it]Best trial: 93. Best value: 0.969243:  56%|█████▌    | 56/100 [2:22:47<1:56:01, 158.21s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  56%|█████▌    | 56/100 [2:22:47<1:56:01, 158.21s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=8, input_dim=84, dec_in=84, output_dim=1, d_model=384, n_heads=4, e_layers=8, hidden_d_model=144, seq_layers=2, last_d_model=768, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=96, pos_d_model=64, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.2, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=256), exp_settings='seq_len-12-lr-0.0194-d-384-hid_d-144-last_d-768-tok_d-64-time_d-96-pos_d-64-e_layers-8-tok_conv_k-8-dropout-0.2-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0194309377951269, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 01:59:07,045] Trial 107 finished with value: 1.0013074398040773 and parameters: {'num_heads': 36, 'hidden_d_model': 144, 'token_conv_kernel': 8, 'last_d_model': 768, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 96, 'pos_d_model': 64, 'd_model': 384, 'conv_out_dim': 256, 'e_layers': 8, 'learning_rate': 0.0194309377951269, 'dropout': 0.2, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 01:59:07,285] The parameter 'norm_type' in trial#109 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=416, n_heads=4, e_layers=6, hidden_d_model=160, seq_layers=2, last_d_model=768, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=48, time_d_model=96, pos_d_model=48, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.2, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=68, fc_layer_type='mha', conv_out_dim=288), exp_settings='seq_len-12-lr-0.0586-d-416-hid_d-160-last_d-768-tok_d-48-time_d-96-pos_d-48-e_layers-6-tok_conv_k-9-dropout-0.2-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-68-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.05863074014280287, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 02:01:03,349] Trial 109 finished with value: 0.9966886043548584 and parameters: {'num_heads': 68, 'hidden_d_model': 160, 'token_conv_kernel': 9, 'last_d_model': 768, 'seq_len': 12, 'token_d_model': 48, 'time_d_model': 96, 'pos_d_model': 48, 'd_model': 416, 'conv_out_dim': 288, 'e_layers': 6, 'learning_rate': 0.05863074014280287, 'dropout': 0.2, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 02:01:03,699] The parameter 'norm_type' in trial#111 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 24.1 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
24.1 M    Trainable params
0         Non-trainable params
24.1 M    Total params
96.237    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.150
Metric Loss/val improved by 0.076 >= min_delta = 0.0. New best score: 1.073
Metric Loss/val improved by 0.017 >= min_delta = 0.0. New best score: 1.057
Metric Loss/val improved by 0.028 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.028
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.028. Signaling Trainer to stop.
                                                                                           Best trial: 93. Best value: 0.969243:  56%|█████▌    | 56/100 [2:24:57<1:56:01, 158.21s/it]Best trial: 93. Best value: 0.969243:  56%|█████▌    | 56/100 [2:24:57<1:56:01, 158.21s/it]Best trial: 93. Best value: 0.969243:  57%|█████▋    | 57/100 [2:24:57<1:47:20, 149.78s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  57%|█████▋    | 57/100 [2:24:58<1:47:20, 149.78s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 32.3 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
32.3 M    Trainable params
0         Non-trainable params
32.3 M    Total params
129.114   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                           Best trial: 93. Best value: 0.969243:  57%|█████▋    | 57/100 [2:29:46<1:47:20, 149.78s/it]Best trial: 93. Best value: 0.969243:  57%|█████▋    | 57/100 [2:29:46<1:47:20, 149.78s/it]Best trial: 93. Best value: 0.969243:  58%|█████▊    | 58/100 [2:29:46<2:13:59, 191.41s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  58%|█████▊    | 58/100 [2:29:46<2:13:59, 191.41s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=288, n_heads=4, e_layers=6, hidden_d_model=192, seq_layers=2, last_d_model=864, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=96, pos_d_model=64, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=68, fc_layer_type='mha', conv_out_dim=384), exp_settings='seq_len-12-lr-0.0168-d-288-hid_d-192-last_d-864-tok_d-64-time_d-96-pos_d-64-e_layers-6-tok_conv_k-9-dropout-0.16-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-68-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.016803266019592592, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 02:03:13,454] Trial 111 finished with value: 1.2764277696609496 and parameters: {'num_heads': 68, 'hidden_d_model': 192, 'token_conv_kernel': 9, 'last_d_model': 864, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 96, 'pos_d_model': 64, 'd_model': 288, 'conv_out_dim': 384, 'e_layers': 6, 'learning_rate': 0.016803266019592592, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 02:03:13,732] The parameter 'norm_type' in trial#113 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=512, n_heads=4, e_layers=8, hidden_d_model=208, seq_layers=2, last_d_model=1408, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=112, pos_d_model=80, combine_type='add', seq_len=14, pred_len=1, min_y_value=0.0, dropout=0.18, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=288), exp_settings='seq_len-14-lr-0.0041-d-512-hid_d-208-last_d-1408-tok_d-64-time_d-112-pos_d-80-e_layers-8-tok_conv_k-9-dropout-0.18-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0040643263279333975, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 02:08:01,986] Trial 113 finished with value: 1.0045062899589539 and parameters: {'num_heads': 36, 'hidden_d_model': 208, 'token_conv_kernel': 9, 'last_d_model': 1408, 'seq_len': 14, 'token_d_model': 64, 'time_d_model': 112, 'pos_d_model': 80, 'd_model': 512, 'conv_out_dim': 288, 'e_layers': 8, 'learning_rate': 0.0040643263279333975, 'dropout': 0.18, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 02:08:02,248] The parameter 'norm_type' in trial#116 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 19.0 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
19.0 M    Trainable params
0         Non-trainable params
19.0 M    Total params
75.832    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.175
Metric Loss/val improved by 0.132 >= min_delta = 0.0. New best score: 1.043
Metric Loss/val improved by 0.004 >= min_delta = 0.0. New best score: 1.039
Metric Loss/val improved by 0.006 >= min_delta = 0.0. New best score: 1.034
Metric Loss/val improved by 0.004 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                           Best trial: 93. Best value: 0.969243:  58%|█████▊    | 58/100 [2:32:45<2:13:59, 191.41s/it]Best trial: 93. Best value: 0.969243:  58%|█████▊    | 58/100 [2:32:45<2:13:59, 191.41s/it]Best trial: 93. Best value: 0.969243:  59%|█████▉    | 59/100 [2:32:45<2:08:12, 187.63s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  59%|█████▉    | 59/100 [2:32:45<2:08:12, 187.63s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 23.1 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
23.1 M    Trainable params
0         Non-trainable params
23.1 M    Total params
92.443    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.065
Metric Loss/val improved by 0.034 >= min_delta = 0.0. New best score: 1.031
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                           Best trial: 93. Best value: 0.969243:  59%|█████▉    | 59/100 [2:36:05<2:08:12, 187.63s/it]Best trial: 93. Best value: 0.969243:  59%|█████▉    | 59/100 [2:36:05<2:08:12, 187.63s/it]Best trial: 93. Best value: 0.969243:  60%|██████    | 60/100 [2:36:05<2:07:35, 191.38s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  60%|██████    | 60/100 [2:36:05<2:07:35, 191.38s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=384, n_heads=4, e_layers=6, hidden_d_model=192, seq_layers=2, last_d_model=448, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=128, pos_d_model=96, combine_type='add', seq_len=10, pred_len=1, min_y_value=0.0, dropout=0.2, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=224), exp_settings='seq_len-10-lr-0.0101-d-384-hid_d-192-last_d-448-tok_d-64-time_d-128-pos_d-96-e_layers-6-tok_conv_k-9-dropout-0.2-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.010106516462127778, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 02:11:00,800] Trial 116 finished with value: 0.9962531447410584 and parameters: {'num_heads': 36, 'hidden_d_model': 192, 'token_conv_kernel': 9, 'last_d_model': 448, 'seq_len': 10, 'token_d_model': 64, 'time_d_model': 128, 'pos_d_model': 96, 'd_model': 384, 'conv_out_dim': 224, 'e_layers': 6, 'learning_rate': 0.010106516462127778, 'dropout': 0.2, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 02:11:01,063] The parameter 'norm_type' in trial#118 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=480, n_heads=4, e_layers=6, hidden_d_model=192, seq_layers=2, last_d_model=1312, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=96, pos_d_model=64, combine_type='add', seq_len=10, pred_len=1, min_y_value=0.0, dropout=0.18, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=224), exp_settings='seq_len-10-lr-0.0073-d-480-hid_d-192-last_d-1312-tok_d-64-time_d-96-pos_d-64-e_layers-6-tok_conv_k-9-dropout-0.18-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.007295967057692362, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 02:14:20,914] Trial 118 finished with value: 1.0062215685844422 and parameters: {'num_heads': 36, 'hidden_d_model': 192, 'token_conv_kernel': 9, 'last_d_model': 1312, 'seq_len': 10, 'token_d_model': 64, 'time_d_model': 96, 'pos_d_model': 64, 'd_model': 480, 'conv_out_dim': 224, 'e_layers': 6, 'learning_rate': 0.007295967057692362, 'dropout': 0.18, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 02:14:21,189] The parameter 'norm_type' in trial#120 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 29.7 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
29.7 M    Trainable params
0         Non-trainable params
29.7 M    Total params
118.685   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.053
Metric Loss/val improved by 0.017 >= min_delta = 0.0. New best score: 1.036
Metric Loss/val improved by 0.004 >= min_delta = 0.0. New best score: 1.032
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.031
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                           Best trial: 93. Best value: 0.969243:  60%|██████    | 60/100 [2:40:38<2:07:35, 191.38s/it]Best trial: 93. Best value: 0.969243:  60%|██████    | 60/100 [2:40:38<2:07:35, 191.38s/it]Best trial: 93. Best value: 0.969243:  61%|██████    | 61/100 [2:40:38<2:20:25, 216.03s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  61%|██████    | 61/100 [2:40:39<2:20:25, 216.03s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 13.3 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
13.3 M    Trainable params
0         Non-trainable params
13.3 M    Total params
53.190    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.033
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.033. Signaling Trainer to stop.
                                                                                           Best trial: 93. Best value: 0.969243:  61%|██████    | 61/100 [2:42:15<2:20:25, 216.03s/it]Best trial: 93. Best value: 0.969243:  61%|██████    | 61/100 [2:42:15<2:20:25, 216.03s/it]Best trial: 93. Best value: 0.969243:  62%|██████▏   | 62/100 [2:42:15<1:54:04, 180.12s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  62%|██████▏   | 62/100 [2:42:15<1:54:04, 180.12s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=512, n_heads=4, e_layers=6, hidden_d_model=160, seq_layers=2, last_d_model=1280, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=112, pos_d_model=80, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.2, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=320), exp_settings='seq_len-12-lr-0.0089-d-512-hid_d-160-last_d-1280-tok_d-64-time_d-112-pos_d-80-e_layers-6-tok_conv_k-9-dropout-0.2-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.008938531387913335, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 02:18:54,468] Trial 120 finished with value: 0.9953753471374512 and parameters: {'num_heads': 36, 'hidden_d_model': 160, 'token_conv_kernel': 9, 'last_d_model': 1280, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 112, 'pos_d_model': 80, 'd_model': 512, 'conv_out_dim': 320, 'e_layers': 6, 'learning_rate': 0.008938531387913335, 'dropout': 0.2, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 02:18:54,830] The parameter 'norm_type' in trial#122 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=8, input_dim=84, dec_in=84, output_dim=1, d_model=352, n_heads=4, e_layers=4, hidden_d_model=224, seq_layers=2, last_d_model=864, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=48, time_d_model=128, pos_d_model=64, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=68, fc_layer_type='mha', conv_out_dim=256), exp_settings='seq_len-12-lr-0.0042-d-352-hid_d-224-last_d-864-tok_d-48-time_d-128-pos_d-64-e_layers-4-tok_conv_k-8-dropout-0.14-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-68-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.004248264584189309, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 02:20:30,824] Trial 122 finished with value: 1.0646584987640382 and parameters: {'num_heads': 68, 'hidden_d_model': 224, 'token_conv_kernel': 8, 'last_d_model': 864, 'seq_len': 12, 'token_d_model': 48, 'time_d_model': 128, 'pos_d_model': 64, 'd_model': 352, 'conv_out_dim': 256, 'e_layers': 4, 'learning_rate': 0.004248264584189309, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 02:20:31,079] The parameter 'norm_type' in trial#124 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 28.9 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
28.9 M    Trainable params
0         Non-trainable params
28.9 M    Total params
115.764   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 6.490
Metric Loss/val improved by 4.026 >= min_delta = 0.0. New best score: 2.463
Metric Loss/val improved by 1.392 >= min_delta = 0.0. New best score: 1.071
Metric Loss/val improved by 0.042 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.028
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.028
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.028
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                           Best trial: 93. Best value: 0.969243:  62%|██████▏   | 62/100 [2:45:11<1:54:04, 180.12s/it]Best trial: 93. Best value: 0.969243:  62%|██████▏   | 62/100 [2:45:11<1:54:04, 180.12s/it]Best trial: 93. Best value: 0.969243:  63%|██████▎   | 63/100 [2:45:11<1:50:23, 179.03s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  63%|██████▎   | 63/100 [2:45:11<1:50:23, 179.03s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 28.4 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
28.4 M    Trainable params
0         Non-trainable params
28.4 M    Total params
113.775   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.046
Metric Loss/val improved by 0.013 >= min_delta = 0.0. New best score: 1.034
Metric Loss/val improved by 0.004 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                           Best trial: 93. Best value: 0.969243:  63%|██████▎   | 63/100 [2:48:09<1:50:23, 179.03s/it]Best trial: 93. Best value: 0.969243:  63%|██████▎   | 63/100 [2:48:09<1:50:23, 179.03s/it]Best trial: 93. Best value: 0.969243:  64%|██████▍   | 64/100 [2:48:09<1:47:12, 178.67s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  64%|██████▍   | 64/100 [2:48:09<1:47:12, 178.67s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=416, n_heads=4, e_layers=6, hidden_d_model=176, seq_layers=2, last_d_model=1216, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=112, pos_d_model=64, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=352), exp_settings='seq_len-8-lr-0.0288-d-416-hid_d-176-last_d-1216-tok_d-64-time_d-112-pos_d-64-e_layers-6-tok_conv_k-10-dropout-0.16-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.02876156225130335, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 02:23:27,289] Trial 124 finished with value: 0.994678020477295 and parameters: {'num_heads': 36, 'hidden_d_model': 176, 'token_conv_kernel': 10, 'last_d_model': 1216, 'seq_len': 8, 'token_d_model': 64, 'time_d_model': 112, 'pos_d_model': 64, 'd_model': 416, 'conv_out_dim': 352, 'e_layers': 6, 'learning_rate': 0.02876156225130335, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 02:23:27,570] The parameter 'norm_type' in trial#126 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=448, n_heads=4, e_layers=6, hidden_d_model=208, seq_layers=2, last_d_model=1152, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=128, pos_d_model=96, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=352), exp_settings='seq_len-12-lr-0.0085-d-448-hid_d-208-last_d-1152-tok_d-64-time_d-128-pos_d-96-e_layers-6-tok_conv_k-9-dropout-0.14-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.008519792905539508, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 02:26:25,135] Trial 126 finished with value: 0.9955465555191041 and parameters: {'num_heads': 36, 'hidden_d_model': 208, 'token_conv_kernel': 9, 'last_d_model': 1152, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 128, 'pos_d_model': 96, 'd_model': 448, 'conv_out_dim': 352, 'e_layers': 6, 'learning_rate': 0.008519792905539508, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 02:26:25,479] The parameter 'norm_type' in trial#128 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 25.1 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
25.1 M    Trainable params
0         Non-trainable params
25.1 M    Total params
100.418   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.038
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.038. Signaling Trainer to stop.
                                                                                           Best trial: 93. Best value: 0.969243:  64%|██████▍   | 64/100 [2:49:56<1:47:12, 178.67s/it]Best trial: 93. Best value: 0.969243:  64%|██████▍   | 64/100 [2:49:56<1:47:12, 178.67s/it]Best trial: 93. Best value: 0.969243:  65%|██████▌   | 65/100 [2:49:56<1:31:46, 157.32s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  65%|██████▌   | 65/100 [2:49:57<1:31:46, 157.32s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 32.9 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
32.9 M    Trainable params
0         Non-trainable params
32.9 M    Total params
131.710   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.309
Metric Loss/val improved by 0.144 >= min_delta = 0.0. New best score: 1.165
Metric Loss/val improved by 0.106 >= min_delta = 0.0. New best score: 1.059
Metric Loss/val improved by 0.030 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                           Best trial: 93. Best value: 0.969243:  65%|██████▌   | 65/100 [2:52:28<1:31:46, 157.32s/it]Best trial: 93. Best value: 0.969243:  65%|██████▌   | 65/100 [2:52:28<1:31:46, 157.32s/it]Best trial: 93. Best value: 0.969243:  66%|██████▌   | 66/100 [2:52:28<1:28:09, 155.57s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  66%|██████▌   | 66/100 [2:52:28<1:28:09, 155.57s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=320, n_heads=4, e_layers=8, hidden_d_model=224, seq_layers=2, last_d_model=672, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=112, pos_d_model=80, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.2, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=68, fc_layer_type='mha', conv_out_dim=320), exp_settings='seq_len-12-lr-0.0041-d-320-hid_d-224-last_d-672-tok_d-64-time_d-112-pos_d-80-e_layers-8-tok_conv_k-10-dropout-0.2-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-68-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.004074018699241482, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 02:28:12,615] Trial 128 finished with value: 1.0918052554130555 and parameters: {'num_heads': 68, 'hidden_d_model': 224, 'token_conv_kernel': 10, 'last_d_model': 672, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 112, 'pos_d_model': 80, 'd_model': 320, 'conv_out_dim': 320, 'e_layers': 8, 'learning_rate': 0.004074018699241482, 'dropout': 0.2, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 02:28:12,885] The parameter 'norm_type' in trial#129 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=11, input_dim=84, dec_in=84, output_dim=1, d_model=480, n_heads=4, e_layers=8, hidden_d_model=224, seq_layers=2, last_d_model=512, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=96, pos_d_model=64, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=288), exp_settings='seq_len-12-lr-0.0162-d-480-hid_d-224-last_d-512-tok_d-64-time_d-96-pos_d-64-e_layers-8-tok_conv_k-11-dropout-0.14-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.016218578679724416, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 02:30:44,106] Trial 129 finished with value: 1.0038868427276613 and parameters: {'num_heads': 36, 'hidden_d_model': 224, 'token_conv_kernel': 11, 'last_d_model': 512, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 96, 'pos_d_model': 64, 'd_model': 480, 'conv_out_dim': 288, 'e_layers': 8, 'learning_rate': 0.016218578679724416, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 02:30:44,376] The parameter 'norm_type' in trial#131 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 36.8 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
36.8 M    Trainable params
0         Non-trainable params
36.8 M    Total params
147.177   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.135
Metric Loss/val improved by 0.075 >= min_delta = 0.0. New best score: 1.060
Metric Loss/val improved by 0.030 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                           Best trial: 93. Best value: 0.969243:  66%|██████▌   | 66/100 [2:55:19<1:28:09, 155.57s/it]Best trial: 93. Best value: 0.969243:  66%|██████▌   | 66/100 [2:55:19<1:28:09, 155.57s/it]Best trial: 93. Best value: 0.969243:  67%|██████▋   | 67/100 [2:55:19<1:28:04, 160.13s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  67%|██████▋   | 67/100 [2:55:19<1:28:04, 160.13s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 18.1 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
18.1 M    Trainable params
0         Non-trainable params
18.1 M    Total params
72.325    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.046
Metric Loss/val improved by 0.013 >= min_delta = 0.0. New best score: 1.033
Metric Loss/val improved by 0.004 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                           Best trial: 93. Best value: 0.969243:  67%|██████▋   | 67/100 [2:58:10<1:28:04, 160.13s/it]Best trial: 93. Best value: 0.969243:  67%|██████▋   | 67/100 [2:58:10<1:28:04, 160.13s/it]Best trial: 93. Best value: 0.969243:  68%|██████▊   | 68/100 [2:58:10<1:27:07, 163.36s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  68%|██████▊   | 68/100 [2:58:10<1:27:07, 163.36s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=512, n_heads=4, e_layers=6, hidden_d_model=224, seq_layers=2, last_d_model=1024, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=96, pos_d_model=80, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=68, fc_layer_type='mha', conv_out_dim=416), exp_settings='seq_len-12-lr-0.0142-d-512-hid_d-224-last_d-1024-tok_d-64-time_d-96-pos_d-80-e_layers-6-tok_conv_k-10-dropout-0.16-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-68-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.014163249908473903, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 02:33:34,875] Trial 131 finished with value: 3.374155998229981 and parameters: {'num_heads': 68, 'hidden_d_model': 224, 'token_conv_kernel': 10, 'last_d_model': 1024, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 96, 'pos_d_model': 80, 'd_model': 512, 'conv_out_dim': 416, 'e_layers': 6, 'learning_rate': 0.014163249908473903, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 02:33:35,141] The parameter 'norm_type' in trial#133 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=384, n_heads=4, e_layers=8, hidden_d_model=208, seq_layers=2, last_d_model=704, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=128, pos_d_model=80, combine_type='add', seq_len=10, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=68, fc_layer_type='mha', conv_out_dim=160), exp_settings='seq_len-10-lr-0.0079-d-384-hid_d-208-last_d-704-tok_d-64-time_d-128-pos_d-80-e_layers-8-tok_conv_k-9-dropout-0.16-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-68-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.007946066220327907, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 02:36:25,787] Trial 133 finished with value: 0.9955354452133179 and parameters: {'num_heads': 68, 'hidden_d_model': 208, 'token_conv_kernel': 9, 'last_d_model': 704, 'seq_len': 10, 'token_d_model': 64, 'time_d_model': 128, 'pos_d_model': 80, 'd_model': 384, 'conv_out_dim': 160, 'e_layers': 8, 'learning_rate': 0.007946066220327907, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 02:36:26,110] The parameter 'norm_type' in trial#135 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 21.3 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
21.3 M    Trainable params
0         Non-trainable params
21.3 M    Total params
85.324    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.046
Metric Loss/val improved by 0.012 >= min_delta = 0.0. New best score: 1.035
Metric Loss/val improved by 0.005 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                           Best trial: 93. Best value: 0.969243:  68%|██████▊   | 68/100 [3:01:02<1:27:07, 163.36s/it]Best trial: 93. Best value: 0.969243:  68%|██████▊   | 68/100 [3:01:02<1:27:07, 163.36s/it]Best trial: 93. Best value: 0.969243:  69%|██████▉   | 69/100 [3:01:02<1:25:44, 165.97s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  69%|██████▉   | 69/100 [3:01:02<1:25:44, 165.97s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 25.8 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
25.8 M    Trainable params
0         Non-trainable params
25.8 M    Total params
103.049   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.061
Metric Loss/val improved by 0.026 >= min_delta = 0.0. New best score: 1.035
Metric Loss/val improved by 0.006 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                           Best trial: 93. Best value: 0.969243:  69%|██████▉   | 69/100 [3:03:55<1:25:44, 165.97s/it]Best trial: 93. Best value: 0.969243:  69%|██████▉   | 69/100 [3:03:55<1:25:44, 165.97s/it]Best trial: 93. Best value: 0.969243:  70%|███████   | 70/100 [3:03:55<1:24:02, 168.08s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  70%|███████   | 70/100 [3:03:55<1:24:02, 168.08s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=480, n_heads=4, e_layers=6, hidden_d_model=208, seq_layers=2, last_d_model=992, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=112, pos_d_model=64, combine_type='add', seq_len=10, pred_len=1, min_y_value=0.0, dropout=0.18, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=192), exp_settings='seq_len-10-lr-0.01-d-480-hid_d-208-last_d-992-tok_d-64-time_d-112-pos_d-64-e_layers-6-tok_conv_k-9-dropout-0.18-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.009967797488093726, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 02:39:17,801] Trial 135 finished with value: 0.9961827278137207 and parameters: {'num_heads': 36, 'hidden_d_model': 208, 'token_conv_kernel': 9, 'last_d_model': 992, 'seq_len': 10, 'token_d_model': 64, 'time_d_model': 112, 'pos_d_model': 64, 'd_model': 480, 'conv_out_dim': 192, 'e_layers': 6, 'learning_rate': 0.009967797488093726, 'dropout': 0.18, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 02:39:18,087] The parameter 'norm_type' in trial#137 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=416, n_heads=4, e_layers=4, hidden_d_model=192, seq_layers=2, last_d_model=864, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=96, pos_d_model=96, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=384), exp_settings='seq_len-12-lr-0.0099-d-416-hid_d-192-last_d-864-tok_d-64-time_d-96-pos_d-96-e_layers-4-tok_conv_k-9-dropout-0.16-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.009860121199804296, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 02:42:10,838] Trial 137 finished with value: 0.9951876163482667 and parameters: {'num_heads': 36, 'hidden_d_model': 192, 'token_conv_kernel': 9, 'last_d_model': 864, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 96, 'pos_d_model': 96, 'd_model': 416, 'conv_out_dim': 384, 'e_layers': 4, 'learning_rate': 0.009860121199804296, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 02:42:11,138] The parameter 'norm_type' in trial#139 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 20.1 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
20.1 M    Trainable params
0         Non-trainable params
20.1 M    Total params
80.246    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.123
Metric Loss/val improved by 0.081 >= min_delta = 0.0. New best score: 1.042
Metric Loss/val improved by 0.012 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                           Best trial: 93. Best value: 0.969243:  70%|███████   | 70/100 [3:05:40<1:24:02, 168.08s/it]Best trial: 93. Best value: 0.969243:  70%|███████   | 70/100 [3:05:40<1:24:02, 168.08s/it]Best trial: 93. Best value: 0.969243:  71%|███████   | 71/100 [3:05:40<1:12:10, 149.33s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  71%|███████   | 71/100 [3:05:40<1:12:10, 149.33s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 28.1 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
28.1 M    Trainable params
0         Non-trainable params
28.1 M    Total params
112.243   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.034
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.033
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.033. Signaling Trainer to stop.
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                           Best trial: 93. Best value: 0.969243:  71%|███████   | 71/100 [3:09:13<1:12:10, 149.33s/it]Best trial: 93. Best value: 0.969243:  71%|███████   | 71/100 [3:09:13<1:12:10, 149.33s/it]Best trial: 93. Best value: 0.969243:  72%|███████▏  | 72/100 [3:09:13<1:18:32, 168.30s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  72%|███████▏  | 72/100 [3:09:13<1:18:32, 168.30s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=8, input_dim=84, dec_in=84, output_dim=1, d_model=384, n_heads=4, e_layers=4, hidden_d_model=176, seq_layers=2, last_d_model=1728, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=80, pos_d_model=64, combine_type='add', seq_len=10, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=320), exp_settings='seq_len-10-lr-0.0205-d-384-hid_d-176-last_d-1728-tok_d-64-time_d-80-pos_d-64-e_layers-4-tok_conv_k-8-dropout-0.14-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.02046589412727246, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 02:43:56,425] Trial 139 finished with value: 1.1380998373031617 and parameters: {'num_heads': 36, 'hidden_d_model': 176, 'token_conv_kernel': 8, 'last_d_model': 1728, 'seq_len': 10, 'token_d_model': 64, 'time_d_model': 80, 'pos_d_model': 64, 'd_model': 384, 'conv_out_dim': 320, 'e_layers': 4, 'learning_rate': 0.02046589412727246, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 02:43:56,709] The parameter 'norm_type' in trial#141 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=416, n_heads=4, e_layers=8, hidden_d_model=208, seq_layers=2, last_d_model=1216, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=96, pos_d_model=80, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.1, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=288), exp_settings='seq_len-12-lr-0.0025-d-416-hid_d-208-last_d-1216-tok_d-64-time_d-96-pos_d-80-e_layers-8-tok_conv_k-10-dropout-0.1-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0025384411372887444, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 02:47:28,968] Trial 141 finished with value: 4.122301411628723 and parameters: {'num_heads': 36, 'hidden_d_model': 208, 'token_conv_kernel': 10, 'last_d_model': 1216, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 96, 'pos_d_model': 80, 'd_model': 416, 'conv_out_dim': 288, 'e_layers': 8, 'learning_rate': 0.0025384411372887444, 'dropout': 0.1, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 02:47:29,246] The parameter 'norm_type' in trial#143 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 24.5 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
24.5 M    Trainable params
0         Non-trainable params
24.5 M    Total params
97.980    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.385
Metric Loss/val improved by 0.220 >= min_delta = 0.0. New best score: 1.165
Metric Loss/val improved by 0.126 >= min_delta = 0.0. New best score: 1.038
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 1.037
Metric Loss/val improved by 0.007 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                           Best trial: 93. Best value: 0.969243:  72%|███████▏  | 72/100 [3:12:40<1:18:32, 168.30s/it]Best trial: 93. Best value: 0.969243:  72%|███████▏  | 72/100 [3:12:40<1:18:32, 168.30s/it]Best trial: 93. Best value: 0.969243:  73%|███████▎  | 73/100 [3:12:40<1:20:57, 179.92s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  73%|███████▎  | 73/100 [3:12:40<1:20:57, 179.92s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 23.9 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
23.9 M    Trainable params
0         Non-trainable params
23.9 M    Total params
95.643    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 2.738
Metric Loss/val improved by 1.701 >= min_delta = 0.0. New best score: 1.037
Metric Loss/val improved by 0.003 >= min_delta = 0.0. New best score: 1.034
Metric Loss/val improved by 0.004 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                           Best trial: 93. Best value: 0.969243:  73%|███████▎  | 73/100 [3:15:22<1:20:57, 179.92s/it]Best trial: 93. Best value: 0.969243:  73%|███████▎  | 73/100 [3:15:22<1:20:57, 179.92s/it]Best trial: 93. Best value: 0.969243:  74%|███████▍  | 74/100 [3:15:22<1:15:40, 174.62s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  74%|███████▍  | 74/100 [3:15:22<1:15:40, 174.62s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=480, n_heads=4, e_layers=6, hidden_d_model=176, seq_layers=2, last_d_model=672, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=96, pos_d_model=80, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=256), exp_settings='seq_len-12-lr-0.0208-d-480-hid_d-176-last_d-672-tok_d-64-time_d-96-pos_d-80-e_layers-6-tok_conv_k-9-dropout-0.14-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.020835226730097883, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 02:50:56,008] Trial 143 finished with value: 0.996825385093689 and parameters: {'num_heads': 36, 'hidden_d_model': 176, 'token_conv_kernel': 9, 'last_d_model': 672, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 96, 'pos_d_model': 80, 'd_model': 480, 'conv_out_dim': 256, 'e_layers': 6, 'learning_rate': 0.020835226730097883, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 02:50:56,270] The parameter 'norm_type' in trial#145 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=384, n_heads=4, e_layers=6, hidden_d_model=192, seq_layers=2, last_d_model=608, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=112, pos_d_model=80, combine_type='add', seq_len=10, pred_len=1, min_y_value=0.0, dropout=0.12000000000000001, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=320), exp_settings='seq_len-10-lr-0.0212-d-384-hid_d-192-last_d-608-tok_d-64-time_d-112-pos_d-80-e_layers-6-tok_conv_k-9-dropout-0.12000000000000001-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.021207141967001783, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 02:53:38,249] Trial 145 finished with value: 0.9961106300354003 and parameters: {'num_heads': 36, 'hidden_d_model': 192, 'token_conv_kernel': 9, 'last_d_model': 608, 'seq_len': 10, 'token_d_model': 64, 'time_d_model': 112, 'pos_d_model': 80, 'd_model': 384, 'conv_out_dim': 320, 'e_layers': 6, 'learning_rate': 0.021207141967001783, 'dropout': 0.12000000000000001, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 02:53:38,591] The parameter 'norm_type' in trial#147 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 27.0 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
27.0 M    Trainable params
0         Non-trainable params
27.0 M    Total params
107.961   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.095
Metric Loss/val improved by 0.046 >= min_delta = 0.0. New best score: 1.050
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.049
Metric Loss/val improved by 0.014 >= min_delta = 0.0. New best score: 1.035
Metric Loss/val improved by 0.006 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                           Best trial: 93. Best value: 0.969243:  74%|███████▍  | 74/100 [3:17:50<1:15:40, 174.62s/it]Best trial: 93. Best value: 0.969243:  74%|███████▍  | 74/100 [3:17:50<1:15:40, 174.62s/it]Best trial: 93. Best value: 0.969243:  75%|███████▌  | 75/100 [3:17:50<1:09:27, 166.72s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  75%|███████▌  | 75/100 [3:17:51<1:09:27, 166.72s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 18.4 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
18.4 M    Trainable params
0         Non-trainable params
18.4 M    Total params
73.407    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.064
Metric Loss/val improved by 0.035 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                           Best trial: 93. Best value: 0.969243:  75%|███████▌  | 75/100 [3:21:00<1:09:27, 166.72s/it]Best trial: 93. Best value: 0.969243:  75%|███████▌  | 75/100 [3:21:00<1:09:27, 166.72s/it]Best trial: 93. Best value: 0.969243:  76%|███████▌  | 76/100 [3:21:00<1:09:28, 173.68s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  76%|███████▌  | 76/100 [3:21:01<1:09:28, 173.68s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=384, n_heads=4, e_layers=4, hidden_d_model=208, seq_layers=2, last_d_model=1024, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=80, pos_d_model=112, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=384), exp_settings='seq_len-12-lr-0.0273-d-384-hid_d-208-last_d-1024-tok_d-64-time_d-80-pos_d-112-e_layers-4-tok_conv_k-10-dropout-0.14-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.02727046659780626, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 02:56:06,540] Trial 147 finished with value: 0.9974118471145631 and parameters: {'num_heads': 36, 'hidden_d_model': 208, 'token_conv_kernel': 10, 'last_d_model': 1024, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 80, 'pos_d_model': 112, 'd_model': 384, 'conv_out_dim': 384, 'e_layers': 4, 'learning_rate': 0.02727046659780626, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 02:56:06,790] The parameter 'norm_type' in trial#149 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=8, input_dim=84, dec_in=84, output_dim=1, d_model=320, n_heads=4, e_layers=4, hidden_d_model=192, seq_layers=2, last_d_model=1024, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=80, pos_d_model=112, combine_type='add', seq_len=10, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=320), exp_settings='seq_len-10-lr-0.0083-d-320-hid_d-192-last_d-1024-tok_d-64-time_d-80-pos_d-112-e_layers-4-tok_conv_k-8-dropout-0.14-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.00828406914505, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 02:59:16,457] Trial 149 finished with value: 0.9974984407424927 and parameters: {'num_heads': 36, 'hidden_d_model': 192, 'token_conv_kernel': 8, 'last_d_model': 1024, 'seq_len': 10, 'token_d_model': 64, 'time_d_model': 80, 'pos_d_model': 112, 'd_model': 320, 'conv_out_dim': 320, 'e_layers': 4, 'learning_rate': 0.00828406914505, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 02:59:16,742] The parameter 'norm_type' in trial#151 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 19.2 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
19.2 M    Trainable params
0         Non-trainable params
19.2 M    Total params
76.942    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.040
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.039
Metric Loss/val improved by 0.005 >= min_delta = 0.0. New best score: 1.034
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.034. Signaling Trainer to stop.
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                           Best trial: 93. Best value: 0.969243:  76%|███████▌  | 76/100 [3:24:14<1:09:28, 173.68s/it]Best trial: 93. Best value: 0.969243:  76%|███████▌  | 76/100 [3:24:14<1:09:28, 173.68s/it]Best trial: 93. Best value: 0.969243:  77%|███████▋  | 77/100 [3:24:14<1:08:52, 179.66s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  77%|███████▋  | 77/100 [3:24:14<1:08:52, 179.66s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 29.6 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
29.6 M    Trainable params
0         Non-trainable params
29.6 M    Total params
118.375   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.157
Metric Loss/val improved by 0.097 >= min_delta = 0.0. New best score: 1.061
Metric Loss/val improved by 0.028 >= min_delta = 0.0. New best score: 1.033
Metric Loss/val improved by 0.004 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                           Best trial: 93. Best value: 0.969243:  77%|███████▋  | 77/100 [3:27:21<1:08:52, 179.66s/it]Best trial: 93. Best value: 0.969243:  77%|███████▋  | 77/100 [3:27:21<1:08:52, 179.66s/it]Best trial: 93. Best value: 0.969243:  78%|███████▊  | 78/100 [3:27:21<1:06:39, 181.78s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  78%|███████▊  | 78/100 [3:27:21<1:06:39, 181.78s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=448, n_heads=4, e_layers=4, hidden_d_model=160, seq_layers=2, last_d_model=768, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=48, time_d_model=96, pos_d_model=96, combine_type='add', seq_len=10, pred_len=1, min_y_value=0.0, dropout=0.1, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=288), exp_settings='seq_len-10-lr-0.0057-d-448-hid_d-160-last_d-768-tok_d-48-time_d-96-pos_d-96-e_layers-4-tok_conv_k-10-dropout-0.1-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.005679997963921219, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 03:02:30,086] Trial 151 finished with value: 1.3994018316268924 and parameters: {'num_heads': 36, 'hidden_d_model': 160, 'token_conv_kernel': 10, 'last_d_model': 768, 'seq_len': 10, 'token_d_model': 48, 'time_d_model': 96, 'pos_d_model': 96, 'd_model': 448, 'conv_out_dim': 288, 'e_layers': 4, 'learning_rate': 0.005679997963921219, 'dropout': 0.1, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 03:02:30,396] The parameter 'norm_type' in trial#153 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=11, input_dim=84, dec_in=84, output_dim=1, d_model=480, n_heads=4, e_layers=6, hidden_d_model=176, seq_layers=2, last_d_model=960, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=96, pos_d_model=112, combine_type='add', seq_len=10, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=288), exp_settings='seq_len-10-lr-0.0124-d-480-hid_d-176-last_d-960-tok_d-64-time_d-96-pos_d-112-e_layers-6-tok_conv_k-11-dropout-0.16-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.01241744100699428, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 03:05:36,806] Trial 153 finished with value: 0.9965131640434266 and parameters: {'num_heads': 36, 'hidden_d_model': 176, 'token_conv_kernel': 11, 'last_d_model': 960, 'seq_len': 10, 'token_d_model': 64, 'time_d_model': 96, 'pos_d_model': 112, 'd_model': 480, 'conv_out_dim': 288, 'e_layers': 6, 'learning_rate': 0.01241744100699428, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 03:05:37,092] The parameter 'norm_type' in trial#155 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 29.8 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
29.8 M    Trainable params
0         Non-trainable params
29.8 M    Total params
119.111   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 10.000
Metric Loss/val improved by 8.956 >= min_delta = 0.0. New best score: 1.044
Metric Loss/val improved by 0.015 >= min_delta = 0.0. New best score: 1.028
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.028. Signaling Trainer to stop.
                                                                                           Best trial: 93. Best value: 0.969243:  78%|███████▊  | 78/100 [3:30:17<1:06:39, 181.78s/it]Best trial: 93. Best value: 0.969243:  78%|███████▊  | 78/100 [3:30:17<1:06:39, 181.78s/it]Best trial: 93. Best value: 0.969243:  79%|███████▉  | 79/100 [3:30:17<1:03:03, 180.15s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  79%|███████▉  | 79/100 [3:30:17<1:03:03, 180.15s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 31.4 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
31.4 M    Trainable params
0         Non-trainable params
31.4 M    Total params
125.758   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.082
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 1.080
Metric Loss/val improved by 0.040 >= min_delta = 0.0. New best score: 1.040
Metric Loss/val improved by 0.009 >= min_delta = 0.0. New best score: 1.031
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                           Best trial: 93. Best value: 0.969243:  79%|███████▉  | 79/100 [3:32:59<1:03:03, 180.15s/it]Best trial: 93. Best value: 0.969243:  79%|███████▉  | 79/100 [3:32:59<1:03:03, 180.15s/it]Best trial: 93. Best value: 0.969243:  80%|████████  | 80/100 [3:32:59<58:11, 174.58s/it]  /home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                         Best trial: 93. Best value: 0.969243:  80%|████████  | 80/100 [3:32:59<58:11, 174.58s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=480, n_heads=4, e_layers=6, hidden_d_model=176, seq_layers=2, last_d_model=1056, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=96, pos_d_model=80, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.2, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=320), exp_settings='seq_len-12-lr-0.0449-d-480-hid_d-176-last_d-1056-tok_d-64-time_d-96-pos_d-80-e_layers-6-tok_conv_k-10-dropout-0.2-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.04491536704847051, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 03:08:33,161] Trial 155 finished with value: 0.9952010154724121 and parameters: {'num_heads': 36, 'hidden_d_model': 176, 'token_conv_kernel': 10, 'last_d_model': 1056, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 96, 'pos_d_model': 80, 'd_model': 480, 'conv_out_dim': 320, 'e_layers': 6, 'learning_rate': 0.04491536704847051, 'dropout': 0.2, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 03:08:33,431] The parameter 'norm_type' in trial#157 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=384, n_heads=4, e_layers=6, hidden_d_model=192, seq_layers=2, last_d_model=1280, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=80, pos_d_model=112, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.12000000000000001, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=4, fc_layer_type='mha', conv_out_dim=416), exp_settings='seq_len-12-lr-0.0092-d-384-hid_d-192-last_d-1280-tok_d-64-time_d-80-pos_d-112-e_layers-6-tok_conv_k-10-dropout-0.12000000000000001-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-4-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.009186697436256209, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 03:11:14,728] Trial 157 finished with value: 1.1536527633666993 and parameters: {'num_heads': 4, 'hidden_d_model': 192, 'token_conv_kernel': 10, 'last_d_model': 1280, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 80, 'pos_d_model': 112, 'd_model': 384, 'conv_out_dim': 416, 'e_layers': 6, 'learning_rate': 0.009186697436256209, 'dropout': 0.12000000000000001, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 03:11:15,061] The parameter 'norm_type' in trial#159 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 21.2 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
21.2 M    Trainable params
0         Non-trainable params
21.2 M    Total params
84.828    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.099
Metric Loss/val improved by 0.061 >= min_delta = 0.0. New best score: 1.038
Metric Loss/val improved by 0.008 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                         Best trial: 93. Best value: 0.969243:  80%|████████  | 80/100 [3:35:22<58:11, 174.58s/it]Best trial: 93. Best value: 0.969243:  80%|████████  | 80/100 [3:35:22<58:11, 174.58s/it]Best trial: 93. Best value: 0.969243:  81%|████████  | 81/100 [3:35:22<52:21, 165.33s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                         Best trial: 93. Best value: 0.969243:  81%|████████  | 81/100 [3:35:23<52:21, 165.33s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 33.8 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
33.8 M    Trainable params
0         Non-trainable params
33.8 M    Total params
135.231   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.076
Metric Loss/val improved by 0.042 >= min_delta = 0.0. New best score: 1.034
Metric Loss/val improved by 0.005 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                         Best trial: 93. Best value: 0.969243:  81%|████████  | 81/100 [3:38:10<52:21, 165.33s/it]Best trial: 93. Best value: 0.969243:  81%|████████  | 81/100 [3:38:10<52:21, 165.33s/it]Best trial: 93. Best value: 0.969243:  82%|████████▏ | 82/100 [3:38:10<49:48, 166.02s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                         Best trial: 93. Best value: 0.969243:  82%|████████▏ | 82/100 [3:38:10<49:48, 166.02s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=11, input_dim=84, dec_in=84, output_dim=1, d_model=384, n_heads=4, e_layers=4, hidden_d_model=224, seq_layers=2, last_d_model=1056, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=80, pos_d_model=96, combine_type='add', seq_len=10, pred_len=1, min_y_value=0.0, dropout=0.18, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=256), exp_settings='seq_len-10-lr-0.0138-d-384-hid_d-224-last_d-1056-tok_d-64-time_d-80-pos_d-96-e_layers-4-tok_conv_k-11-dropout-0.18-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0137931209312903, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 03:13:38,485] Trial 159 finished with value: 0.9955389380455018 and parameters: {'num_heads': 36, 'hidden_d_model': 224, 'token_conv_kernel': 11, 'last_d_model': 1056, 'seq_len': 10, 'token_d_model': 64, 'time_d_model': 80, 'pos_d_model': 96, 'd_model': 384, 'conv_out_dim': 256, 'e_layers': 4, 'learning_rate': 0.0137931209312903, 'dropout': 0.18, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 03:13:38,754] The parameter 'norm_type' in trial#160 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=11, input_dim=84, dec_in=84, output_dim=1, d_model=384, n_heads=4, e_layers=6, hidden_d_model=176, seq_layers=2, last_d_model=1408, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=96, pos_d_model=96, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=416), exp_settings='seq_len-12-lr-0.0117-d-384-hid_d-176-last_d-1408-tok_d-64-time_d-96-pos_d-96-e_layers-6-tok_conv_k-11-dropout-0.14-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.011651027639066038, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 03:16:26,121] Trial 160 finished with value: 0.9959133982658386 and parameters: {'num_heads': 36, 'hidden_d_model': 176, 'token_conv_kernel': 11, 'last_d_model': 1408, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 96, 'pos_d_model': 96, 'd_model': 384, 'conv_out_dim': 416, 'e_layers': 6, 'learning_rate': 0.011651027639066038, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 03:16:26,391] The parameter 'norm_type' in trial#162 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 22.4 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
22.4 M    Trainable params
0         Non-trainable params
22.4 M    Total params
89.503    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.051
Metric Loss/val improved by 0.020 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                         Best trial: 93. Best value: 0.969243:  82%|████████▏ | 82/100 [3:40:29<49:48, 166.02s/it]Best trial: 93. Best value: 0.969243:  82%|████████▏ | 82/100 [3:40:29<49:48, 166.02s/it]Best trial: 93. Best value: 0.969243:  83%|████████▎ | 83/100 [3:40:29<44:42, 157.80s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                         Best trial: 93. Best value: 0.969243:  83%|████████▎ | 83/100 [3:40:29<44:42, 157.80s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 31.4 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
31.4 M    Trainable params
0         Non-trainable params
31.4 M    Total params
125.665   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.208
Metric Loss/val improved by 0.075 >= min_delta = 0.0. New best score: 1.133
Metric Loss/val improved by 0.093 >= min_delta = 0.0. New best score: 1.040
Metric Loss/val improved by 0.010 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                         Best trial: 93. Best value: 0.969243:  83%|████████▎ | 83/100 [3:43:12<44:42, 157.80s/it]Best trial: 93. Best value: 0.969243:  83%|████████▎ | 83/100 [3:43:12<44:42, 157.80s/it]Best trial: 93. Best value: 0.969243:  84%|████████▍ | 84/100 [3:43:12<42:29, 159.37s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                         Best trial: 93. Best value: 0.969243:  84%|████████▍ | 84/100 [3:43:12<42:29, 159.37s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=416, n_heads=4, e_layers=4, hidden_d_model=208, seq_layers=2, last_d_model=928, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=96, pos_d_model=80, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.12000000000000001, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=288), exp_settings='seq_len-12-lr-0.0098-d-416-hid_d-208-last_d-928-tok_d-64-time_d-96-pos_d-80-e_layers-4-tok_conv_k-10-dropout-0.12000000000000001-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.009807083310467654, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 03:18:44,739] Trial 162 finished with value: 2.2541060924530028 and parameters: {'num_heads': 36, 'hidden_d_model': 208, 'token_conv_kernel': 10, 'last_d_model': 928, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 96, 'pos_d_model': 80, 'd_model': 416, 'conv_out_dim': 288, 'e_layers': 4, 'learning_rate': 0.009807083310467654, 'dropout': 0.12000000000000001, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 03:18:45,038] The parameter 'norm_type' in trial#165 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=11, input_dim=84, dec_in=84, output_dim=1, d_model=384, n_heads=4, e_layers=6, hidden_d_model=192, seq_layers=2, last_d_model=800, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=96, pos_d_model=96, combine_type='add', seq_len=10, pred_len=1, min_y_value=0.0, dropout=0.1, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=384), exp_settings='seq_len-10-lr-0.0172-d-384-hid_d-192-last_d-800-tok_d-64-time_d-96-pos_d-96-e_layers-6-tok_conv_k-11-dropout-0.1-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.01716817683790236, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 03:21:27,783] Trial 165 finished with value: 0.996622896194458 and parameters: {'num_heads': 36, 'hidden_d_model': 192, 'token_conv_kernel': 11, 'last_d_model': 800, 'seq_len': 10, 'token_d_model': 64, 'time_d_model': 96, 'pos_d_model': 96, 'd_model': 384, 'conv_out_dim': 384, 'e_layers': 6, 'learning_rate': 0.01716817683790236, 'dropout': 0.1, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 03:21:28,048] The parameter 'norm_type' in trial#167 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 19.2 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
19.2 M    Trainable params
0         Non-trainable params
19.2 M    Total params
76.818    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.546
Metric Loss/val improved by 0.497 >= min_delta = 0.0. New best score: 1.049
Metric Loss/val improved by 0.020 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                         Best trial: 93. Best value: 0.969243:  84%|████████▍ | 84/100 [3:45:51<42:29, 159.37s/it]Best trial: 93. Best value: 0.969243:  84%|████████▍ | 84/100 [3:45:51<42:29, 159.37s/it]Best trial: 93. Best value: 0.969243:  85%|████████▌ | 85/100 [3:45:51<39:52, 159.47s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                         Best trial: 93. Best value: 0.969243:  85%|████████▌ | 85/100 [3:45:52<39:52, 159.47s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 26.7 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
26.7 M    Trainable params
0         Non-trainable params
26.7 M    Total params
106.778   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.051
Metric Loss/val improved by 0.020 >= min_delta = 0.0. New best score: 1.031
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                         Best trial: 93. Best value: 0.969243:  85%|████████▌ | 85/100 [3:48:36<39:52, 159.47s/it]Best trial: 93. Best value: 0.969243:  85%|████████▌ | 85/100 [3:48:36<39:52, 159.47s/it]Best trial: 93. Best value: 0.969243:  86%|████████▌ | 86/100 [3:48:36<37:35, 161.12s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                         Best trial: 93. Best value: 0.969243:  86%|████████▌ | 86/100 [3:48:37<37:35, 161.12s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=448, n_heads=4, e_layers=6, hidden_d_model=208, seq_layers=2, last_d_model=1024, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=96, pos_d_model=96, combine_type='add', seq_len=10, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=160), exp_settings='seq_len-10-lr-0.0173-d-448-hid_d-208-last_d-1024-tok_d-64-time_d-96-pos_d-96-e_layers-6-tok_conv_k-10-dropout-0.14-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.017320960093889393, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 03:24:07,480] Trial 167 finished with value: 0.9962058782577516 and parameters: {'num_heads': 36, 'hidden_d_model': 208, 'token_conv_kernel': 10, 'last_d_model': 1024, 'seq_len': 10, 'token_d_model': 64, 'time_d_model': 96, 'pos_d_model': 96, 'd_model': 448, 'conv_out_dim': 160, 'e_layers': 6, 'learning_rate': 0.017320960093889393, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 03:24:07,728] The parameter 'norm_type' in trial#169 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=11, input_dim=84, dec_in=84, output_dim=1, d_model=512, n_heads=4, e_layers=6, hidden_d_model=176, seq_layers=2, last_d_model=800, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=48, time_d_model=128, pos_d_model=96, combine_type='add', seq_len=10, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=288), exp_settings='seq_len-10-lr-0.0139-d-512-hid_d-176-last_d-800-tok_d-48-time_d-128-pos_d-96-e_layers-6-tok_conv_k-11-dropout-0.16-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.013917784847775552, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 03:26:52,452] Trial 169 finished with value: 0.9975330829620362 and parameters: {'num_heads': 36, 'hidden_d_model': 176, 'token_conv_kernel': 11, 'last_d_model': 800, 'seq_len': 10, 'token_d_model': 48, 'time_d_model': 128, 'pos_d_model': 96, 'd_model': 512, 'conv_out_dim': 288, 'e_layers': 6, 'learning_rate': 0.013917784847775552, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 03:26:52,775] The parameter 'norm_type' in trial#171 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 33.7 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
33.7 M    Trainable params
0         Non-trainable params
33.7 M    Total params
134.681   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.044
Metric Loss/val improved by 0.010 >= min_delta = 0.0. New best score: 1.034
Metric Loss/val improved by 0.005 >= min_delta = 0.0. New best score: 1.030
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.030. Signaling Trainer to stop.
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                         Best trial: 93. Best value: 0.969243:  86%|████████▌ | 86/100 [3:51:40<37:35, 161.12s/it]Best trial: 93. Best value: 0.969243:  86%|████████▌ | 86/100 [3:51:40<37:35, 161.12s/it]Best trial: 93. Best value: 0.969243:  87%|████████▋ | 87/100 [3:51:40<36:21, 167.83s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                         Best trial: 93. Best value: 0.969243:  87%|████████▋ | 87/100 [3:51:40<36:21, 167.83s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 25.2 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
25.2 M    Trainable params
0         Non-trainable params
25.2 M    Total params
100.812   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.052
Metric Loss/val improved by 0.018 >= min_delta = 0.0. New best score: 1.034
Metric Loss/val improved by 0.007 >= min_delta = 0.0. New best score: 1.027
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.027. Signaling Trainer to stop.
                                                                                         Best trial: 93. Best value: 0.969243:  87%|████████▋ | 87/100 [3:54:27<36:21, 167.83s/it]Best trial: 93. Best value: 0.969243:  87%|████████▋ | 87/100 [3:54:27<36:21, 167.83s/it]Best trial: 93. Best value: 0.969243:  88%|████████▊ | 88/100 [3:54:27<33:32, 167.74s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                         Best trial: 93. Best value: 0.969243:  88%|████████▊ | 88/100 [3:54:28<33:32, 167.74s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=11, input_dim=84, dec_in=84, output_dim=1, d_model=384, n_heads=4, e_layers=6, hidden_d_model=192, seq_layers=2, last_d_model=1248, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=80, pos_d_model=96, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=416), exp_settings='seq_len-12-lr-0.0062-d-384-hid_d-192-last_d-1248-tok_d-64-time_d-80-pos_d-96-e_layers-6-tok_conv_k-11-dropout-0.16-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.006240500201990646, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 03:29:55,949] Trial 171 finished with value: 0.993682885169983 and parameters: {'num_heads': 36, 'hidden_d_model': 192, 'token_conv_kernel': 11, 'last_d_model': 1248, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 80, 'pos_d_model': 96, 'd_model': 384, 'conv_out_dim': 416, 'e_layers': 6, 'learning_rate': 0.006240500201990646, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 03:29:56,198] The parameter 'norm_type' in trial#173 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=416, n_heads=4, e_layers=6, hidden_d_model=208, seq_layers=2, last_d_model=1152, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=80, pos_d_model=80, combine_type='add', seq_len=10, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=288), exp_settings='seq_len-10-lr-0.0083-d-416-hid_d-208-last_d-1152-tok_d-64-time_d-80-pos_d-80-e_layers-6-tok_conv_k-10-dropout-0.14-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.008278061978170864, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 03:32:43,470] Trial 173 finished with value: 0.9973359704017639 and parameters: {'num_heads': 36, 'hidden_d_model': 208, 'token_conv_kernel': 10, 'last_d_model': 1152, 'seq_len': 10, 'token_d_model': 64, 'time_d_model': 80, 'pos_d_model': 80, 'd_model': 416, 'conv_out_dim': 288, 'e_layers': 6, 'learning_rate': 0.008278061978170864, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 03:32:43,738] The parameter 'norm_type' in trial#175 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 21.0 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
21.0 M    Trainable params
0         Non-trainable params
21.0 M    Total params
84.167    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 10.000
Metric Loss/val improved by 6.304 >= min_delta = 0.0. New best score: 3.696
Metric Loss/val improved by 2.663 >= min_delta = 0.0. New best score: 1.033
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                         Best trial: 93. Best value: 0.969243:  88%|████████▊ | 88/100 [3:56:56<33:32, 167.74s/it]Best trial: 93. Best value: 0.969243:  88%|████████▊ | 88/100 [3:56:56<33:32, 167.74s/it]Best trial: 93. Best value: 0.969243:  89%|████████▉ | 89/100 [3:56:56<29:41, 161.96s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                         Best trial: 93. Best value: 0.969243:  89%|████████▉ | 89/100 [3:56:56<29:41, 161.96s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 22.6 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
22.6 M    Trainable params
0         Non-trainable params
22.6 M    Total params
90.236    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.031
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                         Best trial: 93. Best value: 0.969243:  89%|████████▉ | 89/100 [3:59:23<29:41, 161.96s/it]Best trial: 93. Best value: 0.969243:  89%|████████▉ | 89/100 [3:59:23<29:41, 161.96s/it]Best trial: 93. Best value: 0.969243:  90%|█████████ | 90/100 [3:59:23<26:16, 157.69s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                         Best trial: 93. Best value: 0.969243:  90%|█████████ | 90/100 [3:59:24<26:16, 157.69s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=448, n_heads=4, e_layers=6, hidden_d_model=240, seq_layers=2, last_d_model=1088, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=96, pos_d_model=80, combine_type='add', seq_len=10, pred_len=1, min_y_value=0.0, dropout=0.18, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=68, fc_layer_type='mha', conv_out_dim=192), exp_settings='seq_len-10-lr-0.0286-d-448-hid_d-240-last_d-1088-tok_d-64-time_d-96-pos_d-80-e_layers-6-tok_conv_k-10-dropout-0.18-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-68-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.02857393635625911, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 03:35:11,928] Trial 175 finished with value: 0.9963546752929688 and parameters: {'num_heads': 68, 'hidden_d_model': 240, 'token_conv_kernel': 10, 'last_d_model': 1088, 'seq_len': 10, 'token_d_model': 64, 'time_d_model': 96, 'pos_d_model': 80, 'd_model': 448, 'conv_out_dim': 192, 'e_layers': 6, 'learning_rate': 0.02857393635625911, 'dropout': 0.18, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 03:35:12,198] The parameter 'norm_type' in trial#177 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=11, input_dim=84, dec_in=84, output_dim=1, d_model=416, n_heads=4, e_layers=6, hidden_d_model=224, seq_layers=2, last_d_model=1344, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=48, time_d_model=96, pos_d_model=80, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.18, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=68, fc_layer_type='mha', conv_out_dim=288), exp_settings='seq_len-12-lr-0.0057-d-416-hid_d-224-last_d-1344-tok_d-48-time_d-96-pos_d-80-e_layers-6-tok_conv_k-11-dropout-0.18-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-68-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.005738570705560847, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 03:37:39,647] Trial 177 finished with value: 0.996079182624817 and parameters: {'num_heads': 68, 'hidden_d_model': 224, 'token_conv_kernel': 11, 'last_d_model': 1344, 'seq_len': 12, 'token_d_model': 48, 'time_d_model': 96, 'pos_d_model': 80, 'd_model': 416, 'conv_out_dim': 288, 'e_layers': 6, 'learning_rate': 0.005738570705560847, 'dropout': 0.18, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 03:37:39,899] The parameter 'norm_type' in trial#178 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 16.1 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
16.1 M    Trainable params
0         Non-trainable params
16.1 M    Total params
64.416    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.410
Metric Loss/val improved by 0.306 >= min_delta = 0.0. New best score: 1.104
Metric Loss/val improved by 0.040 >= min_delta = 0.0. New best score: 1.064
Metric Loss/val improved by 0.027 >= min_delta = 0.0. New best score: 1.037
Metric Loss/val improved by 0.008 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                         Best trial: 93. Best value: 0.969243:  90%|█████████ | 90/100 [4:01:24<26:16, 157.69s/it]Best trial: 93. Best value: 0.969243:  90%|█████████ | 90/100 [4:01:24<26:16, 157.69s/it]Best trial: 93. Best value: 0.969243:  91%|█████████ | 91/100 [4:01:24<22:00, 146.68s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                         Best trial: 93. Best value: 0.969243:  91%|█████████ | 91/100 [4:01:25<22:00, 146.68s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 22.2 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
22.2 M    Trainable params
0         Non-trainable params
22.2 M    Total params
88.853    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.108
Metric Loss/val improved by 0.080 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                         Best trial: 93. Best value: 0.969243:  91%|█████████ | 91/100 [4:03:14<22:00, 146.68s/it]Best trial: 93. Best value: 0.969243:  91%|█████████ | 91/100 [4:03:14<22:00, 146.68s/it]Best trial: 93. Best value: 0.969243:  92%|█████████▏| 92/100 [4:03:14<18:03, 135.50s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                         Best trial: 93. Best value: 0.969243:  92%|█████████▏| 92/100 [4:03:14<18:03, 135.50s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=352, n_heads=4, e_layers=6, hidden_d_model=224, seq_layers=2, last_d_model=1120, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=48, time_d_model=80, pos_d_model=96, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=224), exp_settings='seq_len-12-lr-0.0185-d-352-hid_d-224-last_d-1120-tok_d-48-time_d-80-pos_d-96-e_layers-6-tok_conv_k-10-dropout-0.14-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.01848525381146542, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 03:39:40,668] Trial 178 finished with value: 0.9959434866905212 and parameters: {'num_heads': 36, 'hidden_d_model': 224, 'token_conv_kernel': 10, 'last_d_model': 1120, 'seq_len': 12, 'token_d_model': 48, 'time_d_model': 80, 'pos_d_model': 96, 'd_model': 352, 'conv_out_dim': 224, 'e_layers': 6, 'learning_rate': 0.01848525381146542, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 03:39:41,309] The parameter 'norm_type' in trial#180 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=384, n_heads=4, e_layers=6, hidden_d_model=224, seq_layers=2, last_d_model=1088, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=96, pos_d_model=96, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=256), exp_settings='seq_len-12-lr-0.0263-d-384-hid_d-224-last_d-1088-tok_d-64-time_d-96-pos_d-96-e_layers-6-tok_conv_k-10-dropout-0.14-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0262533111942775, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 03:41:30,066] Trial 180 finished with value: 0.9950947165489197 and parameters: {'num_heads': 36, 'hidden_d_model': 224, 'token_conv_kernel': 10, 'last_d_model': 1088, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 96, 'pos_d_model': 96, 'd_model': 384, 'conv_out_dim': 256, 'e_layers': 6, 'learning_rate': 0.0262533111942775, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 03:41:30,355] The parameter 'norm_type' in trial#182 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 26.8 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
26.8 M    Trainable params
0         Non-trainable params
26.8 M    Total params
107.329   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.531
Metric Loss/val improved by 0.390 >= min_delta = 0.0. New best score: 1.141
Metric Loss/val improved by 0.110 >= min_delta = 0.0. New best score: 1.031
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                         Best trial: 93. Best value: 0.969243:  92%|█████████▏| 92/100 [4:05:47<18:03, 135.50s/it]Best trial: 93. Best value: 0.969243:  92%|█████████▏| 92/100 [4:05:47<18:03, 135.50s/it]Best trial: 93. Best value: 0.969243:  93%|█████████▎| 93/100 [4:05:47<16:25, 140.75s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                         Best trial: 93. Best value: 0.969243:  93%|█████████▎| 93/100 [4:05:47<16:25, 140.75s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 30.6 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
30.6 M    Trainable params
0         Non-trainable params
30.6 M    Total params
122.527   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.039
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.037
Metric Loss/val improved by 0.009 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                         Best trial: 93. Best value: 0.969243:  93%|█████████▎| 93/100 [4:08:49<16:25, 140.75s/it]Best trial: 93. Best value: 0.969243:  93%|█████████▎| 93/100 [4:08:49<16:25, 140.75s/it]Best trial: 93. Best value: 0.969243:  94%|█████████▍| 94/100 [4:08:49<15:19, 153.20s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                         Best trial: 93. Best value: 0.969243:  94%|█████████▍| 94/100 [4:08:49<15:19, 153.20s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=352, n_heads=4, e_layers=8, hidden_d_model=208, seq_layers=2, last_d_model=1376, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=96, pos_d_model=96, combine_type='add', seq_len=14, pred_len=1, min_y_value=0.0, dropout=0.18, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=320), exp_settings='seq_len-14-lr-0.0135-d-352-hid_d-208-last_d-1376-tok_d-64-time_d-96-pos_d-96-e_layers-8-tok_conv_k-10-dropout-0.18-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.013541362640114784, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 03:44:03,077] Trial 182 finished with value: 0.996555495262146 and parameters: {'num_heads': 36, 'hidden_d_model': 208, 'token_conv_kernel': 10, 'last_d_model': 1376, 'seq_len': 14, 'token_d_model': 64, 'time_d_model': 96, 'pos_d_model': 96, 'd_model': 352, 'conv_out_dim': 320, 'e_layers': 8, 'learning_rate': 0.013541362640114784, 'dropout': 0.18, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 03:44:03,330] The parameter 'norm_type' in trial#184 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=416, n_heads=4, e_layers=6, hidden_d_model=192, seq_layers=2, last_d_model=896, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=80, pos_d_model=96, combine_type='add', seq_len=16, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=384), exp_settings='seq_len-16-lr-0.0096-d-416-hid_d-192-last_d-896-tok_d-64-time_d-80-pos_d-96-e_layers-6-tok_conv_k-10-dropout-0.16-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.009553513858315713, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 03:47:05,306] Trial 184 finished with value: 0.9956001043319702 and parameters: {'num_heads': 36, 'hidden_d_model': 192, 'token_conv_kernel': 10, 'last_d_model': 896, 'seq_len': 16, 'token_d_model': 64, 'time_d_model': 80, 'pos_d_model': 96, 'd_model': 416, 'conv_out_dim': 384, 'e_layers': 6, 'learning_rate': 0.009553513858315713, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 03:47:05,585] The parameter 'norm_type' in trial#185 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 36.2 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
36.2 M    Trainable params
0         Non-trainable params
36.2 M    Total params
144.868   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.153
Metric Loss/val improved by 0.044 >= min_delta = 0.0. New best score: 1.109
Metric Loss/val improved by 0.008 >= min_delta = 0.0. New best score: 1.101
Metric Loss/val improved by 0.035 >= min_delta = 0.0. New best score: 1.066
Metric Loss/val improved by 0.037 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                         Best trial: 93. Best value: 0.969243:  94%|█████████▍| 94/100 [4:11:39<15:19, 153.20s/it]Best trial: 93. Best value: 0.969243:  94%|█████████▍| 94/100 [4:11:39<15:19, 153.20s/it]Best trial: 93. Best value: 0.969243:  95%|█████████▌| 95/100 [4:11:39<13:11, 158.23s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                         Best trial: 93. Best value: 0.969243:  95%|█████████▌| 95/100 [4:11:39<13:11, 158.23s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 26.7 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
26.7 M    Trainable params
0         Non-trainable params
26.7 M    Total params
106.606   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.038
Metric Loss/val improved by 0.007 >= min_delta = 0.0. New best score: 1.031
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                         Best trial: 93. Best value: 0.969243:  95%|█████████▌| 95/100 [4:15:07<13:11, 158.23s/it]Best trial: 93. Best value: 0.969243:  95%|█████████▌| 95/100 [4:15:07<13:11, 158.23s/it]Best trial: 93. Best value: 0.969243:  96%|█████████▌| 96/100 [4:15:07<11:32, 173.21s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                         Best trial: 93. Best value: 0.969243:  96%|█████████▌| 96/100 [4:15:08<11:32, 173.21s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=11, input_dim=84, dec_in=84, output_dim=1, d_model=448, n_heads=4, e_layers=6, hidden_d_model=192, seq_layers=2, last_d_model=992, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=80, pos_d_model=112, combine_type='add', seq_len=10, pred_len=1, min_y_value=0.0, dropout=0.2, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=416), exp_settings='seq_len-10-lr-0.0088-d-448-hid_d-192-last_d-992-tok_d-64-time_d-80-pos_d-112-e_layers-6-tok_conv_k-11-dropout-0.2-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.008845137930327717, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 03:49:55,285] Trial 185 finished with value: 0.9960658311843873 and parameters: {'num_heads': 36, 'hidden_d_model': 192, 'token_conv_kernel': 11, 'last_d_model': 992, 'seq_len': 10, 'token_d_model': 64, 'time_d_model': 80, 'pos_d_model': 112, 'd_model': 448, 'conv_out_dim': 416, 'e_layers': 6, 'learning_rate': 0.008845137930327717, 'dropout': 0.2, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 03:49:55,554] The parameter 'norm_type' in trial#187 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=11, input_dim=84, dec_in=84, output_dim=1, d_model=352, n_heads=4, e_layers=6, hidden_d_model=208, seq_layers=2, last_d_model=1472, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=80, pos_d_model=112, combine_type='add', seq_len=14, pred_len=1, min_y_value=0.0, dropout=0.2, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=320), exp_settings='seq_len-14-lr-0.0069-d-352-hid_d-208-last_d-1472-tok_d-64-time_d-80-pos_d-112-e_layers-6-tok_conv_k-11-dropout-0.2-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.006932230849932684, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 03:53:23,459] Trial 187 finished with value: 0.9959491968154908 and parameters: {'num_heads': 36, 'hidden_d_model': 208, 'token_conv_kernel': 11, 'last_d_model': 1472, 'seq_len': 14, 'token_d_model': 64, 'time_d_model': 80, 'pos_d_model': 112, 'd_model': 352, 'conv_out_dim': 320, 'e_layers': 6, 'learning_rate': 0.006932230849932684, 'dropout': 0.2, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 03:53:23,743] The parameter 'norm_type' in trial#189 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 28.1 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
28.1 M    Trainable params
0         Non-trainable params
28.1 M    Total params
112.462   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 2.248
Metric Loss/val improved by 1.192 >= min_delta = 0.0. New best score: 1.055
Metric Loss/val improved by 0.020 >= min_delta = 0.0. New best score: 1.035
Metric Loss/val improved by 0.006 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                         Best trial: 93. Best value: 0.969243:  96%|█████████▌| 96/100 [4:18:55<11:32, 173.21s/it]Best trial: 93. Best value: 0.969243:  96%|█████████▌| 96/100 [4:18:55<11:32, 173.21s/it]Best trial: 93. Best value: 0.969243:  97%|█████████▋| 97/100 [4:18:55<09:29, 189.70s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                         Best trial: 93. Best value: 0.969243:  97%|█████████▋| 97/100 [4:18:56<09:29, 189.70s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 26.5 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
26.5 M    Trainable params
0         Non-trainable params
26.5 M    Total params
105.837   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.460
Metric Loss/val improved by 0.179 >= min_delta = 0.0. New best score: 1.280
Metric Loss/val improved by 0.246 >= min_delta = 0.0. New best score: 1.035
Metric Loss/val improved by 0.004 >= min_delta = 0.0. New best score: 1.031
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                         Best trial: 93. Best value: 0.969243:  97%|█████████▋| 97/100 [4:21:50<09:29, 189.70s/it]Best trial: 93. Best value: 0.969243:  97%|█████████▋| 97/100 [4:21:50<09:29, 189.70s/it]Best trial: 93. Best value: 0.969243:  98%|█████████▊| 98/100 [4:21:50<06:10, 185.25s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                         Best trial: 93. Best value: 0.969243:  98%|█████████▊| 98/100 [4:21:51<06:10, 185.25s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=448, n_heads=4, e_layers=8, hidden_d_model=176, seq_layers=2, last_d_model=1312, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=80, pos_d_model=128, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.12000000000000001, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=256), exp_settings='seq_len-12-lr-0.0157-d-448-hid_d-176-last_d-1312-tok_d-64-time_d-80-pos_d-128-e_layers-8-tok_conv_k-10-dropout-0.12000000000000001-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.015709550874807302, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 03:57:11,620] Trial 189 finished with value: 0.9960692882537842 and parameters: {'num_heads': 36, 'hidden_d_model': 176, 'token_conv_kernel': 10, 'last_d_model': 1312, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 80, 'pos_d_model': 128, 'd_model': 448, 'conv_out_dim': 256, 'e_layers': 8, 'learning_rate': 0.015709550874807302, 'dropout': 0.12000000000000001, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 03:57:11,889] The parameter 'norm_type' in trial#192 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=352, n_heads=4, e_layers=6, hidden_d_model=192, seq_layers=2, last_d_model=1312, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=80, pos_d_model=96, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=352), exp_settings='seq_len-12-lr-0.0217-d-352-hid_d-192-last_d-1312-tok_d-64-time_d-80-pos_d-96-e_layers-6-tok_conv_k-10-dropout-0.16-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.021677520777291882, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 04:00:06,478] Trial 192 finished with value: 0.9967541217803957 and parameters: {'num_heads': 36, 'hidden_d_model': 192, 'token_conv_kernel': 10, 'last_d_model': 1312, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 80, 'pos_d_model': 96, 'd_model': 352, 'conv_out_dim': 352, 'e_layers': 6, 'learning_rate': 0.021677520777291882, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 04:00:07,082] The parameter 'norm_type' in trial#194 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 26.8 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
26.8 M    Trainable params
0         Non-trainable params
26.8 M    Total params
107.168   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.083
Metric Loss/val improved by 0.014 >= min_delta = 0.0. New best score: 1.069
Metric Loss/val improved by 0.027 >= min_delta = 0.0. New best score: 1.042
Metric Loss/val improved by 0.013 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                         Best trial: 93. Best value: 0.969243:  98%|█████████▊| 98/100 [4:24:51<06:10, 185.25s/it]Best trial: 93. Best value: 0.969243:  98%|█████████▊| 98/100 [4:24:51<06:10, 185.25s/it]Best trial: 93. Best value: 0.969243:  99%|█████████▉| 99/100 [4:24:51<03:04, 184.03s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                         Best trial: 93. Best value: 0.969243:  99%|█████████▉| 99/100 [4:24:52<03:04, 184.03s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 26.7 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
26.7 M    Trainable params
0         Non-trainable params
26.7 M    Total params
106.632   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.045
Metric Loss/val improved by 0.015 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                         Best trial: 93. Best value: 0.969243:  99%|█████████▉| 99/100 [4:27:57<03:04, 184.03s/it]Best trial: 93. Best value: 0.969243:  99%|█████████▉| 99/100 [4:27:57<03:04, 184.03s/it]Best trial: 93. Best value: 0.969243: 100%|██████████| 100/100 [4:27:57<00:00, 184.50s/it]Best trial: 93. Best value: 0.969243: 100%|██████████| 100/100 [4:27:57<00:00, 160.78s/it]

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=11, input_dim=84, dec_in=84, output_dim=1, d_model=416, n_heads=4, e_layers=6, hidden_d_model=208, seq_layers=2, last_d_model=1152, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=48, pos_d_model=112, combine_type='add', seq_len=14, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=288), exp_settings='seq_len-14-lr-0.0111-d-416-hid_d-208-last_d-1152-tok_d-64-time_d-48-pos_d-112-e_layers-6-tok_conv_k-11-dropout-0.14-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.011136409295144218, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 04:03:07,661] Trial 194 finished with value: 1.241103482246399 and parameters: {'num_heads': 36, 'hidden_d_model': 208, 'token_conv_kernel': 11, 'last_d_model': 1152, 'seq_len': 14, 'token_d_model': 64, 'time_d_model': 48, 'pos_d_model': 112, 'd_model': 416, 'conv_out_dim': 288, 'e_layers': 6, 'learning_rate': 0.011136409295144218, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 04:03:07,930] The parameter 'norm_type' in trial#196 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=11, input_dim=84, dec_in=84, output_dim=1, d_model=416, n_heads=4, e_layers=6, hidden_d_model=208, seq_layers=2, last_d_model=576, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=80, pos_d_model=112, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.18, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=288), exp_settings='seq_len-12-lr-0.0078-d-416-hid_d-208-last_d-576-tok_d-64-time_d-80-pos_d-112-e_layers-6-tok_conv_k-11-dropout-0.18-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.007776717290961747, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 04:06:13,276] Trial 196 finished with value: 0.9960413932800293 and parameters: {'num_heads': 36, 'hidden_d_model': 208, 'token_conv_kernel': 11, 'last_d_model': 576, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 80, 'pos_d_model': 112, 'd_model': 416, 'conv_out_dim': 288, 'e_layers': 6, 'learning_rate': 0.007776717290961747, 'dropout': 0.18, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
Number of finished trials: 198
Best trial:
Value: 0.969242835044861
Params: 
    num_heads: 36
    hidden_d_model: 176
    token_conv_kernel: 7
    last_d_model: 672
    seq_len: 12
    token_d_model: 64
    time_d_model: 96
    pos_d_model: 64
    d_model: 384
    conv_out_dim: 288
    e_layers: 6
    learning_rate: 0.017511033836363155
    dropout: 0.18
    combine_type: add
    use_pos_enc: True
    norm_type: layer
    fc_layer_type: mha
    batch_size: 1024
    train_epochs: 30
Top 10 trials saved to /data3/lsf/Pein/Power-Prediction/optuna_results/24-08-13-mlp_v3-search/24-08-13-mlp_v3-search-farm_89_top10_params.json
Total time taken:  16079.7439 seconds,  268.00 minutes
Done!
