[I 2024-08-12 14:28:18,565] A new study created in RDB with name: 24-08-12-farm_98-mlpv3-search-time=hour
Creating study "24-08-12-farm_98-mlpv3-search-time=hour" with storage "sqlite:////data3/lsf/Pein/Power-Prediction/optuna_results/24-08-12/24-08-12-farm_98-mlpv3-search-time=hour.db?mode=wal"...
  0%|          | 0/100 [00:00<?, ?it/s]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA RTX 6000 Ada Generation') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 18.1 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
18.1 M    Trainable params
0         Non-trainable params
18.1 M    Total params
72.218    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.179
Metric Loss/val improved by 0.031 >= min_delta = 0.0. New best score: 1.148
Metric Loss/val improved by 0.107 >= min_delta = 0.0. New best score: 1.041
Metric Loss/val improved by 0.188 >= min_delta = 0.0. New best score: 0.853
`Trainer.fit` stopped: `max_epochs=30` reached.
                                         0%|          | 0/100 [04:47<?, ?it/s]Best trial: 0. Best value: 0.683257:   0%|          | 0/100 [04:47<?, ?it/s]Best trial: 0. Best value: 0.683257:   1%|          | 1/100 [04:47<7:54:14, 287.42s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                       Best trial: 0. Best value: 0.683257:   1%|          | 1/100 [04:48<7:54:14, 287.42s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 31.5 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
31.5 M    Trainable params
0         Non-trainable params
31.5 M    Total params
126.042   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.184
Metric Loss/val improved by 0.085 >= min_delta = 0.0. New best score: 1.100
Metric Loss/val improved by 0.111 >= min_delta = 0.0. New best score: 0.988
Metric Loss/val improved by 0.204 >= min_delta = 0.0. New best score: 0.784
Metric Loss/val improved by 0.016 >= min_delta = 0.0. New best score: 0.768
Metric Loss/val improved by 0.030 >= min_delta = 0.0. New best score: 0.738
Metric Loss/val improved by 0.049 >= min_delta = 0.0. New best score: 0.689
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 0.688
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 0.686
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 0.685
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                       Best trial: 0. Best value: 0.683257:   1%|          | 1/100 [11:26<7:54:14, 287.42s/it]Best trial: 3. Best value: 0.595208:   1%|          | 1/100 [11:26<7:54:14, 287.42s/it]Best trial: 3. Best value: 0.595208:   2%|▏         | 2/100 [11:26<9:36:48, 353.15s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                       Best trial: 3. Best value: 0.595208:   2%|▏         | 2/100 [11:27<9:36:48, 353.15s/it]Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=11, input_dim=84, dec_in=84, output_dim=1, d_model=992, n_heads=4, e_layers=2, hidden_d_model=944, seq_layers=2, last_d_model=96, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=8, time_d_model=144, pos_d_model=128, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.3, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=8, fc_layer_type='mha', conv_out_dim=352), exp_settings='seq_len-42-lr-0.0002-d-992-hid_d-944-last_d-96-time_d-144-e_layers-2-tok_conv_k-11-dropout-0.3-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=20, learning_rate=0.00019477091237405614, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 14:33:06,016] Trial 0 finished with value: 0.6832572543993592 and parameters: {'hidden_d_model': 944, 'token_conv_kernel': 11, 'last_d_model': 96, 'seq_len': 42, 'token_d_model': 8, 'time_d_model': 144, 'pos_d_model': 128, 'd_model': 992, 'conv_out_dim': 352, 'e_layers': 2, 'learning_rate': 0.00019477091237405614, 'dropout': 0.3, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 0 with value: 0.6832572543993592.
[W 2024-08-12 14:33:06,806] The parameter 'norm_type' in trial#3 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=768, n_heads=4, e_layers=6, hidden_d_model=752, seq_layers=2, last_d_model=576, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=144, pos_d_model=192, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.24000000000000002, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=8, fc_layer_type='mha', conv_out_dim=160), exp_settings='seq_len-42-lr-0.0009-d-768-hid_d-752-last_d-576-time_d-144-e_layers-6-tok_conv_k-10-dropout-0.24000000000000002-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=20, learning_rate=0.0009306325724138493, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 14:39:45,166] Trial 3 finished with value: 0.5952083382755519 and parameters: {'hidden_d_model': 752, 'token_conv_kernel': 10, 'last_d_model': 576, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 144, 'pos_d_model': 192, 'd_model': 768, 'conv_out_dim': 160, 'e_layers': 6, 'learning_rate': 0.0009306325724138493, 'dropout': 0.24000000000000002, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 3 with value: 0.5952083382755519.
[W 2024-08-12 14:39:46,113] The parameter 'norm_type' in trial#5 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 20.2 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
20.2 M    Trainable params
0         Non-trainable params
20.2 M    Total params
80.754    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.207
Metric Loss/val improved by 0.047 >= min_delta = 0.0. New best score: 1.159
Metric Loss/val improved by 0.193 >= min_delta = 0.0. New best score: 0.966
Metric Loss/val improved by 0.171 >= min_delta = 0.0. New best score: 0.795
Metric Loss/val improved by 0.025 >= min_delta = 0.0. New best score: 0.770
Metric Loss/val improved by 0.003 >= min_delta = 0.0. New best score: 0.767
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                       Best trial: 3. Best value: 0.595208:   2%|▏         | 2/100 [16:32<9:36:48, 353.15s/it]Best trial: 3. Best value: 0.595208:   2%|▏         | 2/100 [16:32<9:36:48, 353.15s/it]Best trial: 3. Best value: 0.595208:   3%|▎         | 3/100 [16:32<8:55:51, 331.46s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                       Best trial: 3. Best value: 0.595208:   3%|▎         | 3/100 [16:33<8:55:51, 331.46s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 14.4 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
14.4 M    Trainable params
0         Non-trainable params
14.4 M    Total params
57.578    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.222
Metric Loss/val improved by 0.019 >= min_delta = 0.0. New best score: 1.203
Metric Loss/val improved by 0.109 >= min_delta = 0.0. New best score: 1.094
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 1.092
Metric Loss/val improved by 0.132 >= min_delta = 0.0. New best score: 0.960
Metric Loss/val improved by 0.042 >= min_delta = 0.0. New best score: 0.917
Metric Loss/val improved by 0.096 >= min_delta = 0.0. New best score: 0.821
Metric Loss/val improved by 0.007 >= min_delta = 0.0. New best score: 0.815
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                       Best trial: 3. Best value: 0.595208:   3%|▎         | 3/100 [21:27<8:55:51, 331.46s/it]Best trial: 3. Best value: 0.595208:   3%|▎         | 3/100 [21:27<8:55:51, 331.46s/it]Best trial: 3. Best value: 0.595208:   4%|▍         | 4/100 [21:27<8:27:18, 317.06s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                       Best trial: 3. Best value: 0.595208:   4%|▍         | 4/100 [21:28<8:27:18, 317.06s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=13, input_dim=84, dec_in=84, output_dim=1, d_model=736, n_heads=4, e_layers=4, hidden_d_model=624, seq_layers=2, last_d_model=544, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=8, time_d_model=144, pos_d_model=144, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.22, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=8, fc_layer_type='mha', conv_out_dim=256), exp_settings='seq_len-42-lr-0.0012-d-736-hid_d-624-last_d-544-time_d-144-e_layers-4-tok_conv_k-13-dropout-0.22-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=20, learning_rate=0.0011782505404409145, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 14:44:50,828] Trial 5 finished with value: 0.6493234105408192 and parameters: {'hidden_d_model': 624, 'token_conv_kernel': 13, 'last_d_model': 544, 'seq_len': 42, 'token_d_model': 8, 'time_d_model': 144, 'pos_d_model': 144, 'd_model': 736, 'conv_out_dim': 256, 'e_layers': 4, 'learning_rate': 0.0011782505404409145, 'dropout': 0.22, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 3 with value: 0.5952083382755519.
[W 2024-08-12 14:44:51,629] The parameter 'norm_type' in trial#7 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=12, input_dim=84, dec_in=84, output_dim=1, d_model=544, n_heads=4, e_layers=4, hidden_d_model=736, seq_layers=2, last_d_model=384, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=8, time_d_model=176, pos_d_model=160, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.2, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=8, fc_layer_type='mha', conv_out_dim=384), exp_settings='seq_len-42-lr-0.0003-d-544-hid_d-736-last_d-384-time_d-176-e_layers-4-tok_conv_k-12-dropout-0.2-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0003134235998909641, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 14:49:45,819] Trial 7 finished with value: 0.6227329764515162 and parameters: {'hidden_d_model': 736, 'token_conv_kernel': 12, 'last_d_model': 384, 'seq_len': 42, 'token_d_model': 8, 'time_d_model': 176, 'pos_d_model': 160, 'd_model': 544, 'conv_out_dim': 384, 'e_layers': 4, 'learning_rate': 0.0003134235998909641, 'dropout': 0.2, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 3 with value: 0.5952083382755519.
[W 2024-08-12 14:49:46,662] The parameter 'norm_type' in trial#8 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 46.8 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
46.8 M    Trainable params
0         Non-trainable params
46.8 M    Total params
187.341   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.246
Metric Loss/val improved by 0.018 >= min_delta = 0.0. New best score: 1.228
Metric Loss/val improved by 0.197 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.048 >= min_delta = 0.0. New best score: 0.982
Metric Loss/val improved by 0.021 >= min_delta = 0.0. New best score: 0.962
Metric Loss/val improved by 0.091 >= min_delta = 0.0. New best score: 0.871
Metric Loss/val improved by 0.111 >= min_delta = 0.0. New best score: 0.760
Metric Loss/val improved by 0.015 >= min_delta = 0.0. New best score: 0.745
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                       Best trial: 3. Best value: 0.595208:   4%|▍         | 4/100 [32:38<8:27:18, 317.06s/it]Best trial: 6. Best value: 0.590104:   4%|▍         | 4/100 [32:38<8:27:18, 317.06s/it]Best trial: 6. Best value: 0.590104:   5%|▌         | 5/100 [32:38<11:44:21, 444.86s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                        Best trial: 6. Best value: 0.590104:   5%|▌         | 5/100 [32:38<11:44:21, 444.86s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 26.6 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
26.6 M    Trainable params
0         Non-trainable params
26.6 M    Total params
106.530   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.244
Metric Loss/val improved by 0.059 >= min_delta = 0.0. New best score: 1.185
Metric Loss/val improved by 0.163 >= min_delta = 0.0. New best score: 1.021
Metric Loss/val improved by 0.040 >= min_delta = 0.0. New best score: 0.981
Metric Loss/val improved by 0.079 >= min_delta = 0.0. New best score: 0.903
Metric Loss/val improved by 0.163 >= min_delta = 0.0. New best score: 0.740
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                        Best trial: 6. Best value: 0.590104:   5%|▌         | 5/100 [40:41<11:44:21, 444.86s/it]Best trial: 6. Best value: 0.590104:   5%|▌         | 5/100 [40:41<11:44:21, 444.86s/it]Best trial: 6. Best value: 0.590104:   6%|▌         | 6/100 [40:41<11:57:02, 457.68s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                        Best trial: 6. Best value: 0.590104:   6%|▌         | 6/100 [40:41<11:57:02, 457.68s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=13, input_dim=84, dec_in=84, output_dim=1, d_model=800, n_heads=4, e_layers=8, hidden_d_model=512, seq_layers=2, last_d_model=800, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=8, time_d_model=224, pos_d_model=144, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.26, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=8, fc_layer_type='mha', conv_out_dim=416), exp_settings='seq_len-42-lr-0.0006-d-800-hid_d-512-last_d-800-time_d-224-e_layers-8-tok_conv_k-13-dropout-0.26-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0005565440718915108, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 15:00:57,268] Trial 8 finished with value: 0.6362592462450266 and parameters: {'hidden_d_model': 512, 'token_conv_kernel': 13, 'last_d_model': 800, 'seq_len': 42, 'token_d_model': 8, 'time_d_model': 224, 'pos_d_model': 144, 'd_model': 800, 'conv_out_dim': 416, 'e_layers': 8, 'learning_rate': 0.0005565440718915108, 'dropout': 0.26, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 6 with value: 0.5901038590818644.
[W 2024-08-12 15:00:57,530] The parameter 'norm_type' in trial#11 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=11, input_dim=84, dec_in=84, output_dim=1, d_model=544, n_heads=4, e_layers=8, hidden_d_model=656, seq_layers=2, last_d_model=800, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=160, pos_d_model=112, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.22, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=8, fc_layer_type='mha', conv_out_dim=288), exp_settings='seq_len-42-lr-0.0006-d-544-hid_d-656-last_d-800-time_d-160-e_layers-8-tok_conv_k-11-dropout-0.22-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0005967033072614927, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 15:08:59,849] Trial 11 finished with value: 0.605155685544014 and parameters: {'hidden_d_model': 656, 'token_conv_kernel': 11, 'last_d_model': 800, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 160, 'pos_d_model': 112, 'd_model': 544, 'conv_out_dim': 288, 'e_layers': 8, 'learning_rate': 0.0005967033072614927, 'dropout': 0.22, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 6 with value: 0.5901038590818644.
[W 2024-08-12 15:09:00,169] The parameter 'norm_type' in trial#14 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 33.8 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
33.8 M    Trainable params
0         Non-trainable params
33.8 M    Total params
135.358   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.227
Metric Loss/val improved by 0.013 >= min_delta = 0.0. New best score: 1.214
Metric Loss/val improved by 0.153 >= min_delta = 0.0. New best score: 1.061
Metric Loss/val improved by 0.013 >= min_delta = 0.0. New best score: 1.048
Metric Loss/val improved by 0.095 >= min_delta = 0.0. New best score: 0.952
Metric Loss/val improved by 0.043 >= min_delta = 0.0. New best score: 0.909
Metric Loss/val improved by 0.069 >= min_delta = 0.0. New best score: 0.840
Metric Loss/val improved by 0.007 >= min_delta = 0.0. New best score: 0.833
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 0.831
Metric Loss/val improved by 0.009 >= min_delta = 0.0. New best score: 0.822
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                        Best trial: 6. Best value: 0.590104:   6%|▌         | 6/100 [49:41<11:57:02, 457.68s/it]Best trial: 6. Best value: 0.590104:   6%|▌         | 6/100 [49:41<11:57:02, 457.68s/it]Best trial: 6. Best value: 0.590104:   7%|▋         | 7/100 [49:41<12:31:17, 484.71s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                        Best trial: 6. Best value: 0.590104:   7%|▋         | 7/100 [49:41<12:31:17, 484.71s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 26.2 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
26.2 M    Trainable params
0         Non-trainable params
26.2 M    Total params
104.792   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.229
Metric Loss/val improved by 0.019 >= min_delta = 0.0. New best score: 1.210
Metric Loss/val improved by 0.233 >= min_delta = 0.0. New best score: 0.977
Metric Loss/val improved by 0.043 >= min_delta = 0.0. New best score: 0.934
Metric Loss/val improved by 0.051 >= min_delta = 0.0. New best score: 0.883
Metric Loss/val improved by 0.046 >= min_delta = 0.0. New best score: 0.837
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                        Best trial: 6. Best value: 0.590104:   7%|▋         | 7/100 [58:19<12:31:17, 484.71s/it]Best trial: 6. Best value: 0.590104:   7%|▋         | 7/100 [58:19<12:31:17, 484.71s/it]Best trial: 6. Best value: 0.590104:   8%|▊         | 8/100 [58:19<12:39:38, 495.42s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                        Best trial: 6. Best value: 0.590104:   8%|▊         | 8/100 [58:20<12:39:38, 495.42s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=768, n_heads=4, e_layers=6, hidden_d_model=720, seq_layers=2, last_d_model=512, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=80, pos_d_model=192, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.12000000000000001, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=8, fc_layer_type='mha', conv_out_dim=256), exp_settings='seq_len-42-lr-0.0004-d-768-hid_d-720-last_d-512-time_d-80-e_layers-6-tok_conv_k-10-dropout-0.12000000000000001-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.00042055599765295154, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 15:18:00,187] Trial 14 finished with value: 0.6318011920899154 and parameters: {'hidden_d_model': 720, 'token_conv_kernel': 10, 'last_d_model': 512, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 80, 'pos_d_model': 192, 'd_model': 768, 'conv_out_dim': 256, 'e_layers': 6, 'learning_rate': 0.00042055599765295154, 'dropout': 0.12000000000000001, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 6 with value: 0.5901038590818644.
[W 2024-08-12 15:18:00,439] The parameter 'norm_type' in trial#16 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=8, input_dim=84, dec_in=84, output_dim=1, d_model=704, n_heads=4, e_layers=6, hidden_d_model=640, seq_layers=2, last_d_model=448, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=128, pos_d_model=96, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=8, fc_layer_type='mha', conv_out_dim=160), exp_settings='seq_len-42-lr-0.0012-d-704-hid_d-640-last_d-448-time_d-128-e_layers-6-tok_conv_k-8-dropout-0.16-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0012364813219680015, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 15:26:38,550] Trial 16 finished with value: 0.6276394031941892 and parameters: {'hidden_d_model': 640, 'token_conv_kernel': 8, 'last_d_model': 448, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 128, 'pos_d_model': 96, 'd_model': 704, 'conv_out_dim': 160, 'e_layers': 6, 'learning_rate': 0.0012364813219680015, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 6 with value: 0.5901038590818644.
[W 2024-08-12 15:26:38,898] The parameter 'norm_type' in trial#18 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 56.4 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
56.4 M    Trainable params
0         Non-trainable params
56.4 M    Total params
225.590   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.231
Metric Loss/val improved by 0.017 >= min_delta = 0.0. New best score: 1.214
Metric Loss/val improved by 0.170 >= min_delta = 0.0. New best score: 1.044
Metric Loss/val improved by 0.135 >= min_delta = 0.0. New best score: 0.909
Metric Loss/val improved by 0.050 >= min_delta = 0.0. New best score: 0.859
Metric Loss/val improved by 0.025 >= min_delta = 0.0. New best score: 0.833
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                        Best trial: 6. Best value: 0.590104:   8%|▊         | 8/100 [1:09:11<12:39:38, 495.42s/it]Best trial: 6. Best value: 0.590104:   8%|▊         | 8/100 [1:09:11<12:39:38, 495.42s/it]Best trial: 6. Best value: 0.590104:   9%|▉         | 9/100 [1:09:11<13:45:30, 544.29s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                          Best trial: 6. Best value: 0.590104:   9%|▉         | 9/100 [1:09:11<13:45:30, 544.29s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 17.7 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
17.7 M    Trainable params
0         Non-trainable params
17.7 M    Total params
70.659    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.207
Metric Loss/val improved by 0.043 >= min_delta = 0.0. New best score: 1.164
Metric Loss/val improved by 0.184 >= min_delta = 0.0. New best score: 0.979
Metric Loss/val improved by 0.061 >= min_delta = 0.0. New best score: 0.918
Metric Loss/val improved by 0.020 >= min_delta = 0.0. New best score: 0.899
Metric Loss/val improved by 0.025 >= min_delta = 0.0. New best score: 0.873
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                          Best trial: 6. Best value: 0.590104:   9%|▉         | 9/100 [1:14:19<13:45:30, 544.29s/it]Best trial: 17. Best value: 0.562415:   9%|▉         | 9/100 [1:14:19<13:45:30, 544.29s/it]Best trial: 17. Best value: 0.562415:  10%|█         | 10/100 [1:14:19<11:47:04, 471.38s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                            Best trial: 17. Best value: 0.562415:  10%|█         | 10/100 [1:14:20<11:47:04, 471.38s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=12, input_dim=84, dec_in=84, output_dim=1, d_model=704, n_heads=4, e_layers=12, hidden_d_model=640, seq_layers=2, last_d_model=576, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=160, pos_d_model=80, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.18, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=8, fc_layer_type='mha', conv_out_dim=320), exp_settings='seq_len-42-lr-0.0004-d-704-hid_d-640-last_d-576-time_d-160-e_layers-12-tok_conv_k-12-dropout-0.18-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.00040210136035163685, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 15:37:30,291] Trial 18 finished with value: 0.64903357103467 and parameters: {'hidden_d_model': 640, 'token_conv_kernel': 12, 'last_d_model': 576, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 160, 'pos_d_model': 80, 'd_model': 704, 'conv_out_dim': 320, 'e_layers': 12, 'learning_rate': 0.00040210136035163685, 'dropout': 0.18, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 6 with value: 0.5901038590818644.
[W 2024-08-12 15:37:30,551] The parameter 'norm_type' in trial#19 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=672, n_heads=4, e_layers=4, hidden_d_model=800, seq_layers=2, last_d_model=384, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=176, pos_d_model=80, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=8, fc_layer_type='mha', conv_out_dim=160), exp_settings='seq_len-42-lr-0.0006-d-672-hid_d-800-last_d-384-time_d-176-e_layers-4-tok_conv_k-10-dropout-0.14-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0006093635213528346, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 15:42:38,409] Trial 19 finished with value: 0.6759127914905549 and parameters: {'hidden_d_model': 800, 'token_conv_kernel': 10, 'last_d_model': 384, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 176, 'pos_d_model': 80, 'd_model': 672, 'conv_out_dim': 160, 'e_layers': 4, 'learning_rate': 0.0006093635213528346, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 17 with value: 0.562415336444974.
[W 2024-08-12 15:42:38,736] The parameter 'norm_type' in trial#21 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 18.4 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
18.4 M    Trainable params
0         Non-trainable params
18.4 M    Total params
73.753    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.201
Metric Loss/val improved by 0.098 >= min_delta = 0.0. New best score: 1.103
Metric Loss/val improved by 0.255 >= min_delta = 0.0. New best score: 0.848
Metric Loss/val improved by 0.034 >= min_delta = 0.0. New best score: 0.814
Metric Loss/val improved by 0.072 >= min_delta = 0.0. New best score: 0.742
Metric Loss/val improved by 0.024 >= min_delta = 0.0. New best score: 0.718
Monitored metric Loss/val did not improve in the last 15 records. Best score: 0.718. Signaling Trainer to stop.
                                                                                            Best trial: 17. Best value: 0.562415:  10%|█         | 10/100 [1:18:43<11:47:04, 471.38s/it]Best trial: 17. Best value: 0.562415:  10%|█         | 10/100 [1:18:43<11:47:04, 471.38s/it]Best trial: 17. Best value: 0.562415:  11%|█         | 11/100 [1:18:43<10:05:05, 407.92s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                            Best trial: 17. Best value: 0.562415:  11%|█         | 11/100 [1:18:44<10:05:05, 407.92s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 64.6 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
64.6 M    Trainable params
0         Non-trainable params
64.6 M    Total params
258.459   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.239
Metric Loss/val improved by 0.115 >= min_delta = 0.0. New best score: 1.124
Metric Loss/val improved by 0.035 >= min_delta = 0.0. New best score: 1.089
Metric Loss/val improved by 0.013 >= min_delta = 0.0. New best score: 1.076
Metric Loss/val improved by 0.063 >= min_delta = 0.0. New best score: 1.013
Metric Loss/val improved by 0.030 >= min_delta = 0.0. New best score: 0.984
Metric Loss/val improved by 0.161 >= min_delta = 0.0. New best score: 0.823
Metric Loss/val improved by 0.042 >= min_delta = 0.0. New best score: 0.781
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 0.779
Metric Loss/val improved by 0.027 >= min_delta = 0.0. New best score: 0.753
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 0.751
Metric Loss/val improved by 0.003 >= min_delta = 0.0. New best score: 0.748
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 0.747
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                            Best trial: 17. Best value: 0.562415:  11%|█         | 11/100 [1:30:51<10:05:05, 407.92s/it]Best trial: 17. Best value: 0.562415:  11%|█         | 11/100 [1:30:51<10:05:05, 407.92s/it]Best trial: 17. Best value: 0.562415:  12%|█▏        | 12/100 [1:30:51<12:20:49, 505.11s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                            Best trial: 17. Best value: 0.562415:  12%|█▏        | 12/100 [1:30:51<12:20:49, 505.11s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=11, input_dim=84, dec_in=84, output_dim=1, d_model=608, n_heads=4, e_layers=4, hidden_d_model=752, seq_layers=2, last_d_model=384, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=160, pos_d_model=176, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=8, fc_layer_type='mha', conv_out_dim=256), exp_settings='seq_len-42-lr-0.0009-d-608-hid_d-752-last_d-384-time_d-160-e_layers-4-tok_conv_k-11-dropout-0.16-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0009398997838455256, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 15:47:02,449] Trial 21 finished with value: 0.6279676157981158 and parameters: {'hidden_d_model': 752, 'token_conv_kernel': 11, 'last_d_model': 384, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 160, 'pos_d_model': 176, 'd_model': 608, 'conv_out_dim': 256, 'e_layers': 4, 'learning_rate': 0.0009398997838455256, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 17 with value: 0.562415336444974.
[W 2024-08-12 15:47:02,726] The parameter 'norm_type' in trial#22 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=11, input_dim=84, dec_in=84, output_dim=1, d_model=864, n_heads=4, e_layers=10, hidden_d_model=640, seq_layers=2, last_d_model=448, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=144, pos_d_model=128, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.18, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=8, fc_layer_type='mha', conv_out_dim=224), exp_settings='seq_len-42-lr-0.0009-d-864-hid_d-640-last_d-448-time_d-144-e_layers-10-tok_conv_k-11-dropout-0.18-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0009010445205073401, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 15:59:09,846] Trial 22 finished with value: 0.6215283378958703 and parameters: {'hidden_d_model': 640, 'token_conv_kernel': 11, 'last_d_model': 448, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 144, 'pos_d_model': 128, 'd_model': 864, 'conv_out_dim': 224, 'e_layers': 10, 'learning_rate': 0.0009010445205073401, 'dropout': 0.18, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 17 with value: 0.562415336444974.
[W 2024-08-12 15:59:10,147] The parameter 'norm_type' in trial#25 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 45.8 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
45.8 M    Trainable params
0         Non-trainable params
45.8 M    Total params
183.067   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.249
Metric Loss/val improved by 0.044 >= min_delta = 0.0. New best score: 1.205
Metric Loss/val improved by 0.443 >= min_delta = 0.0. New best score: 0.762
Monitored metric Loss/val did not improve in the last 15 records. Best score: 0.762. Signaling Trainer to stop.
                                                                                            Best trial: 17. Best value: 0.562415:  12%|█▏        | 12/100 [1:40:19<12:20:49, 505.11s/it]Best trial: 17. Best value: 0.562415:  12%|█▏        | 12/100 [1:40:19<12:20:49, 505.11s/it]Best trial: 17. Best value: 0.562415:  13%|█▎        | 13/100 [1:40:19<12:40:19, 524.36s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                            Best trial: 17. Best value: 0.562415:  13%|█▎        | 13/100 [1:40:20<12:40:19, 524.36s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 43.1 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
43.1 M    Trainable params
0         Non-trainable params
43.1 M    Total params
172.258   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.203
Metric Loss/val improved by 0.075 >= min_delta = 0.0. New best score: 1.128
Metric Loss/val improved by 0.100 >= min_delta = 0.0. New best score: 1.028
Metric Loss/val improved by 0.005 >= min_delta = 0.0. New best score: 1.023
Metric Loss/val improved by 0.286 >= min_delta = 0.0. New best score: 0.738
Metric Loss/val improved by 0.068 >= min_delta = 0.0. New best score: 0.670
Monitored metric Loss/val did not improve in the last 15 records. Best score: 0.670. Signaling Trainer to stop.
                                                                                            Best trial: 17. Best value: 0.562415:  13%|█▎        | 13/100 [1:47:49<12:40:19, 524.36s/it]Best trial: 26. Best value: 0.544069:  13%|█▎        | 13/100 [1:47:49<12:40:19, 524.36s/it]Best trial: 26. Best value: 0.544069:  14%|█▍        | 14/100 [1:47:49<11:59:02, 501.66s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                            Best trial: 26. Best value: 0.544069:  14%|█▍        | 14/100 [1:47:49<11:59:02, 501.66s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=11, input_dim=84, dec_in=84, output_dim=1, d_model=832, n_heads=4, e_layers=8, hidden_d_model=768, seq_layers=2, last_d_model=576, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=176, pos_d_model=112, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=8, fc_layer_type='mha', conv_out_dim=96), exp_settings='seq_len-42-lr-0.0004-d-832-hid_d-768-last_d-576-time_d-176-e_layers-8-tok_conv_k-11-dropout-0.16-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0003753580352600608, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 16:08:38,503] Trial 25 finished with value: 0.5977018203586341 and parameters: {'hidden_d_model': 768, 'token_conv_kernel': 11, 'last_d_model': 576, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 176, 'pos_d_model': 112, 'd_model': 832, 'conv_out_dim': 96, 'e_layers': 8, 'learning_rate': 0.0003753580352600608, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 17 with value: 0.562415336444974.
[W 2024-08-12 16:08:38,831] The parameter 'norm_type' in trial#27 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=896, n_heads=4, e_layers=6, hidden_d_model=752, seq_layers=2, last_d_model=352, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=160, pos_d_model=176, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.2, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=8, fc_layer_type='mha', conv_out_dim=256), exp_settings='seq_len-42-lr-0.0008-d-896-hid_d-752-last_d-352-time_d-160-e_layers-6-tok_conv_k-9-dropout-0.2-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0008167822670892803, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 16:16:07,707] Trial 27 finished with value: 0.5742954697459937 and parameters: {'hidden_d_model': 752, 'token_conv_kernel': 9, 'last_d_model': 352, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 160, 'pos_d_model': 176, 'd_model': 896, 'conv_out_dim': 256, 'e_layers': 6, 'learning_rate': 0.0008167822670892803, 'dropout': 0.2, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 16:16:07,974] The parameter 'norm_type' in trial#29 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 46.7 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
46.7 M    Trainable params
0         Non-trainable params
46.7 M    Total params
186.807   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.250
Metric Loss/val improved by 0.049 >= min_delta = 0.0. New best score: 1.201
Metric Loss/val improved by 0.209 >= min_delta = 0.0. New best score: 0.993
Metric Loss/val improved by 0.096 >= min_delta = 0.0. New best score: 0.896
Metric Loss/val improved by 0.100 >= min_delta = 0.0. New best score: 0.796
Metric Loss/val improved by 0.037 >= min_delta = 0.0. New best score: 0.759
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                            Best trial: 26. Best value: 0.544069:  14%|█▍        | 14/100 [1:57:18<11:59:02, 501.66s/it]Best trial: 26. Best value: 0.544069:  14%|█▍        | 14/100 [1:57:18<11:59:02, 501.66s/it]Best trial: 26. Best value: 0.544069:  15%|█▌        | 15/100 [1:57:18<12:19:32, 522.03s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                            Best trial: 26. Best value: 0.544069:  15%|█▌        | 15/100 [1:57:18<12:19:32, 522.03s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 42.8 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
42.8 M    Trainable params
0         Non-trainable params
42.8 M    Total params
171.189   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.240
Metric Loss/val improved by 0.051 >= min_delta = 0.0. New best score: 1.190
Metric Loss/val improved by 0.177 >= min_delta = 0.0. New best score: 1.013
Metric Loss/val improved by 0.132 >= min_delta = 0.0. New best score: 0.881
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                            Best trial: 26. Best value: 0.544069:  15%|█▌        | 15/100 [2:05:37<12:19:32, 522.03s/it]Best trial: 26. Best value: 0.544069:  15%|█▌        | 15/100 [2:05:37<12:19:32, 522.03s/it]Best trial: 26. Best value: 0.544069:  16%|█▌        | 16/100 [2:05:37<12:01:06, 515.08s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                            Best trial: 26. Best value: 0.544069:  16%|█▌        | 16/100 [2:05:37<12:01:06, 515.08s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=800, n_heads=4, e_layers=8, hidden_d_model=624, seq_layers=2, last_d_model=320, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=176, pos_d_model=192, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.2, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=8, fc_layer_type='mha', conv_out_dim=288), exp_settings='seq_len-42-lr-0.0006-d-800-hid_d-624-last_d-320-time_d-176-e_layers-8-tok_conv_k-9-dropout-0.2-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0006435560871079242, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 16:25:36,956] Trial 29 finished with value: 0.6032962091267109 and parameters: {'hidden_d_model': 624, 'token_conv_kernel': 9, 'last_d_model': 320, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 176, 'pos_d_model': 192, 'd_model': 800, 'conv_out_dim': 288, 'e_layers': 8, 'learning_rate': 0.0006435560871079242, 'dropout': 0.2, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 16:25:37,222] The parameter 'norm_type' in trial#31 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=768, n_heads=4, e_layers=8, hidden_d_model=816, seq_layers=2, last_d_model=512, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=128, pos_d_model=176, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.18, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=8, fc_layer_type='mha', conv_out_dim=256), exp_settings='seq_len-42-lr-0.0004-d-768-hid_d-816-last_d-512-time_d-128-e_layers-8-tok_conv_k-9-dropout-0.18-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0004280341380271441, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 16:33:55,899] Trial 31 finished with value: 0.7447602007538081 and parameters: {'hidden_d_model': 816, 'token_conv_kernel': 9, 'last_d_model': 512, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 128, 'pos_d_model': 176, 'd_model': 768, 'conv_out_dim': 256, 'e_layers': 8, 'learning_rate': 0.0004280341380271441, 'dropout': 0.18, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 16:33:56,147] The parameter 'norm_type' in trial#32 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 95.7 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
95.7 M    Trainable params
0         Non-trainable params
95.7 M    Total params
382.799   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.266
Metric Loss/val improved by 0.043 >= min_delta = 0.0. New best score: 1.224
Metric Loss/val improved by 0.014 >= min_delta = 0.0. New best score: 1.210
Metric Loss/val improved by 0.399 >= min_delta = 0.0. New best score: 0.811
Metric Loss/val improved by 0.048 >= min_delta = 0.0. New best score: 0.763
Metric Loss/val improved by 0.025 >= min_delta = 0.0. New best score: 0.738
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                            Best trial: 26. Best value: 0.544069:  16%|█▌        | 16/100 [2:21:20<12:01:06, 515.08s/it]Best trial: 26. Best value: 0.544069:  16%|█▌        | 16/100 [2:21:20<12:01:06, 515.08s/it]Best trial: 26. Best value: 0.544069:  17%|█▋        | 17/100 [2:21:20<14:50:34, 643.79s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                            Best trial: 26. Best value: 0.544069:  17%|█▋        | 17/100 [2:21:20<14:50:34, 643.79s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 69.7 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
69.7 M    Trainable params
0         Non-trainable params
69.7 M    Total params
278.879   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.238
Metric Loss/val improved by 0.077 >= min_delta = 0.0. New best score: 1.160
Metric Loss/val improved by 0.046 >= min_delta = 0.0. New best score: 1.115
Metric Loss/val improved by 0.209 >= min_delta = 0.0. New best score: 0.906
Metric Loss/val improved by 0.070 >= min_delta = 0.0. New best score: 0.835
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                            Best trial: 26. Best value: 0.544069:  17%|█▋        | 17/100 [2:33:53<14:50:34, 643.79s/it]Best trial: 26. Best value: 0.544069:  17%|█▋        | 17/100 [2:33:53<14:50:34, 643.79s/it]Best trial: 26. Best value: 0.544069:  18%|█▊        | 18/100 [2:33:53<15:24:38, 676.57s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                            Best trial: 26. Best value: 0.544069:  18%|█▊        | 18/100 [2:33:53<15:24:38, 676.57s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=960, n_heads=4, e_layers=12, hidden_d_model=800, seq_layers=2, last_d_model=352, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=144, pos_d_model=96, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.24000000000000002, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=8, fc_layer_type='mha', conv_out_dim=384), exp_settings='seq_len-42-lr-0.0004-d-960-hid_d-800-last_d-352-time_d-144-e_layers-12-tok_conv_k-9-dropout-0.24000000000000002-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.00041874509726236243, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 16:49:39,017] Trial 32 finished with value: 0.6491206921637058 and parameters: {'hidden_d_model': 800, 'token_conv_kernel': 9, 'last_d_model': 352, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 144, 'pos_d_model': 96, 'd_model': 960, 'conv_out_dim': 384, 'e_layers': 12, 'learning_rate': 0.00041874509726236243, 'dropout': 0.24000000000000002, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 16:49:39,298] The parameter 'norm_type' in trial#35 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=896, n_heads=4, e_layers=10, hidden_d_model=736, seq_layers=2, last_d_model=576, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=144, pos_d_model=160, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.2, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=8, fc_layer_type='mha', conv_out_dim=288), exp_settings='seq_len-42-lr-0.001-d-896-hid_d-736-last_d-576-time_d-144-e_layers-10-tok_conv_k-9-dropout-0.2-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0010213570535474443, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 17:02:11,884] Trial 35 finished with value: 0.689245349727571 and parameters: {'hidden_d_model': 736, 'token_conv_kernel': 9, 'last_d_model': 576, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 144, 'pos_d_model': 160, 'd_model': 896, 'conv_out_dim': 288, 'e_layers': 10, 'learning_rate': 0.0010213570535474443, 'dropout': 0.2, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 17:02:12,202] The parameter 'norm_type' in trial#38 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 37.0 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
37.0 M    Trainable params
0         Non-trainable params
37.0 M    Total params
147.908   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.216
Metric Loss/val improved by 0.007 >= min_delta = 0.0. New best score: 1.209
Metric Loss/val improved by 0.155 >= min_delta = 0.0. New best score: 1.054
Metric Loss/val improved by 0.018 >= min_delta = 0.0. New best score: 1.036
Metric Loss/val improved by 0.114 >= min_delta = 0.0. New best score: 0.922
Metric Loss/val improved by 0.007 >= min_delta = 0.0. New best score: 0.915
Metric Loss/val improved by 0.048 >= min_delta = 0.0. New best score: 0.867
Metric Loss/val improved by 0.010 >= min_delta = 0.0. New best score: 0.857
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                            Best trial: 26. Best value: 0.544069:  18%|█▊        | 18/100 [2:41:05<15:24:38, 676.57s/it]Best trial: 26. Best value: 0.544069:  18%|█▊        | 18/100 [2:41:05<15:24:38, 676.57s/it]Best trial: 26. Best value: 0.544069:  19%|█▉        | 19/100 [2:41:05<13:34:08, 603.07s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                            Best trial: 26. Best value: 0.544069:  19%|█▉        | 19/100 [2:41:05<13:34:08, 603.07s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 28.3 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
28.3 M    Trainable params
0         Non-trainable params
28.3 M    Total params
113.137   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.213
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.212
Metric Loss/val improved by 0.082 >= min_delta = 0.0. New best score: 1.130
Metric Loss/val improved by 0.110 >= min_delta = 0.0. New best score: 1.020
Metric Loss/val improved by 0.086 >= min_delta = 0.0. New best score: 0.935
Metric Loss/val improved by 0.109 >= min_delta = 0.0. New best score: 0.825
Metric Loss/val improved by 0.062 >= min_delta = 0.0. New best score: 0.763
Monitored metric Loss/val did not improve in the last 15 records. Best score: 0.763. Signaling Trainer to stop.
                                                                                            Best trial: 26. Best value: 0.544069:  19%|█▉        | 19/100 [2:47:24<13:34:08, 603.07s/it]Best trial: 26. Best value: 0.544069:  19%|█▉        | 19/100 [2:47:24<13:34:08, 603.07s/it]Best trial: 26. Best value: 0.544069:  20%|██        | 20/100 [2:47:24<11:54:29, 535.86s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                            Best trial: 26. Best value: 0.544069:  20%|██        | 20/100 [2:47:24<11:54:29, 535.86s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=736, n_heads=4, e_layers=6, hidden_d_model=768, seq_layers=2, last_d_model=352, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=160, pos_d_model=224, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.28, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=8, fc_layer_type='mha', conv_out_dim=448), exp_settings='seq_len-42-lr-0.0004-d-736-hid_d-768-last_d-352-time_d-160-e_layers-6-tok_conv_k-10-dropout-0.28-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0003518496105154597, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 17:09:23,720] Trial 38 finished with value: 0.6247585337609053 and parameters: {'hidden_d_model': 768, 'token_conv_kernel': 10, 'last_d_model': 352, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 160, 'pos_d_model': 224, 'd_model': 736, 'conv_out_dim': 448, 'e_layers': 6, 'learning_rate': 0.0003518496105154597, 'dropout': 0.28, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 17:09:23,990] The parameter 'norm_type' in trial#39 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=8, input_dim=84, dec_in=84, output_dim=1, d_model=704, n_heads=4, e_layers=6, hidden_d_model=688, seq_layers=2, last_d_model=448, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=112, pos_d_model=160, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.28, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=8, fc_layer_type='mha', conv_out_dim=256), exp_settings='seq_len-42-lr-0.0002-d-704-hid_d-688-last_d-448-time_d-112-e_layers-6-tok_conv_k-8-dropout-0.28-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.00022961118043269096, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 17:15:42,967] Trial 39 finished with value: 0.5788745110854506 and parameters: {'hidden_d_model': 688, 'token_conv_kernel': 8, 'last_d_model': 448, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 112, 'pos_d_model': 160, 'd_model': 704, 'conv_out_dim': 256, 'e_layers': 6, 'learning_rate': 0.00022961118043269096, 'dropout': 0.28, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 17:15:43,192] The parameter 'norm_type' in trial#41 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 35.3 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
35.3 M    Trainable params
0         Non-trainable params
35.3 M    Total params
141.371   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.217
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 1.215
Metric Loss/val improved by 0.027 >= min_delta = 0.0. New best score: 1.188
Metric Loss/val improved by 0.195 >= min_delta = 0.0. New best score: 0.993
Metric Loss/val improved by 0.121 >= min_delta = 0.0. New best score: 0.872
Metric Loss/val improved by 0.015 >= min_delta = 0.0. New best score: 0.858
Metric Loss/val improved by 0.012 >= min_delta = 0.0. New best score: 0.845
Metric Loss/val improved by 0.033 >= min_delta = 0.0. New best score: 0.812
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                            Best trial: 26. Best value: 0.544069:  20%|██        | 20/100 [2:55:24<11:54:29, 535.86s/it]Best trial: 26. Best value: 0.544069:  20%|██        | 20/100 [2:55:24<11:54:29, 535.86s/it]Best trial: 26. Best value: 0.544069:  21%|██        | 21/100 [2:55:24<11:23:40, 519.25s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                            Best trial: 26. Best value: 0.544069:  21%|██        | 21/100 [2:55:25<11:23:40, 519.25s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 68.2 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
68.2 M    Trainable params
0         Non-trainable params
68.2 M    Total params
272.965   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.266
Metric Loss/val improved by 0.035 >= min_delta = 0.0. New best score: 1.231
Metric Loss/val improved by 0.065 >= min_delta = 0.0. New best score: 1.166
Metric Loss/val improved by 0.044 >= min_delta = 0.0. New best score: 1.122
Metric Loss/val improved by 0.255 >= min_delta = 0.0. New best score: 0.867
Metric Loss/val improved by 0.099 >= min_delta = 0.0. New best score: 0.768
Metric Loss/val improved by 0.077 >= min_delta = 0.0. New best score: 0.691
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                            Best trial: 26. Best value: 0.544069:  21%|██        | 21/100 [3:07:46<11:23:40, 519.25s/it]Best trial: 26. Best value: 0.544069:  21%|██        | 21/100 [3:07:46<11:23:40, 519.25s/it]Best trial: 26. Best value: 0.544069:  22%|██▏       | 22/100 [3:07:46<12:41:54, 586.09s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                            Best trial: 26. Best value: 0.544069:  22%|██▏       | 22/100 [3:07:47<12:41:54, 586.09s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=8, input_dim=84, dec_in=84, output_dim=1, d_model=832, n_heads=4, e_layers=6, hidden_d_model=816, seq_layers=2, last_d_model=704, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=8, time_d_model=160, pos_d_model=96, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.24000000000000002, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=8, fc_layer_type='mha', conv_out_dim=320), exp_settings='seq_len-42-lr-0.0003-d-832-hid_d-816-last_d-704-time_d-160-e_layers-6-tok_conv_k-8-dropout-0.24000000000000002-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.00031514731277786556, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 17:23:43,473] Trial 41 finished with value: 0.6411894265562297 and parameters: {'hidden_d_model': 816, 'token_conv_kernel': 8, 'last_d_model': 704, 'seq_len': 42, 'token_d_model': 8, 'time_d_model': 160, 'pos_d_model': 96, 'd_model': 832, 'conv_out_dim': 320, 'e_layers': 6, 'learning_rate': 0.00031514731277786556, 'dropout': 0.24000000000000002, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 17:23:43,715] The parameter 'norm_type' in trial#43 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=8, input_dim=84, dec_in=84, output_dim=1, d_model=896, n_heads=4, e_layers=10, hidden_d_model=640, seq_layers=2, last_d_model=384, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=176, pos_d_model=160, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.18, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=8, fc_layer_type='mha', conv_out_dim=256), exp_settings='seq_len-42-lr-0.0006-d-896-hid_d-640-last_d-384-time_d-176-e_layers-10-tok_conv_k-8-dropout-0.18-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0006368146134855367, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 17:36:05,429] Trial 43 finished with value: 0.5803318072110415 and parameters: {'hidden_d_model': 640, 'token_conv_kernel': 8, 'last_d_model': 384, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 176, 'pos_d_model': 160, 'd_model': 896, 'conv_out_dim': 256, 'e_layers': 10, 'learning_rate': 0.0006368146134855367, 'dropout': 0.18, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 17:36:05,697] The parameter 'norm_type' in trial#46 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 46.6 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
46.6 M    Trainable params
0         Non-trainable params
46.6 M    Total params
186.246   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.238
Metric Loss/val improved by 0.077 >= min_delta = 0.0. New best score: 1.161
Metric Loss/val improved by 0.019 >= min_delta = 0.0. New best score: 1.142
Metric Loss/val improved by 0.103 >= min_delta = 0.0. New best score: 1.039
Metric Loss/val improved by 0.347 >= min_delta = 0.0. New best score: 0.692
Metric Loss/val improved by 0.010 >= min_delta = 0.0. New best score: 0.683
Monitored metric Loss/val did not improve in the last 15 records. Best score: 0.683. Signaling Trainer to stop.
                                                                                            Best trial: 26. Best value: 0.544069:  22%|██▏       | 22/100 [3:16:34<12:41:54, 586.09s/it]Best trial: 26. Best value: 0.544069:  22%|██▏       | 22/100 [3:16:34<12:41:54, 586.09s/it]Best trial: 26. Best value: 0.544069:  23%|██▎       | 23/100 [3:16:34<12:09:37, 568.54s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                            Best trial: 26. Best value: 0.544069:  23%|██▎       | 23/100 [3:16:34<12:09:37, 568.54s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 66.6 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
66.6 M    Trainable params
0         Non-trainable params
66.6 M    Total params
266.422   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.248
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.247
Metric Loss/val improved by 0.269 >= min_delta = 0.0. New best score: 0.978
Metric Loss/val improved by 0.251 >= min_delta = 0.0. New best score: 0.727
Metric Loss/val improved by 0.010 >= min_delta = 0.0. New best score: 0.717
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                            Best trial: 26. Best value: 0.544069:  23%|██▎       | 23/100 [3:28:47<12:09:37, 568.54s/it]Best trial: 26. Best value: 0.544069:  23%|██▎       | 23/100 [3:28:47<12:09:37, 568.54s/it]Best trial: 26. Best value: 0.544069:  24%|██▍       | 24/100 [3:28:47<13:02:32, 617.80s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                            Best trial: 26. Best value: 0.544069:  24%|██▍       | 24/100 [3:28:47<13:02:32, 617.80s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=8, input_dim=84, dec_in=84, output_dim=1, d_model=800, n_heads=4, e_layers=8, hidden_d_model=528, seq_layers=2, last_d_model=480, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=128, pos_d_model=128, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.22, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=8, fc_layer_type='mha', conv_out_dim=320), exp_settings='seq_len-42-lr-0.0009-d-800-hid_d-528-last_d-480-time_d-128-e_layers-8-tok_conv_k-8-dropout-0.22-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.000895754156406786, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 17:44:53,025] Trial 46 finished with value: 0.6844477621838451 and parameters: {'hidden_d_model': 528, 'token_conv_kernel': 8, 'last_d_model': 480, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 128, 'pos_d_model': 128, 'd_model': 800, 'conv_out_dim': 320, 'e_layers': 8, 'learning_rate': 0.000895754156406786, 'dropout': 0.22, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 17:44:53,318] The parameter 'norm_type' in trial#47 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=8, input_dim=84, dec_in=84, output_dim=1, d_model=864, n_heads=4, e_layers=10, hidden_d_model=752, seq_layers=2, last_d_model=448, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=128, pos_d_model=160, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.22, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=8, fc_layer_type='mha', conv_out_dim=384), exp_settings='seq_len-42-lr-0.0007-d-864-hid_d-752-last_d-448-time_d-128-e_layers-10-tok_conv_k-8-dropout-0.22-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0006791027372642273, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 17:57:05,751] Trial 47 finished with value: 0.6005308900028468 and parameters: {'hidden_d_model': 752, 'token_conv_kernel': 8, 'last_d_model': 448, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 128, 'pos_d_model': 160, 'd_model': 864, 'conv_out_dim': 384, 'e_layers': 10, 'learning_rate': 0.0006791027372642273, 'dropout': 0.22, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 17:57:06,037] The parameter 'norm_type' in trial#50 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 52.4 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
52.4 M    Trainable params
0         Non-trainable params
52.4 M    Total params
209.492   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.219
Metric Loss/val improved by 0.015 >= min_delta = 0.0. New best score: 1.204
Metric Loss/val improved by 0.124 >= min_delta = 0.0. New best score: 1.079
Metric Loss/val improved by 0.055 >= min_delta = 0.0. New best score: 1.025
Metric Loss/val improved by 0.162 >= min_delta = 0.0. New best score: 0.863
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                            Best trial: 26. Best value: 0.544069:  24%|██▍       | 24/100 [3:38:06<13:02:32, 617.80s/it]Best trial: 26. Best value: 0.544069:  24%|██▍       | 24/100 [3:38:06<13:02:32, 617.80s/it]Best trial: 26. Best value: 0.544069:  25%|██▌       | 25/100 [3:38:06<12:30:22, 600.30s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                            Best trial: 26. Best value: 0.544069:  25%|██▌       | 25/100 [3:38:06<12:30:22, 600.30s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 38.0 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
38.0 M    Trainable params
0         Non-trainable params
38.0 M    Total params
151.963   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.213
Metric Loss/val improved by 0.015 >= min_delta = 0.0. New best score: 1.198
Metric Loss/val improved by 0.215 >= min_delta = 0.0. New best score: 0.983
Metric Loss/val improved by 0.053 >= min_delta = 0.0. New best score: 0.930
Metric Loss/val improved by 0.080 >= min_delta = 0.0. New best score: 0.849
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 0.849
Metric Loss/val improved by 0.010 >= min_delta = 0.0. New best score: 0.839
Metric Loss/val improved by 0.009 >= min_delta = 0.0. New best score: 0.830
Metric Loss/val improved by 0.019 >= min_delta = 0.0. New best score: 0.811
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 0.811
Metric Loss/val improved by 0.009 >= min_delta = 0.0. New best score: 0.802
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                            Best trial: 26. Best value: 0.544069:  25%|██▌       | 25/100 [3:47:01<12:30:22, 600.30s/it]Best trial: 26. Best value: 0.544069:  25%|██▌       | 25/100 [3:47:01<12:30:22, 600.30s/it]Best trial: 26. Best value: 0.544069:  26%|██▌       | 26/100 [3:47:01<11:56:01, 580.56s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                            Best trial: 26. Best value: 0.544069:  26%|██▌       | 26/100 [3:47:01<11:56:01, 580.56s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=8, input_dim=84, dec_in=84, output_dim=1, d_model=864, n_heads=4, e_layers=8, hidden_d_model=720, seq_layers=2, last_d_model=352, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=112, pos_d_model=144, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.22, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=8, fc_layer_type='mha', conv_out_dim=288), exp_settings='seq_len-42-lr-0.0004-d-864-hid_d-720-last_d-352-time_d-112-e_layers-8-tok_conv_k-8-dropout-0.22-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.00042594263474580085, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 18:06:25,224] Trial 50 finished with value: 0.727503164112568 and parameters: {'hidden_d_model': 720, 'token_conv_kernel': 8, 'last_d_model': 352, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 112, 'pos_d_model': 144, 'd_model': 864, 'conv_out_dim': 288, 'e_layers': 8, 'learning_rate': 0.00042594263474580085, 'dropout': 0.22, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 18:06:25,493] The parameter 'norm_type' in trial#53 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=7, input_dim=84, dec_in=84, output_dim=1, d_model=736, n_heads=4, e_layers=8, hidden_d_model=752, seq_layers=2, last_d_model=480, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=176, pos_d_model=128, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.24000000000000002, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=8, fc_layer_type='mha', conv_out_dim=224), exp_settings='seq_len-42-lr-0.0004-d-736-hid_d-752-last_d-480-time_d-176-e_layers-8-tok_conv_k-7-dropout-0.24000000000000002-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0003898337054932511, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 18:15:19,744] Trial 53 finished with value: 0.5771038338541985 and parameters: {'hidden_d_model': 752, 'token_conv_kernel': 7, 'last_d_model': 480, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 176, 'pos_d_model': 128, 'd_model': 736, 'conv_out_dim': 224, 'e_layers': 8, 'learning_rate': 0.0003898337054932511, 'dropout': 0.24000000000000002, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 18:15:20,037] The parameter 'norm_type' in trial#55 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 77.9 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
77.9 M    Trainable params
0         Non-trainable params
77.9 M    Total params
311.746   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.240
Metric Loss/val improved by 0.003 >= min_delta = 0.0. New best score: 1.236
Metric Loss/val improved by 0.102 >= min_delta = 0.0. New best score: 1.135
Metric Loss/val improved by 0.170 >= min_delta = 0.0. New best score: 0.965
Metric Loss/val improved by 0.172 >= min_delta = 0.0. New best score: 0.793
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 0.792
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                            Best trial: 26. Best value: 0.544069:  26%|██▌       | 26/100 [4:01:04<11:56:01, 580.56s/it]Best trial: 26. Best value: 0.544069:  26%|██▌       | 26/100 [4:01:04<11:56:01, 580.56s/it]Best trial: 26. Best value: 0.544069:  27%|██▋       | 27/100 [4:01:04<13:22:10, 659.33s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                            Best trial: 26. Best value: 0.544069:  27%|██▋       | 27/100 [4:01:04<13:22:10, 659.33s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 31.7 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
31.7 M    Trainable params
0         Non-trainable params
31.7 M    Total params
126.856   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.220
Metric Loss/val improved by 0.034 >= min_delta = 0.0. New best score: 1.186
Metric Loss/val improved by 0.098 >= min_delta = 0.0. New best score: 1.088
Metric Loss/val improved by 0.014 >= min_delta = 0.0. New best score: 1.074
Metric Loss/val improved by 0.116 >= min_delta = 0.0. New best score: 0.958
Metric Loss/val improved by 0.098 >= min_delta = 0.0. New best score: 0.860
Metric Loss/val improved by 0.010 >= min_delta = 0.0. New best score: 0.850
Monitored metric Loss/val did not improve in the last 15 records. Best score: 0.850. Signaling Trainer to stop.
                                                                                            Best trial: 26. Best value: 0.544069:  27%|██▋       | 27/100 [4:07:29<13:22:10, 659.33s/it]Best trial: 26. Best value: 0.544069:  27%|██▋       | 27/100 [4:07:29<13:22:10, 659.33s/it]Best trial: 26. Best value: 0.544069:  28%|██▊       | 28/100 [4:07:29<11:32:24, 577.00s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                            Best trial: 26. Best value: 0.544069:  28%|██▊       | 28/100 [4:07:29<11:32:24, 577.00s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=8, input_dim=84, dec_in=84, output_dim=1, d_model=960, n_heads=4, e_layers=10, hidden_d_model=656, seq_layers=2, last_d_model=288, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=176, pos_d_model=80, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.26, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=8, fc_layer_type='mha', conv_out_dim=288), exp_settings='seq_len-42-lr-0.0004-d-960-hid_d-656-last_d-288-time_d-176-e_layers-10-tok_conv_k-8-dropout-0.26-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.00035185152436555383, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 18:29:22,837] Trial 55 finished with value: 0.6041496098041534 and parameters: {'hidden_d_model': 656, 'token_conv_kernel': 8, 'last_d_model': 288, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 176, 'pos_d_model': 80, 'd_model': 960, 'conv_out_dim': 288, 'e_layers': 10, 'learning_rate': 0.00035185152436555383, 'dropout': 0.26, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 18:29:23,098] The parameter 'norm_type' in trial#57 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=736, n_heads=4, e_layers=6, hidden_d_model=704, seq_layers=2, last_d_model=512, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=128, pos_d_model=112, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.24000000000000002, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=8, fc_layer_type='mha', conv_out_dim=288), exp_settings='seq_len-42-lr-0.0004-d-736-hid_d-704-last_d-512-time_d-128-e_layers-6-tok_conv_k-9-dropout-0.24000000000000002-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.00037682335307203974, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 18:35:47,758] Trial 57 finished with value: 0.6416207715868951 and parameters: {'hidden_d_model': 704, 'token_conv_kernel': 9, 'last_d_model': 512, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 128, 'pos_d_model': 112, 'd_model': 736, 'conv_out_dim': 288, 'e_layers': 6, 'learning_rate': 0.00037682335307203974, 'dropout': 0.24000000000000002, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 18:35:48,056] The parameter 'norm_type' in trial#59 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 47.0 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
47.0 M    Trainable params
0         Non-trainable params
47.0 M    Total params
187.919   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.229
Metric Loss/val improved by 0.120 >= min_delta = 0.0. New best score: 1.109
Metric Loss/val improved by 0.007 >= min_delta = 0.0. New best score: 1.102
Metric Loss/val improved by 0.168 >= min_delta = 0.0. New best score: 0.934
Metric Loss/val improved by 0.013 >= min_delta = 0.0. New best score: 0.921
Monitored metric Loss/val did not improve in the last 15 records. Best score: 0.921. Signaling Trainer to stop.
                                                                                            Best trial: 26. Best value: 0.544069:  28%|██▊       | 28/100 [4:16:46<11:32:24, 577.00s/it]Best trial: 26. Best value: 0.544069:  28%|██▊       | 28/100 [4:16:46<11:32:24, 577.00s/it]Best trial: 26. Best value: 0.544069:  29%|██▉       | 29/100 [4:16:46<11:15:57, 571.23s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                            Best trial: 26. Best value: 0.544069:  29%|██▉       | 29/100 [4:16:47<11:15:57, 571.23s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 44.9 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
44.9 M    Trainable params
0         Non-trainable params
44.9 M    Total params
179.564   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.242
Metric Loss/val improved by 0.012 >= min_delta = 0.0. New best score: 1.229
Metric Loss/val improved by 0.065 >= min_delta = 0.0. New best score: 1.165
Metric Loss/val improved by 0.274 >= min_delta = 0.0. New best score: 0.890
Metric Loss/val improved by 0.018 >= min_delta = 0.0. New best score: 0.873
Metric Loss/val improved by 0.107 >= min_delta = 0.0. New best score: 0.766
Metric Loss/val improved by 0.010 >= min_delta = 0.0. New best score: 0.756
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                            Best trial: 26. Best value: 0.544069:  29%|██▉       | 29/100 [4:26:21<11:15:57, 571.23s/it]Best trial: 26. Best value: 0.544069:  29%|██▉       | 29/100 [4:26:21<11:15:57, 571.23s/it]Best trial: 26. Best value: 0.544069:  30%|███       | 30/100 [4:26:21<11:07:30, 572.15s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                            Best trial: 26. Best value: 0.544069:  30%|███       | 30/100 [4:26:21<11:07:30, 572.15s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=704, n_heads=4, e_layers=10, hidden_d_model=752, seq_layers=2, last_d_model=320, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=96, pos_d_model=192, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.24000000000000002, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=8, fc_layer_type='mha', conv_out_dim=352), exp_settings='seq_len-42-lr-0.0008-d-704-hid_d-752-last_d-320-time_d-96-e_layers-10-tok_conv_k-9-dropout-0.24000000000000002-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0008108429752002771, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 18:45:05,535] Trial 59 finished with value: 0.655282425507903 and parameters: {'hidden_d_model': 752, 'token_conv_kernel': 9, 'last_d_model': 320, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 96, 'pos_d_model': 192, 'd_model': 704, 'conv_out_dim': 352, 'e_layers': 10, 'learning_rate': 0.0008108429752002771, 'dropout': 0.24000000000000002, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 18:45:05,846] The parameter 'norm_type' in trial#62 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=832, n_heads=4, e_layers=8, hidden_d_model=672, seq_layers=2, last_d_model=320, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=8, time_d_model=224, pos_d_model=144, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.24000000000000002, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=8, fc_layer_type='mha', conv_out_dim=160), exp_settings='seq_len-42-lr-0.0003-d-832-hid_d-672-last_d-320-time_d-224-e_layers-8-tok_conv_k-10-dropout-0.24000000000000002-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.00032723521359248137, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 18:54:39,811] Trial 62 finished with value: 0.6339743748307228 and parameters: {'hidden_d_model': 672, 'token_conv_kernel': 10, 'last_d_model': 320, 'seq_len': 42, 'token_d_model': 8, 'time_d_model': 224, 'pos_d_model': 144, 'd_model': 832, 'conv_out_dim': 160, 'e_layers': 8, 'learning_rate': 0.00032723521359248137, 'dropout': 0.24000000000000002, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 18:54:40,051] The parameter 'norm_type' in trial#64 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 41.5 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
41.5 M    Trainable params
0         Non-trainable params
41.5 M    Total params
166.061   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.254
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 1.252
Metric Loss/val improved by 0.097 >= min_delta = 0.0. New best score: 1.155
Metric Loss/val improved by 0.344 >= min_delta = 0.0. New best score: 0.811
Metric Loss/val improved by 0.011 >= min_delta = 0.0. New best score: 0.801
Metric Loss/val improved by 0.012 >= min_delta = 0.0. New best score: 0.788
Metric Loss/val improved by 0.040 >= min_delta = 0.0. New best score: 0.749
Metric Loss/val improved by 0.071 >= min_delta = 0.0. New best score: 0.678
Monitored metric Loss/val did not improve in the last 15 records. Best score: 0.678. Signaling Trainer to stop.
                                                                                            Best trial: 26. Best value: 0.544069:  30%|███       | 30/100 [4:35:07<11:07:30, 572.15s/it]Best trial: 26. Best value: 0.544069:  30%|███       | 30/100 [4:35:07<11:07:30, 572.15s/it]Best trial: 26. Best value: 0.544069:  31%|███       | 31/100 [4:35:07<10:42:16, 558.50s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                            Best trial: 26. Best value: 0.544069:  31%|███       | 31/100 [4:35:08<10:42:16, 558.50s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 39.7 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
39.7 M    Trainable params
0         Non-trainable params
39.7 M    Total params
158.673   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.239
Metric Loss/val improved by 0.015 >= min_delta = 0.0. New best score: 1.225
Metric Loss/val improved by 0.131 >= min_delta = 0.0. New best score: 1.093
Metric Loss/val improved by 0.165 >= min_delta = 0.0. New best score: 0.929
Metric Loss/val improved by 0.168 >= min_delta = 0.0. New best score: 0.761
Monitored metric Loss/val did not improve in the last 15 records. Best score: 0.761. Signaling Trainer to stop.
                                                                                            Best trial: 26. Best value: 0.544069:  31%|███       | 31/100 [4:43:02<10:42:16, 558.50s/it]Best trial: 26. Best value: 0.544069:  31%|███       | 31/100 [4:43:02<10:42:16, 558.50s/it]Best trial: 26. Best value: 0.544069:  32%|███▏      | 32/100 [4:43:02<10:04:20, 533.25s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                            Best trial: 26. Best value: 0.544069:  32%|███▏      | 32/100 [4:43:02<10:04:20, 533.25s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=8, input_dim=84, dec_in=84, output_dim=1, d_model=768, n_heads=4, e_layers=8, hidden_d_model=768, seq_layers=2, last_d_model=480, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=128, pos_d_model=192, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.22, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=8, fc_layer_type='mha', conv_out_dim=224), exp_settings='seq_len-42-lr-0.0003-d-768-hid_d-768-last_d-480-time_d-128-e_layers-8-tok_conv_k-8-dropout-0.22-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.00028307550862826834, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 19:03:26,473] Trial 64 finished with value: 0.6642446085810662 and parameters: {'hidden_d_model': 768, 'token_conv_kernel': 8, 'last_d_model': 480, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 128, 'pos_d_model': 192, 'd_model': 768, 'conv_out_dim': 224, 'e_layers': 8, 'learning_rate': 0.00028307550862826834, 'dropout': 0.22, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 19:03:26,734] The parameter 'norm_type' in trial#66 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=864, n_heads=4, e_layers=6, hidden_d_model=704, seq_layers=2, last_d_model=352, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=192, pos_d_model=128, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.24000000000000002, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=8, fc_layer_type='mha', conv_out_dim=224), exp_settings='seq_len-42-lr-0.0004-d-864-hid_d-704-last_d-352-time_d-192-e_layers-6-tok_conv_k-9-dropout-0.24000000000000002-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.00035855647800787444, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 19:11:20,791] Trial 66 finished with value: 0.6434589993208647 and parameters: {'hidden_d_model': 704, 'token_conv_kernel': 9, 'last_d_model': 352, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 192, 'pos_d_model': 128, 'd_model': 864, 'conv_out_dim': 224, 'e_layers': 6, 'learning_rate': 0.00035855647800787444, 'dropout': 0.24000000000000002, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 19:11:21,114] The parameter 'norm_type' in trial#67 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 49.2 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
49.2 M    Trainable params
0         Non-trainable params
49.2 M    Total params
196.967   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.223
Metric Loss/val improved by 0.012 >= min_delta = 0.0. New best score: 1.211
Metric Loss/val improved by 0.126 >= min_delta = 0.0. New best score: 1.086
Metric Loss/val improved by 0.104 >= min_delta = 0.0. New best score: 0.982
Metric Loss/val improved by 0.024 >= min_delta = 0.0. New best score: 0.958
Metric Loss/val improved by 0.021 >= min_delta = 0.0. New best score: 0.937
Metric Loss/val improved by 0.028 >= min_delta = 0.0. New best score: 0.909
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                            Best trial: 26. Best value: 0.544069:  32%|███▏      | 32/100 [4:52:51<10:04:20, 533.25s/it]Best trial: 26. Best value: 0.544069:  32%|███▏      | 32/100 [4:52:51<10:04:20, 533.25s/it]Best trial: 26. Best value: 0.544069:  33%|███▎      | 33/100 [4:52:51<10:14:19, 550.14s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                            Best trial: 26. Best value: 0.544069:  33%|███▎      | 33/100 [4:52:52<10:14:19, 550.14s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 27.5 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
27.5 M    Trainable params
0         Non-trainable params
27.5 M    Total params
110.074   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.236
Metric Loss/val improved by 0.019 >= min_delta = 0.0. New best score: 1.217
Metric Loss/val improved by 0.166 >= min_delta = 0.0. New best score: 1.051
Metric Loss/val improved by 0.176 >= min_delta = 0.0. New best score: 0.875
Metric Loss/val improved by 0.070 >= min_delta = 0.0. New best score: 0.805
Monitored metric Loss/val did not improve in the last 15 records. Best score: 0.805. Signaling Trainer to stop.
                                                                                            Best trial: 26. Best value: 0.544069:  33%|███▎      | 33/100 [4:57:25<10:14:19, 550.14s/it]Best trial: 26. Best value: 0.544069:  33%|███▎      | 33/100 [4:57:25<10:14:19, 550.14s/it]Best trial: 26. Best value: 0.544069:  34%|███▍      | 34/100 [4:57:25<8:33:49, 467.11s/it] /home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                           Best trial: 26. Best value: 0.544069:  34%|███▍      | 34/100 [4:57:25<8:33:49, 467.11s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=8, input_dim=84, dec_in=84, output_dim=1, d_model=832, n_heads=4, e_layers=8, hidden_d_model=800, seq_layers=2, last_d_model=576, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=160, pos_d_model=112, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.26, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=8, fc_layer_type='mha', conv_out_dim=288), exp_settings='seq_len-42-lr-0.0005-d-832-hid_d-800-last_d-576-time_d-160-e_layers-8-tok_conv_k-8-dropout-0.26-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.00047525187655346197, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 19:21:10,369] Trial 67 finished with value: 0.6559543257579208 and parameters: {'hidden_d_model': 800, 'token_conv_kernel': 8, 'last_d_model': 576, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 160, 'pos_d_model': 112, 'd_model': 832, 'conv_out_dim': 288, 'e_layers': 8, 'learning_rate': 0.00047525187655346197, 'dropout': 0.26, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 19:21:10,626] The parameter 'norm_type' in trial#69 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=768, n_heads=4, e_layers=4, hidden_d_model=736, seq_layers=2, last_d_model=576, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=144, pos_d_model=96, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.26, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=8, fc_layer_type='mha', conv_out_dim=416), exp_settings='seq_len-42-lr-0.0002-d-768-hid_d-736-last_d-576-time_d-144-e_layers-4-tok_conv_k-9-dropout-0.26-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0002361086910681007, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 19:25:43,715] Trial 69 finished with value: 0.8881135560572148 and parameters: {'hidden_d_model': 736, 'token_conv_kernel': 9, 'last_d_model': 576, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 144, 'pos_d_model': 96, 'd_model': 768, 'conv_out_dim': 416, 'e_layers': 4, 'learning_rate': 0.0002361086910681007, 'dropout': 0.26, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 19:25:43,982] The parameter 'norm_type' in trial#71 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 60.8 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
60.8 M    Trainable params
0         Non-trainable params
60.8 M    Total params
243.312   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.253
Metric Loss/val improved by 0.006 >= min_delta = 0.0. New best score: 1.247
Metric Loss/val improved by 0.104 >= min_delta = 0.0. New best score: 1.143
Metric Loss/val improved by 0.012 >= min_delta = 0.0. New best score: 1.131
Metric Loss/val improved by 0.077 >= min_delta = 0.0. New best score: 1.054
Metric Loss/val improved by 0.079 >= min_delta = 0.0. New best score: 0.974
Metric Loss/val improved by 0.038 >= min_delta = 0.0. New best score: 0.937
Monitored metric Loss/val did not improve in the last 15 records. Best score: 0.937. Signaling Trainer to stop.
                                                                                           Best trial: 26. Best value: 0.544069:  34%|███▍      | 34/100 [5:08:22<8:33:49, 467.11s/it]Best trial: 26. Best value: 0.544069:  34%|███▍      | 34/100 [5:08:22<8:33:49, 467.11s/it]Best trial: 26. Best value: 0.544069:  35%|███▌      | 35/100 [5:08:22<9:27:51, 524.17s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                           Best trial: 26. Best value: 0.544069:  35%|███▌      | 35/100 [5:08:22<9:27:51, 524.17s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 56.0 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
56.0 M    Trainable params
0         Non-trainable params
56.0 M    Total params
223.917   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.245
Metric Loss/val improved by 0.005 >= min_delta = 0.0. New best score: 1.240
Metric Loss/val improved by 0.086 >= min_delta = 0.0. New best score: 1.154
Metric Loss/val improved by 0.118 >= min_delta = 0.0. New best score: 1.036
Metric Loss/val improved by 0.095 >= min_delta = 0.0. New best score: 0.941
Metric Loss/val improved by 0.060 >= min_delta = 0.0. New best score: 0.881
Metric Loss/val improved by 0.076 >= min_delta = 0.0. New best score: 0.804
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                           Best trial: 26. Best value: 0.544069:  35%|███▌      | 35/100 [5:19:51<9:27:51, 524.17s/it]Best trial: 26. Best value: 0.544069:  35%|███▌      | 35/100 [5:19:51<9:27:51, 524.17s/it]Best trial: 26. Best value: 0.544069:  36%|███▌      | 36/100 [5:19:51<10:11:48, 573.57s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                            Best trial: 26. Best value: 0.544069:  36%|███▌      | 36/100 [5:19:51<10:11:48, 573.57s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=832, n_heads=4, e_layers=10, hidden_d_model=640, seq_layers=2, last_d_model=416, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=160, pos_d_model=96, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.26, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=8, fc_layer_type='mha', conv_out_dim=288), exp_settings='seq_len-42-lr-0.0004-d-832-hid_d-640-last_d-416-time_d-160-e_layers-10-tok_conv_k-9-dropout-0.26-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.00042144369762211857, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 19:36:41,040] Trial 71 finished with value: 0.7539427362382412 and parameters: {'hidden_d_model': 640, 'token_conv_kernel': 9, 'last_d_model': 416, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 160, 'pos_d_model': 96, 'd_model': 832, 'conv_out_dim': 288, 'e_layers': 10, 'learning_rate': 0.00042144369762211857, 'dropout': 0.26, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 19:36:41,283] The parameter 'norm_type' in trial#73 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=7, input_dim=84, dec_in=84, output_dim=1, d_model=800, n_heads=4, e_layers=10, hidden_d_model=640, seq_layers=2, last_d_model=192, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=144, pos_d_model=160, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.22, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=8, fc_layer_type='mha', conv_out_dim=320), exp_settings='seq_len-42-lr-0.0005-d-800-hid_d-640-last_d-192-time_d-144-e_layers-10-tok_conv_k-7-dropout-0.22-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0005079584022202487, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 19:48:09,865] Trial 73 finished with value: 0.6231919229030609 and parameters: {'hidden_d_model': 640, 'token_conv_kernel': 7, 'last_d_model': 192, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 144, 'pos_d_model': 160, 'd_model': 800, 'conv_out_dim': 320, 'e_layers': 10, 'learning_rate': 0.0005079584022202487, 'dropout': 0.22, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 19:48:10,575] The parameter 'norm_type' in trial#75 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 53.1 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
53.1 M    Trainable params
0         Non-trainable params
53.1 M    Total params
212.207   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.237
Metric Loss/val improved by 0.010 >= min_delta = 0.0. New best score: 1.228
Metric Loss/val improved by 0.064 >= min_delta = 0.0. New best score: 1.163
Metric Loss/val improved by 0.193 >= min_delta = 0.0. New best score: 0.970
Metric Loss/val improved by 0.062 >= min_delta = 0.0. New best score: 0.908
Metric Loss/val improved by 0.010 >= min_delta = 0.0. New best score: 0.898
Metric Loss/val improved by 0.017 >= min_delta = 0.0. New best score: 0.880
Metric Loss/val improved by 0.009 >= min_delta = 0.0. New best score: 0.872
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                            Best trial: 26. Best value: 0.544069:  36%|███▌      | 36/100 [5:30:10<10:11:48, 573.57s/it]Best trial: 26. Best value: 0.544069:  36%|███▌      | 36/100 [5:30:10<10:11:48, 573.57s/it]Best trial: 26. Best value: 0.544069:  37%|███▋      | 37/100 [5:30:10<10:16:33, 587.20s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                            Best trial: 26. Best value: 0.544069:  37%|███▋      | 37/100 [5:30:10<10:16:33, 587.20s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 63.9 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
63.9 M    Trainable params
0         Non-trainable params
63.9 M    Total params
255.500   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.250
Metric Loss/val improved by 0.007 >= min_delta = 0.0. New best score: 1.243
Metric Loss/val improved by 0.041 >= min_delta = 0.0. New best score: 1.203
Metric Loss/val improved by 0.092 >= min_delta = 0.0. New best score: 1.111
Metric Loss/val improved by 0.122 >= min_delta = 0.0. New best score: 0.989
Metric Loss/val improved by 0.044 >= min_delta = 0.0. New best score: 0.945
Metric Loss/val improved by 0.010 >= min_delta = 0.0. New best score: 0.935
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 0.935
Metric Loss/val improved by 0.062 >= min_delta = 0.0. New best score: 0.873
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                            Best trial: 26. Best value: 0.544069:  37%|███▋      | 37/100 [5:41:48<10:16:33, 587.20s/it]Best trial: 26. Best value: 0.544069:  37%|███▋      | 37/100 [5:41:48<10:16:33, 587.20s/it]Best trial: 26. Best value: 0.544069:  38%|███▊      | 38/100 [5:41:48<10:41:18, 620.62s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                            Best trial: 26. Best value: 0.544069:  38%|███▊      | 38/100 [5:41:49<10:41:18, 620.62s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=864, n_heads=4, e_layers=8, hidden_d_model=784, seq_layers=2, last_d_model=320, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=176, pos_d_model=96, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.26, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=8, fc_layer_type='mha', conv_out_dim=288), exp_settings='seq_len-42-lr-0.0002-d-864-hid_d-784-last_d-320-time_d-176-e_layers-8-tok_conv_k-9-dropout-0.26-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.00021005511876178034, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 19:58:28,882] Trial 75 finished with value: 0.6393174462020397 and parameters: {'hidden_d_model': 784, 'token_conv_kernel': 9, 'last_d_model': 320, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 176, 'pos_d_model': 96, 'd_model': 864, 'conv_out_dim': 288, 'e_layers': 8, 'learning_rate': 0.00021005511876178034, 'dropout': 0.26, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 19:58:29,167] The parameter 'norm_type' in trial#78 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=864, n_heads=4, e_layers=10, hidden_d_model=704, seq_layers=2, last_d_model=224, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=160, pos_d_model=96, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.28, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=8, fc_layer_type='mha', conv_out_dim=224), exp_settings='seq_len-42-lr-0.0002-d-864-hid_d-704-last_d-224-time_d-160-e_layers-10-tok_conv_k-10-dropout-0.28-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0002369449285603119, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 20:10:07,492] Trial 78 finished with value: 0.6900989927351475 and parameters: {'hidden_d_model': 704, 'token_conv_kernel': 10, 'last_d_model': 224, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 160, 'pos_d_model': 96, 'd_model': 864, 'conv_out_dim': 224, 'e_layers': 10, 'learning_rate': 0.0002369449285603119, 'dropout': 0.28, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 20:10:07,753] The parameter 'norm_type' in trial#79 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 46.3 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
46.3 M    Trainable params
0         Non-trainable params
46.3 M    Total params
185.387   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.219
Metric Loss/val improved by 0.088 >= min_delta = 0.0. New best score: 1.131
Metric Loss/val improved by 0.217 >= min_delta = 0.0. New best score: 0.914
Metric Loss/val improved by 0.097 >= min_delta = 0.0. New best score: 0.817
Metric Loss/val improved by 0.106 >= min_delta = 0.0. New best score: 0.711
Monitored metric Loss/val did not improve in the last 15 records. Best score: 0.711. Signaling Trainer to stop.
                                                                                            Best trial: 26. Best value: 0.544069:  38%|███▊      | 38/100 [5:51:24<10:41:18, 620.62s/it]Best trial: 26. Best value: 0.544069:  38%|███▊      | 38/100 [5:51:24<10:41:18, 620.62s/it]Best trial: 26. Best value: 0.544069:  39%|███▉      | 39/100 [5:51:24<10:17:12, 607.10s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                            Best trial: 26. Best value: 0.544069:  39%|███▉      | 39/100 [5:51:24<10:17:12, 607.10s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 47.3 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
47.3 M    Trainable params
0         Non-trainable params
47.3 M    Total params
189.021   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.257
Metric Loss/val improved by 0.003 >= min_delta = 0.0. New best score: 1.254
Metric Loss/val improved by 0.016 >= min_delta = 0.0. New best score: 1.239
Metric Loss/val improved by 0.136 >= min_delta = 0.0. New best score: 1.102
Metric Loss/val improved by 0.081 >= min_delta = 0.0. New best score: 1.022
Metric Loss/val improved by 0.203 >= min_delta = 0.0. New best score: 0.819
Metric Loss/val improved by 0.016 >= min_delta = 0.0. New best score: 0.803
Metric Loss/val improved by 0.026 >= min_delta = 0.0. New best score: 0.777
Metric Loss/val improved by 0.018 >= min_delta = 0.0. New best score: 0.759
Metric Loss/val improved by 0.018 >= min_delta = 0.0. New best score: 0.741
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                            Best trial: 26. Best value: 0.544069:  39%|███▉      | 39/100 [6:01:07<10:17:12, 607.10s/it]Best trial: 26. Best value: 0.544069:  39%|███▉      | 39/100 [6:01:07<10:17:12, 607.10s/it]Best trial: 26. Best value: 0.544069:  40%|████      | 40/100 [6:01:07<9:59:49, 599.83s/it] /home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                           Best trial: 26. Best value: 0.544069:  40%|████      | 40/100 [6:01:07<9:59:49, 599.83s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=736, n_heads=4, e_layers=10, hidden_d_model=752, seq_layers=2, last_d_model=320, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=176, pos_d_model=64, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.2, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=8, fc_layer_type='mha', conv_out_dim=160), exp_settings='seq_len-42-lr-0.0004-d-736-hid_d-752-last_d-320-time_d-176-e_layers-10-tok_conv_k-10-dropout-0.2-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.00040180844198087326, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 20:19:43,023] Trial 79 finished with value: 0.5973639249801637 and parameters: {'hidden_d_model': 752, 'token_conv_kernel': 10, 'last_d_model': 320, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 176, 'pos_d_model': 64, 'd_model': 736, 'conv_out_dim': 160, 'e_layers': 10, 'learning_rate': 0.00040180844198087326, 'dropout': 0.2, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 20:19:43,318] The parameter 'norm_type' in trial#82 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=800, n_heads=4, e_layers=8, hidden_d_model=576, seq_layers=2, last_d_model=256, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=176, pos_d_model=128, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.3, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=8, fc_layer_type='mha', conv_out_dim=320), exp_settings='seq_len-42-lr-0.0003-d-800-hid_d-576-last_d-256-time_d-176-e_layers-8-tok_conv_k-9-dropout-0.3-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0002772476145781764, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 20:29:25,886] Trial 82 finished with value: 0.6192312348634005 and parameters: {'hidden_d_model': 576, 'token_conv_kernel': 9, 'last_d_model': 256, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 176, 'pos_d_model': 128, 'd_model': 800, 'conv_out_dim': 320, 'e_layers': 8, 'learning_rate': 0.0002772476145781764, 'dropout': 0.3, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 20:29:26,221] The parameter 'norm_type' in trial#84 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 29.4 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
29.4 M    Trainable params
0         Non-trainable params
29.4 M    Total params
117.506   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.224
Metric Loss/val improved by 0.008 >= min_delta = 0.0. New best score: 1.217
Metric Loss/val improved by 0.104 >= min_delta = 0.0. New best score: 1.113
Metric Loss/val improved by 0.078 >= min_delta = 0.0. New best score: 1.034
Metric Loss/val improved by 0.242 >= min_delta = 0.0. New best score: 0.792
Monitored metric Loss/val did not improve in the last 15 records. Best score: 0.792. Signaling Trainer to stop.
                                                                                           Best trial: 26. Best value: 0.544069:  40%|████      | 40/100 [6:06:20<9:59:49, 599.83s/it]Best trial: 26. Best value: 0.544069:  40%|████      | 40/100 [6:06:20<9:59:49, 599.83s/it]Best trial: 26. Best value: 0.544069:  41%|████      | 41/100 [6:06:20<8:25:19, 513.89s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                           Best trial: 26. Best value: 0.544069:  41%|████      | 41/100 [6:06:21<8:25:19, 513.89s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 58.3 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
58.3 M    Trainable params
0         Non-trainable params
58.3 M    Total params
233.268   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.235
Metric Loss/val improved by 0.004 >= min_delta = 0.0. New best score: 1.232
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 1.229
Metric Loss/val improved by 0.266 >= min_delta = 0.0. New best score: 0.963
Metric Loss/val improved by 0.045 >= min_delta = 0.0. New best score: 0.917
Metric Loss/val improved by 0.055 >= min_delta = 0.0. New best score: 0.862
Metric Loss/val improved by 0.046 >= min_delta = 0.0. New best score: 0.817
Metric Loss/val improved by 0.030 >= min_delta = 0.0. New best score: 0.787
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                           Best trial: 26. Best value: 0.544069:  41%|████      | 41/100 [6:17:44<8:25:19, 513.89s/it]Best trial: 26. Best value: 0.544069:  41%|████      | 41/100 [6:17:44<8:25:19, 513.89s/it]Best trial: 26. Best value: 0.544069:  42%|████▏     | 42/100 [6:17:44<9:06:04, 564.91s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                           Best trial: 26. Best value: 0.544069:  42%|████▏     | 42/100 [6:17:44<9:06:04, 564.91s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=11, input_dim=84, dec_in=84, output_dim=1, d_model=704, n_heads=4, e_layers=6, hidden_d_model=752, seq_layers=2, last_d_model=480, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=176, pos_d_model=176, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.2, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=8, fc_layer_type='mha', conv_out_dim=224), exp_settings='seq_len-42-lr-0.0002-d-704-hid_d-752-last_d-480-time_d-176-e_layers-6-tok_conv_k-11-dropout-0.2-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.00024062751540367055, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 20:34:39,264] Trial 84 finished with value: 0.6604278467595577 and parameters: {'hidden_d_model': 752, 'token_conv_kernel': 11, 'last_d_model': 480, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 176, 'pos_d_model': 176, 'd_model': 704, 'conv_out_dim': 224, 'e_layers': 6, 'learning_rate': 0.00024062751540367055, 'dropout': 0.2, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 20:34:39,886] The parameter 'norm_type' in trial#85 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=800, n_heads=4, e_layers=10, hidden_d_model=720, seq_layers=2, last_d_model=192, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=192, pos_d_model=144, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.28, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=8, fc_layer_type='mha', conv_out_dim=320), exp_settings='seq_len-42-lr-0.0003-d-800-hid_d-720-last_d-192-time_d-192-e_layers-10-tok_conv_k-10-dropout-0.28-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.00025003954981364966, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 20:46:03,225] Trial 85 finished with value: 0.6165787545964122 and parameters: {'hidden_d_model': 720, 'token_conv_kernel': 10, 'last_d_model': 192, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 192, 'pos_d_model': 144, 'd_model': 800, 'conv_out_dim': 320, 'e_layers': 10, 'learning_rate': 0.00025003954981364966, 'dropout': 0.28, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 20:46:03,514] The parameter 'norm_type' in trial#87 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 77.9 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
77.9 M    Trainable params
0         Non-trainable params
77.9 M    Total params
311.408   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.239
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.238
Metric Loss/val improved by 0.033 >= min_delta = 0.0. New best score: 1.205
Metric Loss/val improved by 0.203 >= min_delta = 0.0. New best score: 1.002
Metric Loss/val improved by 0.076 >= min_delta = 0.0. New best score: 0.926
Metric Loss/val improved by 0.007 >= min_delta = 0.0. New best score: 0.920
Metric Loss/val improved by 0.091 >= min_delta = 0.0. New best score: 0.829
Monitored metric Loss/val did not improve in the last 15 records. Best score: 0.829. Signaling Trainer to stop.
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                           Best trial: 26. Best value: 0.544069:  42%|████▏     | 42/100 [6:31:20<9:06:04, 564.91s/it]Best trial: 26. Best value: 0.544069:  42%|████▏     | 42/100 [6:31:20<9:06:04, 564.91s/it]Best trial: 26. Best value: 0.544069:  43%|████▎     | 43/100 [6:31:20<10:08:05, 640.10s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                            Best trial: 26. Best value: 0.544069:  43%|████▎     | 43/100 [6:31:20<10:08:05, 640.10s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 61.8 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
61.8 M    Trainable params
0         Non-trainable params
61.8 M    Total params
247.221   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.243
Metric Loss/val improved by 0.014 >= min_delta = 0.0. New best score: 1.229
Metric Loss/val improved by 0.119 >= min_delta = 0.0. New best score: 1.110
Metric Loss/val improved by 0.213 >= min_delta = 0.0. New best score: 0.897
Metric Loss/val improved by 0.206 >= min_delta = 0.0. New best score: 0.691
Monitored metric Loss/val did not improve in the last 15 records. Best score: 0.691. Signaling Trainer to stop.
                                                                                            Best trial: 26. Best value: 0.544069:  43%|████▎     | 43/100 [6:41:14<10:08:05, 640.10s/it]Best trial: 26. Best value: 0.544069:  43%|████▎     | 43/100 [6:41:14<10:08:05, 640.10s/it]Best trial: 26. Best value: 0.544069:  44%|████▍     | 44/100 [6:41:14<9:44:29, 626.24s/it] /home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                           Best trial: 26. Best value: 0.544069:  44%|████▍     | 44/100 [6:41:14<9:44:29, 626.24s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=960, n_heads=4, e_layers=10, hidden_d_model=704, seq_layers=2, last_d_model=128, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=224, pos_d_model=112, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.26, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=8, fc_layer_type='mha', conv_out_dim=256), exp_settings='seq_len-42-lr-0.0003-d-960-hid_d-704-last_d-128-time_d-224-e_layers-10-tok_conv_k-9-dropout-0.26-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.00027802251423670904, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 20:59:38,753] Trial 87 finished with value: 0.6792991526424885 and parameters: {'hidden_d_model': 704, 'token_conv_kernel': 9, 'last_d_model': 128, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 224, 'pos_d_model': 112, 'd_model': 960, 'conv_out_dim': 256, 'e_layers': 10, 'learning_rate': 0.00027802251423670904, 'dropout': 0.26, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 20:59:39,002] The parameter 'norm_type' in trial#90 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=928, n_heads=4, e_layers=8, hidden_d_model=784, seq_layers=2, last_d_model=256, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=176, pos_d_model=160, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.26, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=8, fc_layer_type='mha', conv_out_dim=320), exp_settings='seq_len-42-lr-0.0004-d-928-hid_d-784-last_d-256-time_d-176-e_layers-8-tok_conv_k-10-dropout-0.26-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0003951235003357553, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 21:09:32,675] Trial 90 finished with value: 0.6074855037033559 and parameters: {'hidden_d_model': 784, 'token_conv_kernel': 10, 'last_d_model': 256, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 176, 'pos_d_model': 160, 'd_model': 928, 'conv_out_dim': 320, 'e_layers': 8, 'learning_rate': 0.0003951235003357553, 'dropout': 0.26, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 21:09:32,951] The parameter 'norm_type' in trial#92 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 44.4 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
44.4 M    Trainable params
0         Non-trainable params
44.4 M    Total params
177.754   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.243
Metric Loss/val improved by 0.017 >= min_delta = 0.0. New best score: 1.226
Metric Loss/val improved by 0.072 >= min_delta = 0.0. New best score: 1.154
Metric Loss/val improved by 0.168 >= min_delta = 0.0. New best score: 0.986
Metric Loss/val improved by 0.028 >= min_delta = 0.0. New best score: 0.959
Metric Loss/val improved by 0.160 >= min_delta = 0.0. New best score: 0.799
Monitored metric Loss/val did not improve in the last 15 records. Best score: 0.799. Signaling Trainer to stop.
                                                                                           Best trial: 26. Best value: 0.544069:  44%|████▍     | 44/100 [6:49:14<9:44:29, 626.24s/it]Best trial: 26. Best value: 0.544069:  44%|████▍     | 44/100 [6:49:14<9:44:29, 626.24s/it]Best trial: 26. Best value: 0.544069:  45%|████▌     | 45/100 [6:49:14<8:53:50, 582.38s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                           Best trial: 26. Best value: 0.544069:  45%|████▌     | 45/100 [6:49:14<8:53:50, 582.38s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 35.7 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
35.7 M    Trainable params
0         Non-trainable params
35.7 M    Total params
142.848   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.202
