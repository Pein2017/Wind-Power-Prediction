[I 2024-08-13 23:05:39,996] A new study created in RDB with name: 24-08-13-mlp_v3-small_range-farm_89
Creating study "24-08-13-mlp_v3-small_range-farm_89" with storage "sqlite:////data3/lsf/Pein/Power-Prediction/optuna_results/24-08-13-mlp_v3-small_range/24-08-13-mlp_v3-small_range-farm_89.db?mode=wal"...
  0%|          | 0/10 [00:00<?, ?it/s]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA RTX 6000 Ada Generation') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 6.8 M  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
6.8 M     Trainable params
0         Non-trainable params
6.8 M     Total params
27.236    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.033
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.032
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.031
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.031. Signaling Trainer to stop.
                                        0%|          | 0/10 [02:03<?, ?it/s]Best trial: 0. Best value: 2.76647:   0%|          | 0/10 [02:03<?, ?it/s]Best trial: 0. Best value: 2.76647:  10%|█         | 1/10 [02:03<18:31, 123.54s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                   Best trial: 0. Best value: 2.76647:  10%|█         | 1/10 [02:04<18:31, 123.54s/it]                                                                                   Best trial: 0. Best value: 2.76647:  10%|█         | 1/10 [02:04<18:31, 123.54s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 6.8 M  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
6.8 M     Trainable params
0         Non-trainable params
6.8 M     Total params
27.236    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.031
[rank: 0] Received SIGTERM: 15
[rank: 0] Received SIGTERM: 15
[rank: 0] Received SIGTERM: 15
Best trial: 0. Best value: 2.76647:  10%|█         | 1/10 [03:35<32:19, 215.48s/it]
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-small_range', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-small_range', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-small_range.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-small_range', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-small_range'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_89_withTime.csv', test_path='test_farm_89_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=5, input_dim=75, dec_in=75, output_dim=1, d_model=64, n_heads=4, e_layers=2, hidden_d_model=128, seq_layers=2, last_d_model=128, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=32, time_d_model=32, pos_d_model=32, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.1, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=68, fc_layer_type='mha', conv_out_dim=64), exp_settings='seq_len-8-lr-0.0008-d-64-hid_d-128-last_d-128-tok_d-32-time_d-32-pos_d-32-e_layers-2-tok_conv_k-5-dropout-0.1-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-68-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=0.1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.0008102444046886643, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-13 23:07:43,587] Trial 0 finished with value: 2.766465902328491 and parameters: {'num_heads': 68, 'hidden_d_model': 128, 'token_conv_kernel': 5, 'last_d_model': 128, 'seq_len': 8, 'token_d_model': 32, 'time_d_model': 32, 'pos_d_model': 32, 'd_model': 64, 'conv_out_dim': 64, 'e_layers': 2, 'learning_rate': 0.0008102444046886643, 'dropout': 0.1, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 50}. Best is trial 0 with value: 2.766465902328491.
[W 2024-08-13 23:07:44,453] The parameter 'norm_type' in trial#3 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
[W 2024-08-13 23:07:44,553] The parameter 'train_epochs' in trial#3 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-small_range', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-small_range', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-small_range.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-small_range', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-small_range'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_89_withTime.csv', test_path='test_farm_89_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=5, input_dim=75, dec_in=75, output_dim=1, d_model=64, n_heads=4, e_layers=2, hidden_d_model=128, seq_layers=2, last_d_model=128, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=32, time_d_model=32, pos_d_model=32, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.1, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=64), exp_settings='seq_len-8-lr-0.0031-d-64-hid_d-128-last_d-128-tok_d-32-time_d-32-pos_d-32-e_layers-2-tok_conv_k-5-dropout-0.1-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=50, batch_size=1024, early_stop_patience=15, learning_rate=0.0031492530428619287, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


