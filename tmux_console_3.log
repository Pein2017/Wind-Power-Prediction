[I 2024-08-13 23:38:18,418] Using an existing study with name '24-08-13-mlp_v3-search-farm_89' instead of creating a new one.
Creating study "24-08-13-mlp_v3-search-farm_89" with storage "sqlite:////data3/lsf/Pein/Power-Prediction/optuna_results/24-08-13-mlp_v3-search/24-08-13-mlp_v3-search-farm_89.db?mode=wal"...
  0%|          | 0/100 [00:00<?, ?it/s]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA RTX 6000 Ada Generation') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 20.8 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
20.8 M    Trainable params
0         Non-trainable params
20.8 M    Total params
83.357    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.705
Metric Loss/val improved by 0.477 >= min_delta = 0.0. New best score: 1.228
Metric Loss/val improved by 0.191 >= min_delta = 0.0. New best score: 1.037
Metric Loss/val improved by 0.007 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
`Trainer.fit` stopped: `max_epochs=30` reached.
                                         0%|          | 0/100 [02:41<?, ?it/s]Best trial: 1. Best value: 0.995314:   0%|          | 0/100 [02:41<?, ?it/s]Best trial: 1. Best value: 0.995314:   1%|          | 1/100 [02:41<4:27:13, 161.95s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                       Best trial: 1. Best value: 0.995314:   1%|          | 1/100 [02:42<4:27:13, 161.95s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 16.6 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
16.6 M    Trainable params
0         Non-trainable params
16.6 M    Total params
66.350    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.078
Metric Loss/val improved by 0.017 >= min_delta = 0.0. New best score: 1.060
Metric Loss/val improved by 0.029 >= min_delta = 0.0. New best score: 1.031
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                       Best trial: 1. Best value: 0.995314:   1%|          | 1/100 [04:59<4:27:13, 161.95s/it]Best trial: 1. Best value: 0.995314:   1%|          | 1/100 [04:59<4:27:13, 161.95s/it]Best trial: 1. Best value: 0.995314:   2%|▏         | 2/100 [04:59<4:01:26, 147.82s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                       Best trial: 1. Best value: 0.995314:   2%|▏         | 2/100 [05:00<4:01:26, 147.82s/it]Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=256, n_heads=4, e_layers=4, hidden_d_model=256, seq_layers=2, last_d_model=1664, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=48, time_d_model=112, pos_d_model=112, combine_type='add', seq_len=16, pred_len=1, min_y_value=0.0, dropout=0.28, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=4, fc_layer_type='mha', conv_out_dim=416), exp_settings='seq_len-16-lr-0.0599-d-256-hid_d-256-last_d-1664-tok_d-48-time_d-112-pos_d-112-e_layers-4-tok_conv_k-10-dropout-0.28-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-4-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.059932629095162074, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-13 23:41:00,373] Trial 1 finished with value: 0.9953140735626221 and parameters: {'num_heads': 4, 'hidden_d_model': 256, 'token_conv_kernel': 10, 'last_d_model': 1664, 'seq_len': 16, 'token_d_model': 48, 'time_d_model': 112, 'pos_d_model': 112, 'd_model': 256, 'conv_out_dim': 416, 'e_layers': 4, 'learning_rate': 0.059932629095162074, 'dropout': 0.28, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 1 with value: 0.9953140735626221.
[W 2024-08-13 23:41:01,255] The parameter 'norm_type' in trial#3 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=8, input_dim=84, dec_in=84, output_dim=1, d_model=320, n_heads=4, e_layers=6, hidden_d_model=160, seq_layers=2, last_d_model=1728, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=48, time_d_model=64, pos_d_model=96, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.18, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=68, fc_layer_type='mha', conv_out_dim=320), exp_settings='seq_len-12-lr-0.007-d-320-hid_d-160-last_d-1728-tok_d-48-time_d-64-pos_d-96-e_layers-6-tok_conv_k-8-dropout-0.18-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-68-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0070313058636795895, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-13 23:43:18,307] Trial 3 finished with value: 1.1033746123313906 and parameters: {'num_heads': 68, 'hidden_d_model': 160, 'token_conv_kernel': 8, 'last_d_model': 1728, 'seq_len': 12, 'token_d_model': 48, 'time_d_model': 64, 'pos_d_model': 96, 'd_model': 320, 'conv_out_dim': 320, 'e_layers': 6, 'learning_rate': 0.0070313058636795895, 'dropout': 0.18, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 1 with value: 0.9953140735626221.
[W 2024-08-13 23:43:19,010] The parameter 'norm_type' in trial#5 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 10.8 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
10.8 M    Trainable params
0         Non-trainable params
10.8 M    Total params
43.369    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.375
Metric Loss/val improved by 0.074 >= min_delta = 0.0. New best score: 1.301
Metric Loss/val improved by 0.243 >= min_delta = 0.0. New best score: 1.058
Metric Loss/val improved by 0.028 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                       Best trial: 1. Best value: 0.995314:   2%|▏         | 2/100 [07:23<4:01:26, 147.82s/it]Best trial: 4. Best value: 0.969465:   2%|▏         | 2/100 [07:23<4:01:26, 147.82s/it]Best trial: 4. Best value: 0.969465:   3%|▎         | 3/100 [07:23<3:55:42, 145.80s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                       Best trial: 4. Best value: 0.969465:   3%|▎         | 3/100 [07:24<3:55:42, 145.80s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 13.7 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
13.7 M    Trainable params
0         Non-trainable params
13.7 M    Total params
54.927    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.078
Metric Loss/val improved by 0.041 >= min_delta = 0.0. New best score: 1.037
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 1.034
Metric Loss/val improved by 0.004 >= min_delta = 0.0. New best score: 1.031
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                       Best trial: 4. Best value: 0.969465:   3%|▎         | 3/100 [09:43<3:55:42, 145.80s/it]Best trial: 4. Best value: 0.969465:   3%|▎         | 3/100 [09:43<3:55:42, 145.80s/it]Best trial: 4. Best value: 0.969465:   4%|▍         | 4/100 [09:43<3:50:01, 143.76s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                       Best trial: 4. Best value: 0.969465:   4%|▍         | 4/100 [09:44<3:50:01, 143.76s/it]dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=8, input_dim=84, dec_in=84, output_dim=1, d_model=288, n_heads=4, e_layers=4, hidden_d_model=160, seq_layers=2, last_d_model=1312, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=48, time_d_model=64, pos_d_model=48, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.22, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=224), exp_settings='seq_len-12-lr-0.0427-d-288-hid_d-160-last_d-1312-tok_d-48-time_d-64-pos_d-48-e_layers-4-tok_conv_k-8-dropout-0.22-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.04274410996182548, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-13 23:45:41,692] Trial 5 finished with value: 0.9966963291168214 and parameters: {'num_heads': 36, 'hidden_d_model': 160, 'token_conv_kernel': 8, 'last_d_model': 1312, 'seq_len': 12, 'token_d_model': 48, 'time_d_model': 64, 'pos_d_model': 48, 'd_model': 288, 'conv_out_dim': 224, 'e_layers': 4, 'learning_rate': 0.04274410996182548, 'dropout': 0.22, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-13 23:45:42,560] The parameter 'norm_type' in trial#7 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=8, input_dim=84, dec_in=84, output_dim=1, d_model=320, n_heads=4, e_layers=4, hidden_d_model=192, seq_layers=2, last_d_model=768, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=48, time_d_model=80, pos_d_model=128, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=288), exp_settings='seq_len-12-lr-0.0092-d-320-hid_d-192-last_d-768-tok_d-48-time_d-80-pos_d-128-e_layers-4-tok_conv_k-8-dropout-0.16-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.009183195843184562, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-13 23:48:02,328] Trial 7 finished with value: 0.9972919940948487 and parameters: {'num_heads': 36, 'hidden_d_model': 192, 'token_conv_kernel': 8, 'last_d_model': 768, 'seq_len': 12, 'token_d_model': 48, 'time_d_model': 80, 'pos_d_model': 128, 'd_model': 320, 'conv_out_dim': 288, 'e_layers': 4, 'learning_rate': 0.009183195843184562, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-13 23:48:03,168] The parameter 'norm_type' in trial#9 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 12.0 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
12.0 M    Trainable params
0         Non-trainable params
12.0 M    Total params
47.923    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.066
Metric Loss/val improved by 0.028 >= min_delta = 0.0. New best score: 1.038
Metric Loss/val improved by 0.008 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                       Best trial: 4. Best value: 0.969465:   4%|▍         | 4/100 [12:07<3:50:01, 143.76s/it]Best trial: 4. Best value: 0.969465:   4%|▍         | 4/100 [12:07<3:50:01, 143.76s/it]Best trial: 4. Best value: 0.969465:   5%|▌         | 5/100 [12:07<3:47:45, 143.85s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                       Best trial: 4. Best value: 0.969465:   5%|▌         | 5/100 [12:08<3:47:45, 143.85s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 12.3 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
12.3 M    Trainable params
0         Non-trainable params
12.3 M    Total params
49.364    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.072
Metric Loss/val improved by 0.043 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                       Best trial: 4. Best value: 0.969465:   5%|▌         | 5/100 [13:41<3:47:45, 143.85s/it]Best trial: 4. Best value: 0.969465:   5%|▌         | 5/100 [13:41<3:47:45, 143.85s/it]Best trial: 4. Best value: 0.969465:   6%|▌         | 6/100 [13:41<3:18:22, 126.62s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                       Best trial: 4. Best value: 0.969465:   6%|▌         | 6/100 [13:41<3:18:22, 126.62s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=256, n_heads=4, e_layers=6, hidden_d_model=208, seq_layers=2, last_d_model=480, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=32, time_d_model=112, pos_d_model=96, combine_type='add', seq_len=14, pred_len=1, min_y_value=0.0, dropout=0.18, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=288), exp_settings='seq_len-14-lr-0.011-d-256-hid_d-208-last_d-480-tok_d-32-time_d-112-pos_d-96-e_layers-6-tok_conv_k-10-dropout-0.18-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.010993348890247816, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-13 23:50:26,323] Trial 9 finished with value: 0.9958126783370971 and parameters: {'num_heads': 36, 'hidden_d_model': 208, 'token_conv_kernel': 10, 'last_d_model': 480, 'seq_len': 14, 'token_d_model': 32, 'time_d_model': 112, 'pos_d_model': 96, 'd_model': 256, 'conv_out_dim': 288, 'e_layers': 6, 'learning_rate': 0.010993348890247816, 'dropout': 0.18, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-13 23:50:26,609] The parameter 'norm_type' in trial#11 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=7, input_dim=84, dec_in=84, output_dim=1, d_model=192, n_heads=4, e_layers=4, hidden_d_model=208, seq_layers=2, last_d_model=928, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=48, time_d_model=112, pos_d_model=96, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.22, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=4, fc_layer_type='mha', conv_out_dim=352), exp_settings='seq_len-12-lr-0.0135-d-192-hid_d-208-last_d-928-tok_d-48-time_d-112-pos_d-96-e_layers-4-tok_conv_k-7-dropout-0.22-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-4-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.013489934219097909, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-13 23:51:59,518] Trial 11 finished with value: 4.611390626430511 and parameters: {'num_heads': 4, 'hidden_d_model': 208, 'token_conv_kernel': 7, 'last_d_model': 928, 'seq_len': 12, 'token_d_model': 48, 'time_d_model': 112, 'pos_d_model': 96, 'd_model': 192, 'conv_out_dim': 352, 'e_layers': 4, 'learning_rate': 0.013489934219097909, 'dropout': 0.22, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-13 23:51:59,797] The parameter 'norm_type' in trial#13 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 20.3 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
20.3 M    Trainable params
0         Non-trainable params
20.3 M    Total params
81.269    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.031
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                       Best trial: 4. Best value: 0.969465:   6%|▌         | 6/100 [15:56<3:18:22, 126.62s/it]Best trial: 4. Best value: 0.969465:   6%|▌         | 6/100 [15:56<3:18:22, 126.62s/it]Best trial: 4. Best value: 0.969465:   7%|▋         | 7/100 [15:56<3:20:50, 129.58s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                       Best trial: 4. Best value: 0.969465:   7%|▋         | 7/100 [15:57<3:20:50, 129.58s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 15.0 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
15.0 M    Trainable params
0         Non-trainable params
15.0 M    Total params
60.068    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.044
Metric Loss/val improved by 0.013 >= min_delta = 0.0. New best score: 1.031
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                       Best trial: 4. Best value: 0.969465:   7%|▋         | 7/100 [18:16<3:20:50, 129.58s/it]Best trial: 4. Best value: 0.969465:   7%|▋         | 7/100 [18:16<3:20:50, 129.58s/it]Best trial: 4. Best value: 0.969465:   8%|▊         | 8/100 [18:16<3:23:26, 132.68s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                       Best trial: 4. Best value: 0.969465:   8%|▊         | 8/100 [18:16<3:23:26, 132.68s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=320, n_heads=4, e_layers=6, hidden_d_model=160, seq_layers=2, last_d_model=1408, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=48, time_d_model=80, pos_d_model=48, combine_type='add', seq_len=10, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=4, fc_layer_type='mha', conv_out_dim=384), exp_settings='seq_len-10-lr-0.0039-d-320-hid_d-160-last_d-1408-tok_d-48-time_d-80-pos_d-48-e_layers-6-tok_conv_k-9-dropout-0.14-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-4-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0038823828542979303, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-13 23:54:15,184] Trial 13 finished with value: 1.1863163232803347 and parameters: {'num_heads': 4, 'hidden_d_model': 160, 'token_conv_kernel': 9, 'last_d_model': 1408, 'seq_len': 10, 'token_d_model': 48, 'time_d_model': 80, 'pos_d_model': 48, 'd_model': 320, 'conv_out_dim': 384, 'e_layers': 6, 'learning_rate': 0.0038823828542979303, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-13 23:54:15,491] The parameter 'norm_type' in trial#15 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=11, input_dim=84, dec_in=84, output_dim=1, d_model=288, n_heads=4, e_layers=6, hidden_d_model=208, seq_layers=2, last_d_model=1216, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=48, time_d_model=80, pos_d_model=64, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.2, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=224), exp_settings='seq_len-12-lr-0.011-d-288-hid_d-208-last_d-1216-tok_d-48-time_d-80-pos_d-64-e_layers-6-tok_conv_k-11-dropout-0.2-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.01103707131896027, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-13 23:56:34,510] Trial 15 finished with value: 3.847883009910584 and parameters: {'num_heads': 36, 'hidden_d_model': 208, 'token_conv_kernel': 11, 'last_d_model': 1216, 'seq_len': 12, 'token_d_model': 48, 'time_d_model': 80, 'pos_d_model': 64, 'd_model': 288, 'conv_out_dim': 224, 'e_layers': 6, 'learning_rate': 0.01103707131896027, 'dropout': 0.2, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-13 23:56:34,780] The parameter 'norm_type' in trial#17 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 9.7 M  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
9.7 M     Trainable params
0         Non-trainable params
9.7 M     Total params
38.907    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 9.862
Metric Loss/val improved by 7.982 >= min_delta = 0.0. New best score: 1.880
Metric Loss/val improved by 0.850 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                       Best trial: 4. Best value: 0.969465:   8%|▊         | 8/100 [20:15<3:23:26, 132.68s/it]Best trial: 4. Best value: 0.969465:   8%|▊         | 8/100 [20:15<3:23:26, 132.68s/it]Best trial: 4. Best value: 0.969465:   9%|▉         | 9/100 [20:15<3:14:53, 128.50s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                       Best trial: 4. Best value: 0.969465:   9%|▉         | 9/100 [20:15<3:14:53, 128.50s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 15.3 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
15.3 M    Trainable params
0         Non-trainable params
15.3 M    Total params
61.052    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.158
Metric Loss/val improved by 0.129 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                       Best trial: 4. Best value: 0.969465:   9%|▉         | 9/100 [21:51<3:14:53, 128.50s/it]Best trial: 4. Best value: 0.969465:   9%|▉         | 9/100 [21:51<3:14:53, 128.50s/it]Best trial: 4. Best value: 0.969465:  10%|█         | 10/100 [21:51<2:57:42, 118.47s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                        Best trial: 4. Best value: 0.969465:  10%|█         | 10/100 [21:51<2:57:42, 118.47s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=7, input_dim=84, dec_in=84, output_dim=1, d_model=288, n_heads=4, e_layers=6, hidden_d_model=176, seq_layers=2, last_d_model=1280, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=96, pos_d_model=80, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.26, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=68, fc_layer_type='mha', conv_out_dim=128), exp_settings='seq_len-12-lr-0.0442-d-288-hid_d-176-last_d-1280-tok_d-64-time_d-96-pos_d-80-e_layers-6-tok_conv_k-7-dropout-0.26-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-68-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.044207478107315464, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-13 23:58:33,815] Trial 17 finished with value: 0.9969400882720948 and parameters: {'num_heads': 68, 'hidden_d_model': 176, 'token_conv_kernel': 7, 'last_d_model': 1280, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 96, 'pos_d_model': 80, 'd_model': 288, 'conv_out_dim': 128, 'e_layers': 6, 'learning_rate': 0.044207478107315464, 'dropout': 0.26, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-13 23:58:34,068] The parameter 'norm_type' in trial#20 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=7, input_dim=84, dec_in=84, output_dim=1, d_model=224, n_heads=4, e_layers=8, hidden_d_model=176, seq_layers=2, last_d_model=704, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=48, time_d_model=96, pos_d_model=80, combine_type='add', seq_len=10, pred_len=1, min_y_value=0.0, dropout=0.18, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=384), exp_settings='seq_len-10-lr-0.0185-d-224-hid_d-176-last_d-704-tok_d-48-time_d-96-pos_d-80-e_layers-8-tok_conv_k-7-dropout-0.18-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.018530625483151607, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 00:00:09,821] Trial 20 finished with value: 0.9978912353515625 and parameters: {'num_heads': 36, 'hidden_d_model': 176, 'token_conv_kernel': 7, 'last_d_model': 704, 'seq_len': 10, 'token_d_model': 48, 'time_d_model': 96, 'pos_d_model': 80, 'd_model': 224, 'conv_out_dim': 384, 'e_layers': 8, 'learning_rate': 0.018530625483151607, 'dropout': 0.18, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 00:00:10,094] The parameter 'norm_type' in trial#21 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 8.6 M  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
8.6 M     Trainable params
0         Non-trainable params
8.6 M     Total params
34.503    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.035
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.034
Metric Loss/val improved by 0.003 >= min_delta = 0.0. New best score: 1.031
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 1.028
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.028. Signaling Trainer to stop.
                                                                                        Best trial: 4. Best value: 0.969465:  10%|█         | 10/100 [24:07<2:57:42, 118.47s/it]Best trial: 4. Best value: 0.969465:  10%|█         | 10/100 [24:07<2:57:42, 118.47s/it]Best trial: 4. Best value: 0.969465:  11%|█         | 11/100 [24:07<3:03:32, 123.74s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                        Best trial: 4. Best value: 0.969465:  11%|█         | 11/100 [24:07<3:03:32, 123.74s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 10.3 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
10.3 M    Trainable params
0         Non-trainable params
10.3 M    Total params
41.351    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                        Best trial: 4. Best value: 0.969465:  11%|█         | 11/100 [25:29<3:03:32, 123.74s/it]Best trial: 4. Best value: 0.969465:  11%|█         | 11/100 [25:29<3:03:32, 123.74s/it]Best trial: 4. Best value: 0.969465:  12%|█▏        | 12/100 [25:29<2:43:10, 111.26s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                        Best trial: 4. Best value: 0.969465:  12%|█▏        | 12/100 [25:30<2:43:10, 111.26s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=320, n_heads=4, e_layers=4, hidden_d_model=208, seq_layers=2, last_d_model=608, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=32, time_d_model=96, pos_d_model=80, combine_type='add', seq_len=10, pred_len=1, min_y_value=0.0, dropout=0.18, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=68, fc_layer_type='mha', conv_out_dim=192), exp_settings='seq_len-10-lr-0.0056-d-320-hid_d-208-last_d-608-tok_d-32-time_d-96-pos_d-80-e_layers-4-tok_conv_k-9-dropout-0.18-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-68-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.005636553493107357, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 00:02:25,495] Trial 21 finished with value: 1.7078478693962098 and parameters: {'num_heads': 68, 'hidden_d_model': 208, 'token_conv_kernel': 9, 'last_d_model': 608, 'seq_len': 10, 'token_d_model': 32, 'time_d_model': 96, 'pos_d_model': 80, 'd_model': 320, 'conv_out_dim': 192, 'e_layers': 4, 'learning_rate': 0.005636553493107357, 'dropout': 0.18, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 00:02:26,022] The parameter 'norm_type' in trial#23 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=8, input_dim=84, dec_in=84, output_dim=1, d_model=160, n_heads=4, e_layers=6, hidden_d_model=224, seq_layers=2, last_d_model=416, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=48, time_d_model=64, pos_d_model=96, combine_type='add', seq_len=14, pred_len=1, min_y_value=0.0, dropout=0.18, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=256), exp_settings='seq_len-14-lr-0.0031-d-160-hid_d-224-last_d-416-tok_d-48-time_d-64-pos_d-96-e_layers-6-tok_conv_k-8-dropout-0.18-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0031301233145570083, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 00:03:48,231] Trial 23 finished with value: 1.0485028505325318 and parameters: {'num_heads': 36, 'hidden_d_model': 224, 'token_conv_kernel': 8, 'last_d_model': 416, 'seq_len': 14, 'token_d_model': 48, 'time_d_model': 64, 'pos_d_model': 96, 'd_model': 160, 'conv_out_dim': 256, 'e_layers': 6, 'learning_rate': 0.0031301233145570083, 'dropout': 0.18, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 00:03:48,479] The parameter 'norm_type' in trial#25 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 13.1 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
13.1 M    Trainable params
0         Non-trainable params
13.1 M    Total params
52.348    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.066
Metric Loss/val improved by 0.032 >= min_delta = 0.0. New best score: 1.033
Metric Loss/val improved by 0.005 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                        Best trial: 4. Best value: 0.969465:  12%|█▏        | 12/100 [27:32<2:43:10, 111.26s/it]Best trial: 4. Best value: 0.969465:  12%|█▏        | 12/100 [27:32<2:43:10, 111.26s/it]Best trial: 4. Best value: 0.969465:  13%|█▎        | 13/100 [27:32<2:46:25, 114.77s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                        Best trial: 4. Best value: 0.969465:  13%|█▎        | 13/100 [27:32<2:46:25, 114.77s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 11.3 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
11.3 M    Trainable params
0         Non-trainable params
11.3 M    Total params
45.081    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.030
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 1.028
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.028. Signaling Trainer to stop.
                                                                                        Best trial: 4. Best value: 0.969465:  13%|█▎        | 13/100 [29:43<2:46:25, 114.77s/it]Best trial: 4. Best value: 0.969465:  13%|█▎        | 13/100 [29:43<2:46:25, 114.77s/it]Best trial: 4. Best value: 0.969465:  14%|█▍        | 14/100 [29:43<2:51:19, 119.53s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                        Best trial: 4. Best value: 0.969465:  14%|█▍        | 14/100 [29:44<2:51:19, 119.53s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=8, input_dim=84, dec_in=84, output_dim=1, d_model=288, n_heads=4, e_layers=6, hidden_d_model=192, seq_layers=2, last_d_model=320, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=48, time_d_model=64, pos_d_model=96, combine_type='add', seq_len=10, pred_len=1, min_y_value=0.0, dropout=0.2, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=256), exp_settings='seq_len-10-lr-0.0102-d-288-hid_d-192-last_d-320-tok_d-48-time_d-64-pos_d-96-e_layers-6-tok_conv_k-8-dropout-0.2-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.010169195421510572, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 00:05:51,076] Trial 25 finished with value: 0.9956626892089844 and parameters: {'num_heads': 36, 'hidden_d_model': 192, 'token_conv_kernel': 8, 'last_d_model': 320, 'seq_len': 10, 'token_d_model': 48, 'time_d_model': 64, 'pos_d_model': 96, 'd_model': 288, 'conv_out_dim': 256, 'e_layers': 6, 'learning_rate': 0.010169195421510572, 'dropout': 0.2, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 00:05:51,380] The parameter 'norm_type' in trial#26 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=8, input_dim=84, dec_in=84, output_dim=1, d_model=352, n_heads=4, e_layers=4, hidden_d_model=208, seq_layers=2, last_d_model=1152, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=32, time_d_model=80, pos_d_model=96, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.22, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=288), exp_settings='seq_len-12-lr-0.0042-d-352-hid_d-208-last_d-1152-tok_d-32-time_d-80-pos_d-96-e_layers-4-tok_conv_k-8-dropout-0.22-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.004203127278899738, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 00:08:01,598] Trial 26 finished with value: 2.246219921112061 and parameters: {'num_heads': 36, 'hidden_d_model': 208, 'token_conv_kernel': 8, 'last_d_model': 1152, 'seq_len': 12, 'token_d_model': 32, 'time_d_model': 80, 'pos_d_model': 96, 'd_model': 352, 'conv_out_dim': 288, 'e_layers': 4, 'learning_rate': 0.004203127278899738, 'dropout': 0.22, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 00:08:03,129] The parameter 'norm_type' in trial#28 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 11.4 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
11.4 M    Trainable params
0         Non-trainable params
11.4 M    Total params
45.443    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.033
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.031
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                        Best trial: 4. Best value: 0.969465:  14%|█▍        | 14/100 [32:48<2:51:19, 119.53s/it]Best trial: 4. Best value: 0.969465:  14%|█▍        | 14/100 [32:48<2:51:19, 119.53s/it]Best trial: 4. Best value: 0.969465:  15%|█▌        | 15/100 [32:48<3:17:25, 139.36s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                        Best trial: 4. Best value: 0.969465:  15%|█▌        | 15/100 [32:48<3:17:25, 139.36s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 20.1 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
20.1 M    Trainable params
0         Non-trainable params
20.1 M    Total params
80.455    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.035
Metric Loss/val improved by 0.006 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                        Best trial: 4. Best value: 0.969465:  15%|█▌        | 15/100 [35:07<3:17:25, 139.36s/it]Best trial: 4. Best value: 0.969465:  15%|█▌        | 15/100 [35:07<3:17:25, 139.36s/it]Best trial: 4. Best value: 0.969465:  16%|█▌        | 16/100 [35:07<3:15:02, 139.32s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                        Best trial: 4. Best value: 0.969465:  16%|█▌        | 16/100 [35:07<3:15:02, 139.32s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=7, input_dim=84, dec_in=84, output_dim=1, d_model=256, n_heads=4, e_layers=4, hidden_d_model=192, seq_layers=2, last_d_model=1312, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=48, time_d_model=96, pos_d_model=64, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.18, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=288), exp_settings='seq_len-12-lr-0.003-d-256-hid_d-192-last_d-1312-tok_d-48-time_d-96-pos_d-64-e_layers-4-tok_conv_k-7-dropout-0.18-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.002953581436508874, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 00:11:06,907] Trial 28 finished with value: 4.010719430446625 and parameters: {'num_heads': 36, 'hidden_d_model': 192, 'token_conv_kernel': 7, 'last_d_model': 1312, 'seq_len': 12, 'token_d_model': 48, 'time_d_model': 96, 'pos_d_model': 64, 'd_model': 256, 'conv_out_dim': 288, 'e_layers': 4, 'learning_rate': 0.002953581436508874, 'dropout': 0.18, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 00:11:07,177] The parameter 'norm_type' in trial#30 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=7, input_dim=84, dec_in=84, output_dim=1, d_model=384, n_heads=4, e_layers=8, hidden_d_model=176, seq_layers=2, last_d_model=576, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=112, pos_d_model=80, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.22, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=4, fc_layer_type='mha', conv_out_dim=256), exp_settings='seq_len-12-lr-0.0088-d-384-hid_d-176-last_d-576-tok_d-64-time_d-112-pos_d-80-e_layers-8-tok_conv_k-7-dropout-0.22-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-4-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.00882232399461023, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 00:13:26,131] Trial 30 finished with value: 0.993084192276001 and parameters: {'num_heads': 4, 'hidden_d_model': 176, 'token_conv_kernel': 7, 'last_d_model': 576, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 112, 'pos_d_model': 80, 'd_model': 384, 'conv_out_dim': 256, 'e_layers': 8, 'learning_rate': 0.00882232399461023, 'dropout': 0.22, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 00:13:26,420] The parameter 'norm_type' in trial#32 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 11.3 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
11.3 M    Trainable params
0         Non-trainable params
11.3 M    Total params
45.024    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.153
Metric Loss/val improved by 0.066 >= min_delta = 0.0. New best score: 1.087
Metric Loss/val improved by 0.057 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                        Best trial: 4. Best value: 0.969465:  16%|█▌        | 16/100 [37:19<3:15:02, 139.32s/it]Best trial: 4. Best value: 0.969465:  16%|█▌        | 16/100 [37:19<3:15:02, 139.32s/it]Best trial: 4. Best value: 0.969465:  17%|█▋        | 17/100 [37:19<3:09:43, 137.15s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                        Best trial: 4. Best value: 0.969465:  17%|█▋        | 17/100 [37:20<3:09:43, 137.15s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 14.2 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
14.2 M    Trainable params
0         Non-trainable params
14.2 M    Total params
56.676    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.037
Metric Loss/val improved by 0.006 >= min_delta = 0.0. New best score: 1.031
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                        Best trial: 4. Best value: 0.969465:  17%|█▋        | 17/100 [40:24<3:09:43, 137.15s/it]Best trial: 4. Best value: 0.969465:  17%|█▋        | 17/100 [40:24<3:09:43, 137.15s/it]Best trial: 4. Best value: 0.969465:  18%|█▊        | 18/100 [40:24<3:27:03, 151.51s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                        Best trial: 4. Best value: 0.969465:  18%|█▊        | 18/100 [40:25<3:27:03, 151.51s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=8, input_dim=84, dec_in=84, output_dim=1, d_model=256, n_heads=4, e_layers=8, hidden_d_model=192, seq_layers=2, last_d_model=704, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=48, time_d_model=96, pos_d_model=96, combine_type='add', seq_len=10, pred_len=1, min_y_value=0.0, dropout=0.2, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=192), exp_settings='seq_len-10-lr-0.0168-d-256-hid_d-192-last_d-704-tok_d-48-time_d-96-pos_d-96-e_layers-8-tok_conv_k-8-dropout-0.2-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.016818225751083226, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 00:15:38,248] Trial 32 finished with value: 0.9969167947769166 and parameters: {'num_heads': 36, 'hidden_d_model': 192, 'token_conv_kernel': 8, 'last_d_model': 704, 'seq_len': 10, 'token_d_model': 48, 'time_d_model': 96, 'pos_d_model': 96, 'd_model': 256, 'conv_out_dim': 192, 'e_layers': 8, 'learning_rate': 0.016818225751083226, 'dropout': 0.2, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 00:15:38,518] The parameter 'norm_type' in trial#34 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=7, input_dim=84, dec_in=84, output_dim=1, d_model=320, n_heads=4, e_layers=6, hidden_d_model=224, seq_layers=2, last_d_model=192, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=112, pos_d_model=64, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.2, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=224), exp_settings='seq_len-12-lr-0.0085-d-320-hid_d-224-last_d-192-tok_d-64-time_d-112-pos_d-64-e_layers-6-tok_conv_k-7-dropout-0.2-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.008509500772333107, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 00:18:43,164] Trial 34 finished with value: 1.0302503347396852 and parameters: {'num_heads': 36, 'hidden_d_model': 224, 'token_conv_kernel': 7, 'last_d_model': 192, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 112, 'pos_d_model': 64, 'd_model': 320, 'conv_out_dim': 224, 'e_layers': 6, 'learning_rate': 0.008509500772333107, 'dropout': 0.2, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 00:18:43,451] The parameter 'norm_type' in trial#37 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 10.1 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
10.1 M    Trainable params
0         Non-trainable params
10.1 M    Total params
40.534    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 6.726
Metric Loss/val improved by 5.697 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                        Best trial: 4. Best value: 0.969465:  18%|█▊        | 18/100 [41:24<3:27:03, 151.51s/it]Best trial: 4. Best value: 0.969465:  18%|█▊        | 18/100 [41:24<3:27:03, 151.51s/it]Best trial: 4. Best value: 0.969465:  19%|█▉        | 19/100 [41:24<2:47:28, 124.06s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                        Best trial: 4. Best value: 0.969465:  19%|█▉        | 19/100 [41:25<2:47:28, 124.06s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 22.8 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
22.8 M    Trainable params
0         Non-trainable params
22.8 M    Total params
91.376    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.031
Metric Loss/val improved by 0.003 >= min_delta = 0.0. New best score: 1.028
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.028. Signaling Trainer to stop.
                                                                                        Best trial: 4. Best value: 0.969465:  19%|█▉        | 19/100 [44:19<2:47:28, 124.06s/it]Best trial: 4. Best value: 0.969465:  19%|█▉        | 19/100 [44:19<2:47:28, 124.06s/it]Best trial: 4. Best value: 0.969465:  20%|██        | 20/100 [44:19<3:05:37, 139.22s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                        Best trial: 4. Best value: 0.969465:  20%|██        | 20/100 [44:19<3:05:37, 139.22s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=5, input_dim=84, dec_in=84, output_dim=1, d_model=384, n_heads=4, e_layers=6, hidden_d_model=192, seq_layers=2, last_d_model=576, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=48, time_d_model=96, pos_d_model=64, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=68, fc_layer_type='mha', conv_out_dim=128), exp_settings='seq_len-12-lr-0.0509-d-384-hid_d-192-last_d-576-tok_d-48-time_d-96-pos_d-64-e_layers-6-tok_conv_k-5-dropout-0.14-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-68-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.05087386622466724, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 00:19:43,291] Trial 37 finished with value: 0.9935665488243104 and parameters: {'num_heads': 68, 'hidden_d_model': 192, 'token_conv_kernel': 5, 'last_d_model': 576, 'seq_len': 12, 'token_d_model': 48, 'time_d_model': 96, 'pos_d_model': 64, 'd_model': 384, 'conv_out_dim': 128, 'e_layers': 6, 'learning_rate': 0.05087386622466724, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 00:19:43,623] The parameter 'norm_type' in trial#38 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=7, input_dim=84, dec_in=84, output_dim=1, d_model=480, n_heads=4, e_layers=6, hidden_d_model=176, seq_layers=2, last_d_model=576, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=80, pos_d_model=64, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.2, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=288), exp_settings='seq_len-12-lr-0.0036-d-480-hid_d-176-last_d-576-tok_d-64-time_d-80-pos_d-64-e_layers-6-tok_conv_k-7-dropout-0.2-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.003637606601018109, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 00:22:37,830] Trial 38 finished with value: 1.2498610973358155 and parameters: {'num_heads': 36, 'hidden_d_model': 176, 'token_conv_kernel': 7, 'last_d_model': 576, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 80, 'pos_d_model': 64, 'd_model': 480, 'conv_out_dim': 288, 'e_layers': 6, 'learning_rate': 0.003637606601018109, 'dropout': 0.2, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 00:22:38,114] The parameter 'norm_type' in trial#41 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 14.2 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
14.2 M    Trainable params
0         Non-trainable params
14.2 M    Total params
56.770    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.031
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                        Best trial: 4. Best value: 0.969465:  20%|██        | 20/100 [47:08<3:05:37, 139.22s/it]Best trial: 4. Best value: 0.969465:  20%|██        | 20/100 [47:08<3:05:37, 139.22s/it]Best trial: 4. Best value: 0.969465:  21%|██        | 21/100 [47:08<3:15:12, 148.26s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                        Best trial: 4. Best value: 0.969465:  21%|██        | 21/100 [47:09<3:15:12, 148.26s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 20.3 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
20.3 M    Trainable params
0         Non-trainable params
20.3 M    Total params
81.049    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.054
Metric Loss/val improved by 0.024 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                        Best trial: 4. Best value: 0.969465:  21%|██        | 21/100 [50:45<3:15:12, 148.26s/it]Best trial: 4. Best value: 0.969465:  21%|██        | 21/100 [50:45<3:15:12, 148.26s/it]Best trial: 4. Best value: 0.969465:  22%|██▏       | 22/100 [50:45<3:39:38, 168.96s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                        Best trial: 4. Best value: 0.969465:  22%|██▏       | 22/100 [50:46<3:39:38, 168.96s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=6, input_dim=84, dec_in=84, output_dim=1, d_model=384, n_heads=4, e_layers=6, hidden_d_model=192, seq_layers=2, last_d_model=1056, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=48, time_d_model=64, pos_d_model=48, combine_type='add', seq_len=14, pred_len=1, min_y_value=0.0, dropout=0.18, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=256), exp_settings='seq_len-14-lr-0.0053-d-384-hid_d-192-last_d-1056-tok_d-48-time_d-64-pos_d-48-e_layers-6-tok_conv_k-6-dropout-0.18-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.005289108819375128, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 00:25:27,187] Trial 41 finished with value: 1.1250167846679688 and parameters: {'num_heads': 36, 'hidden_d_model': 192, 'token_conv_kernel': 6, 'last_d_model': 1056, 'seq_len': 14, 'token_d_model': 48, 'time_d_model': 64, 'pos_d_model': 48, 'd_model': 384, 'conv_out_dim': 256, 'e_layers': 6, 'learning_rate': 0.005289108819375128, 'dropout': 0.18, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 00:25:27,457] The parameter 'norm_type' in trial#43 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=8, input_dim=84, dec_in=84, output_dim=1, d_model=416, n_heads=4, e_layers=8, hidden_d_model=176, seq_layers=2, last_d_model=800, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=80, pos_d_model=96, combine_type='add', seq_len=14, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=192), exp_settings='seq_len-14-lr-0.0076-d-416-hid_d-176-last_d-800-tok_d-64-time_d-80-pos_d-96-e_layers-8-tok_conv_k-8-dropout-0.14-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.007603848855986958, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 00:29:04,395] Trial 43 finished with value: 0.9931003093719483 and parameters: {'num_heads': 36, 'hidden_d_model': 176, 'token_conv_kernel': 8, 'last_d_model': 800, 'seq_len': 14, 'token_d_model': 64, 'time_d_model': 80, 'pos_d_model': 96, 'd_model': 416, 'conv_out_dim': 192, 'e_layers': 8, 'learning_rate': 0.007603848855986958, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 00:29:04,695] The parameter 'norm_type' in trial#46 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 18.9 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
18.9 M    Trainable params
0         Non-trainable params
18.9 M    Total params
75.662    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.916
Metric Loss/val improved by 0.496 >= min_delta = 0.0. New best score: 1.420
Metric Loss/val improved by 0.348 >= min_delta = 0.0. New best score: 1.072
Metric Loss/val improved by 0.021 >= min_delta = 0.0. New best score: 1.052
Metric Loss/val improved by 0.020 >= min_delta = 0.0. New best score: 1.031
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                        Best trial: 4. Best value: 0.969465:  22%|██▏       | 22/100 [53:03<3:39:38, 168.96s/it]Best trial: 4. Best value: 0.969465:  22%|██▏       | 22/100 [53:03<3:39:38, 168.96s/it]Best trial: 4. Best value: 0.969465:  23%|██▎       | 23/100 [53:03<3:24:32, 159.38s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                        Best trial: 4. Best value: 0.969465:  23%|██▎       | 23/100 [53:03<3:24:32, 159.38s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 20.3 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
20.3 M    Trainable params
0         Non-trainable params
20.3 M    Total params
81.298    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.062
Metric Loss/val improved by 0.015 >= min_delta = 0.0. New best score: 1.047
Metric Loss/val improved by 0.008 >= min_delta = 0.0. New best score: 1.040
Metric Loss/val improved by 0.008 >= min_delta = 0.0. New best score: 1.031
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                        Best trial: 4. Best value: 0.969465:  23%|██▎       | 23/100 [56:00<3:24:32, 159.38s/it]Best trial: 4. Best value: 0.969465:  23%|██▎       | 23/100 [56:00<3:24:32, 159.38s/it]Best trial: 4. Best value: 0.969465:  24%|██▍       | 24/100 [56:00<3:28:41, 164.76s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                        Best trial: 4. Best value: 0.969465:  24%|██▍       | 24/100 [56:00<3:28:41, 164.76s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=6, input_dim=84, dec_in=84, output_dim=1, d_model=416, n_heads=4, e_layers=6, hidden_d_model=208, seq_layers=2, last_d_model=1216, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=128, pos_d_model=64, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.24000000000000002, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=288), exp_settings='seq_len-12-lr-0.0238-d-416-hid_d-208-last_d-1216-tok_d-64-time_d-128-pos_d-64-e_layers-6-tok_conv_k-6-dropout-0.24000000000000002-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.023812328265570868, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 00:31:21,436] Trial 46 finished with value: 0.9957833409309387 and parameters: {'num_heads': 36, 'hidden_d_model': 208, 'token_conv_kernel': 6, 'last_d_model': 1216, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 128, 'pos_d_model': 64, 'd_model': 416, 'conv_out_dim': 288, 'e_layers': 6, 'learning_rate': 0.023812328265570868, 'dropout': 0.24000000000000002, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 00:31:21,749] The parameter 'norm_type' in trial#47 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=8, input_dim=84, dec_in=84, output_dim=1, d_model=288, n_heads=4, e_layers=8, hidden_d_model=208, seq_layers=2, last_d_model=896, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=64, pos_d_model=80, combine_type='add', seq_len=10, pred_len=1, min_y_value=0.0, dropout=0.2, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=320), exp_settings='seq_len-10-lr-0.012-d-288-hid_d-208-last_d-896-tok_d-64-time_d-64-pos_d-80-e_layers-8-tok_conv_k-8-dropout-0.2-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.01201633272285195, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 00:34:18,762] Trial 47 finished with value: 0.9946106791496278 and parameters: {'num_heads': 36, 'hidden_d_model': 208, 'token_conv_kernel': 8, 'last_d_model': 896, 'seq_len': 10, 'token_d_model': 64, 'time_d_model': 64, 'pos_d_model': 80, 'd_model': 288, 'conv_out_dim': 320, 'e_layers': 8, 'learning_rate': 0.01201633272285195, 'dropout': 0.2, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 00:34:19,081] The parameter 'norm_type' in trial#50 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 18.0 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
18.0 M    Trainable params
0         Non-trainable params
18.0 M    Total params
71.910    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.200
Metric Loss/val improved by 0.013 >= min_delta = 0.0. New best score: 1.186
Metric Loss/val improved by 0.150 >= min_delta = 0.0. New best score: 1.036
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.036
Metric Loss/val improved by 0.006 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                        Best trial: 4. Best value: 0.969465:  24%|██▍       | 24/100 [59:03<3:28:41, 164.76s/it]Best trial: 4. Best value: 0.969465:  24%|██▍       | 24/100 [59:03<3:28:41, 164.76s/it]Best trial: 4. Best value: 0.969465:  25%|██▌       | 25/100 [59:03<3:32:54, 170.33s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                        Best trial: 4. Best value: 0.969465:  25%|██▌       | 25/100 [59:03<3:32:54, 170.33s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 24.3 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
24.3 M    Trainable params
0         Non-trainable params
24.3 M    Total params
97.056    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 10.000
Metric Loss/val improved by 8.662 >= min_delta = 0.0. New best score: 1.338
Metric Loss/val improved by 0.297 >= min_delta = 0.0. New best score: 1.042
Metric Loss/val improved by 0.013 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.028
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.028. Signaling Trainer to stop.
                                                                                        Best trial: 4. Best value: 0.969465:  25%|██▌       | 25/100 [1:01:59<3:32:54, 170.33s/it]Best trial: 4. Best value: 0.969465:  25%|██▌       | 25/100 [1:01:59<3:32:54, 170.33s/it]Best trial: 4. Best value: 0.969465:  26%|██▌       | 26/100 [1:01:59<3:32:10, 172.03s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                          Best trial: 4. Best value: 0.969465:  26%|██▌       | 26/100 [1:01:59<3:32:10, 172.03s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=6, input_dim=84, dec_in=84, output_dim=1, d_model=320, n_heads=4, e_layers=8, hidden_d_model=176, seq_layers=2, last_d_model=672, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=112, pos_d_model=80, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.2, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=4, fc_layer_type='mha', conv_out_dim=320), exp_settings='seq_len-12-lr-0.0139-d-320-hid_d-176-last_d-672-tok_d-64-time_d-112-pos_d-80-e_layers-8-tok_conv_k-6-dropout-0.2-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-4-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.013948718715402507, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 00:37:22,079] Trial 50 finished with value: 1.3120426416397095 and parameters: {'num_heads': 4, 'hidden_d_model': 176, 'token_conv_kernel': 6, 'last_d_model': 672, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 112, 'pos_d_model': 80, 'd_model': 320, 'conv_out_dim': 320, 'e_layers': 8, 'learning_rate': 0.013948718715402507, 'dropout': 0.2, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 00:37:22,362] The parameter 'norm_type' in trial#52 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=8, input_dim=84, dec_in=84, output_dim=1, d_model=352, n_heads=4, e_layers=8, hidden_d_model=160, seq_layers=2, last_d_model=512, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=96, pos_d_model=80, combine_type='add', seq_len=14, pred_len=1, min_y_value=0.0, dropout=0.2, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=352), exp_settings='seq_len-14-lr-0.0478-d-352-hid_d-160-last_d-512-tok_d-64-time_d-96-pos_d-80-e_layers-8-tok_conv_k-8-dropout-0.2-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.04777365045593731, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 00:40:18,071] Trial 52 finished with value: 0.9948279142379761 and parameters: {'num_heads': 36, 'hidden_d_model': 160, 'token_conv_kernel': 8, 'last_d_model': 512, 'seq_len': 14, 'token_d_model': 64, 'time_d_model': 96, 'pos_d_model': 80, 'd_model': 352, 'conv_out_dim': 352, 'e_layers': 8, 'learning_rate': 0.04777365045593731, 'dropout': 0.2, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 00:40:18,354] The parameter 'norm_type' in trial#54 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 18.7 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
18.7 M    Trainable params
0         Non-trainable params
18.7 M    Total params
74.705    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 6.250
Metric Loss/val improved by 5.220 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                          Best trial: 4. Best value: 0.969465:  26%|██▌       | 26/100 [1:04:57<3:32:10, 172.03s/it]Best trial: 4. Best value: 0.969465:  26%|██▌       | 26/100 [1:04:57<3:32:10, 172.03s/it]Best trial: 4. Best value: 0.969465:  27%|██▋       | 27/100 [1:04:57<3:31:15, 173.63s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                          Best trial: 4. Best value: 0.969465:  27%|██▋       | 27/100 [1:04:57<3:31:15, 173.63s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 19.1 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
19.1 M    Trainable params
0         Non-trainable params
19.1 M    Total params
76.467    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.071
Metric Loss/val improved by 0.022 >= min_delta = 0.0. New best score: 1.049
Metric Loss/val improved by 0.018 >= min_delta = 0.0. New best score: 1.031
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                          Best trial: 4. Best value: 0.969465:  27%|██▋       | 27/100 [1:08:01<3:31:15, 173.63s/it]Best trial: 4. Best value: 0.969465:  27%|██▋       | 27/100 [1:08:01<3:31:15, 173.63s/it]Best trial: 4. Best value: 0.969465:  28%|██▊       | 28/100 [1:08:01<3:32:16, 176.89s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                          Best trial: 4. Best value: 0.969465:  28%|██▊       | 28/100 [1:08:01<3:32:16, 176.89s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=7, input_dim=84, dec_in=84, output_dim=1, d_model=480, n_heads=4, e_layers=8, hidden_d_model=192, seq_layers=2, last_d_model=384, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=80, pos_d_model=80, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=68, fc_layer_type='mha', conv_out_dim=96), exp_settings='seq_len-12-lr-0.0223-d-480-hid_d-192-last_d-384-tok_d-64-time_d-80-pos_d-80-e_layers-8-tok_conv_k-7-dropout-0.16-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-68-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.022330997358422446, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 00:43:15,452] Trial 54 finished with value: 0.9953152179718019 and parameters: {'num_heads': 68, 'hidden_d_model': 192, 'token_conv_kernel': 7, 'last_d_model': 384, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 80, 'pos_d_model': 80, 'd_model': 480, 'conv_out_dim': 96, 'e_layers': 8, 'learning_rate': 0.022330997358422446, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 00:43:15,695] The parameter 'norm_type' in trial#56 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=8, input_dim=84, dec_in=84, output_dim=1, d_model=256, n_heads=4, e_layers=8, hidden_d_model=208, seq_layers=2, last_d_model=448, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=80, pos_d_model=80, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=320), exp_settings='seq_len-12-lr-0.0075-d-256-hid_d-208-last_d-448-tok_d-64-time_d-80-pos_d-80-e_layers-8-tok_conv_k-8-dropout-0.14-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0074720596943231, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 00:46:19,950] Trial 56 finished with value: 0.9981857776641846 and parameters: {'num_heads': 36, 'hidden_d_model': 208, 'token_conv_kernel': 8, 'last_d_model': 448, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 80, 'pos_d_model': 80, 'd_model': 256, 'conv_out_dim': 320, 'e_layers': 8, 'learning_rate': 0.0074720596943231, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 00:46:20,181] The parameter 'norm_type' in trial#59 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 23.0 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
23.0 M    Trainable params
0         Non-trainable params
23.0 M    Total params
92.084    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.402
Metric Loss/val improved by 0.097 >= min_delta = 0.0. New best score: 1.304
Metric Loss/val improved by 0.274 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                          Best trial: 4. Best value: 0.969465:  28%|██▊       | 28/100 [1:11:05<3:32:16, 176.89s/it]Best trial: 4. Best value: 0.969465:  28%|██▊       | 28/100 [1:11:05<3:32:16, 176.89s/it]Best trial: 4. Best value: 0.969465:  29%|██▉       | 29/100 [1:11:05<3:31:50, 179.02s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                          Best trial: 4. Best value: 0.969465:  29%|██▉       | 29/100 [1:11:05<3:31:50, 179.02s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 18.2 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
18.2 M    Trainable params
0         Non-trainable params
18.2 M    Total params
72.631    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.036
Metric Loss/val improved by 0.006 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                          Best trial: 4. Best value: 0.969465:  29%|██▉       | 29/100 [1:13:42<3:31:50, 179.02s/it]Best trial: 4. Best value: 0.969465:  29%|██▉       | 29/100 [1:13:42<3:31:50, 179.02s/it]Best trial: 4. Best value: 0.969465:  30%|███       | 30/100 [1:13:42<3:21:09, 172.42s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                          Best trial: 4. Best value: 0.969465:  30%|███       | 30/100 [1:13:42<3:21:09, 172.42s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=8, input_dim=84, dec_in=84, output_dim=1, d_model=384, n_heads=4, e_layers=8, hidden_d_model=192, seq_layers=2, last_d_model=704, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=80, pos_d_model=96, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.2, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=288), exp_settings='seq_len-12-lr-0.0186-d-384-hid_d-192-last_d-704-tok_d-64-time_d-80-pos_d-96-e_layers-8-tok_conv_k-8-dropout-0.2-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.01861392849392801, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 00:49:23,936] Trial 59 finished with value: 0.9936206579208375 and parameters: {'num_heads': 36, 'hidden_d_model': 192, 'token_conv_kernel': 8, 'last_d_model': 704, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 80, 'pos_d_model': 96, 'd_model': 384, 'conv_out_dim': 288, 'e_layers': 8, 'learning_rate': 0.01861392849392801, 'dropout': 0.2, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 00:49:24,211] The parameter 'norm_type' in trial#60 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=6, input_dim=84, dec_in=84, output_dim=1, d_model=480, n_heads=4, e_layers=8, hidden_d_model=176, seq_layers=2, last_d_model=416, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=64, pos_d_model=96, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.22, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=96), exp_settings='seq_len-12-lr-0.0056-d-480-hid_d-176-last_d-416-tok_d-64-time_d-64-pos_d-96-e_layers-8-tok_conv_k-6-dropout-0.22-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.005586657559197348, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 00:52:00,956] Trial 60 finished with value: 1.0127913236618042 and parameters: {'num_heads': 36, 'hidden_d_model': 176, 'token_conv_kernel': 6, 'last_d_model': 416, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 64, 'pos_d_model': 96, 'd_model': 480, 'conv_out_dim': 96, 'e_layers': 8, 'learning_rate': 0.005586657559197348, 'dropout': 0.22, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 00:52:01,268] The parameter 'norm_type' in trial#62 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 22.9 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
22.9 M    Trainable params
0         Non-trainable params
22.9 M    Total params
91.433    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 10.000
Metric Loss/val improved by 8.694 >= min_delta = 0.0. New best score: 1.306
Metric Loss/val improved by 0.276 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                          Best trial: 4. Best value: 0.969465:  30%|███       | 30/100 [1:17:20<3:21:09, 172.42s/it]Best trial: 4. Best value: 0.969465:  30%|███       | 30/100 [1:17:20<3:21:09, 172.42s/it]Best trial: 4. Best value: 0.969465:  31%|███       | 31/100 [1:17:20<3:34:07, 186.20s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                          Best trial: 4. Best value: 0.969465:  31%|███       | 31/100 [1:17:21<3:34:07, 186.20s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 31.6 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
31.6 M    Trainable params
0         Non-trainable params
31.6 M    Total params
126.234   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.384
Metric Loss/val improved by 0.329 >= min_delta = 0.0. New best score: 1.055
Metric Loss/val improved by 0.024 >= min_delta = 0.0. New best score: 1.032
Metric Loss/val improved by 0.003 >= min_delta = 0.0. New best score: 1.028
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.028. Signaling Trainer to stop.
                                                                                          Best trial: 4. Best value: 0.969465:  31%|███       | 31/100 [1:20:24<3:34:07, 186.20s/it]Best trial: 4. Best value: 0.969465:  31%|███       | 31/100 [1:20:24<3:34:07, 186.20s/it]Best trial: 4. Best value: 0.969465:  32%|███▏      | 32/100 [1:20:24<3:30:01, 185.32s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                          Best trial: 4. Best value: 0.969465:  32%|███▏      | 32/100 [1:20:24<3:30:01, 185.32s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=8, input_dim=84, dec_in=84, output_dim=1, d_model=384, n_heads=4, e_layers=8, hidden_d_model=144, seq_layers=2, last_d_model=576, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=64, pos_d_model=64, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.22, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=288), exp_settings='seq_len-12-lr-0.0915-d-384-hid_d-144-last_d-576-tok_d-64-time_d-64-pos_d-64-e_layers-8-tok_conv_k-8-dropout-0.22-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.09154185944487196, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 00:55:39,296] Trial 62 finished with value: 0.9967327117919923 and parameters: {'num_heads': 36, 'hidden_d_model': 144, 'token_conv_kernel': 8, 'last_d_model': 576, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 64, 'pos_d_model': 64, 'd_model': 384, 'conv_out_dim': 288, 'e_layers': 8, 'learning_rate': 0.09154185944487196, 'dropout': 0.22, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 00:55:39,553] The parameter 'norm_type' in trial#64 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=480, n_heads=4, e_layers=8, hidden_d_model=208, seq_layers=2, last_d_model=768, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=64, pos_d_model=112, combine_type='add', seq_len=10, pred_len=1, min_y_value=0.0, dropout=0.12000000000000001, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=320), exp_settings='seq_len-10-lr-0.011-d-480-hid_d-208-last_d-768-tok_d-64-time_d-64-pos_d-112-e_layers-8-tok_conv_k-9-dropout-0.12000000000000001-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.011014470430189697, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 00:58:42,580] Trial 64 finished with value: 0.9946966409683228 and parameters: {'num_heads': 36, 'hidden_d_model': 208, 'token_conv_kernel': 9, 'last_d_model': 768, 'seq_len': 10, 'token_d_model': 64, 'time_d_model': 64, 'pos_d_model': 112, 'd_model': 480, 'conv_out_dim': 320, 'e_layers': 8, 'learning_rate': 0.011014470430189697, 'dropout': 0.12000000000000001, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 00:58:42,830] The parameter 'norm_type' in trial#66 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 18.2 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
18.2 M    Trainable params
0         Non-trainable params
18.2 M    Total params
72.914    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.891
Metric Loss/val improved by 0.798 >= min_delta = 0.0. New best score: 1.093
Metric Loss/val improved by 0.052 >= min_delta = 0.0. New best score: 1.041
Metric Loss/val improved by 0.012 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                          Best trial: 4. Best value: 0.969465:  32%|███▏      | 32/100 [1:22:46<3:30:01, 185.32s/it]Best trial: 4. Best value: 0.969465:  32%|███▏      | 32/100 [1:22:46<3:30:01, 185.32s/it]Best trial: 4. Best value: 0.969465:  33%|███▎      | 33/100 [1:22:46<3:12:27, 172.35s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                          Best trial: 4. Best value: 0.969465:  33%|███▎      | 33/100 [1:22:46<3:12:27, 172.35s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 18.1 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
18.1 M    Trainable params
0         Non-trainable params
18.1 M    Total params
72.474    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.414
Metric Loss/val improved by 0.090 >= min_delta = 0.0. New best score: 1.324
Metric Loss/val improved by 0.225 >= min_delta = 0.0. New best score: 1.098
Metric Loss/val improved by 0.070 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                          Best trial: 4. Best value: 0.969465:  33%|███▎      | 33/100 [1:24:46<3:12:27, 172.35s/it]Best trial: 4. Best value: 0.969465:  33%|███▎      | 33/100 [1:24:46<3:12:27, 172.35s/it]Best trial: 4. Best value: 0.969465:  34%|███▍      | 34/100 [1:24:46<2:52:22, 156.70s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                          Best trial: 4. Best value: 0.969465:  34%|███▍      | 34/100 [1:24:46<2:52:22, 156.70s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=8, input_dim=84, dec_in=84, output_dim=1, d_model=384, n_heads=4, e_layers=4, hidden_d_model=208, seq_layers=2, last_d_model=672, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=80, pos_d_model=80, combine_type='add', seq_len=10, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=288), exp_settings='seq_len-10-lr-0.0654-d-384-hid_d-208-last_d-672-tok_d-64-time_d-80-pos_d-80-e_layers-4-tok_conv_k-8-dropout-0.14-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.06537724834346126, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 01:01:04,652] Trial 66 finished with value: 0.9988820552825928 and parameters: {'num_heads': 36, 'hidden_d_model': 208, 'token_conv_kernel': 8, 'last_d_model': 672, 'seq_len': 10, 'token_d_model': 64, 'time_d_model': 80, 'pos_d_model': 80, 'd_model': 384, 'conv_out_dim': 288, 'e_layers': 4, 'learning_rate': 0.06537724834346126, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 01:01:04,908] The parameter 'norm_type' in trial#68 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=8, input_dim=84, dec_in=84, output_dim=1, d_model=352, n_heads=4, e_layers=6, hidden_d_model=208, seq_layers=2, last_d_model=960, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=80, pos_d_model=80, combine_type='add', seq_len=14, pred_len=1, min_y_value=0.0, dropout=0.22, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=256), exp_settings='seq_len-14-lr-0.0168-d-352-hid_d-208-last_d-960-tok_d-64-time_d-80-pos_d-80-e_layers-6-tok_conv_k-8-dropout-0.22-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.016810504721220972, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 01:03:04,831] Trial 68 finished with value: 1.037346076965332 and parameters: {'num_heads': 36, 'hidden_d_model': 208, 'token_conv_kernel': 8, 'last_d_model': 960, 'seq_len': 14, 'token_d_model': 64, 'time_d_model': 80, 'pos_d_model': 80, 'd_model': 352, 'conv_out_dim': 256, 'e_layers': 6, 'learning_rate': 0.016810504721220972, 'dropout': 0.22, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 01:03:05,195] The parameter 'norm_type' in trial#70 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 13.1 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
13.1 M    Trainable params
0         Non-trainable params
13.1 M    Total params
52.279    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.123
Metric Loss/val improved by 0.044 >= min_delta = 0.0. New best score: 1.078
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 1.076
Metric Loss/val improved by 0.047 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                          Best trial: 4. Best value: 0.969465:  34%|███▍      | 34/100 [1:26:43<2:52:22, 156.70s/it]Best trial: 4. Best value: 0.969465:  34%|███▍      | 34/100 [1:26:43<2:52:22, 156.70s/it]Best trial: 4. Best value: 0.969465:  35%|███▌      | 35/100 [1:26:43<2:36:45, 144.70s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                          Best trial: 4. Best value: 0.969465:  35%|███▌      | 35/100 [1:26:43<2:36:45, 144.70s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 21.6 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
21.6 M    Trainable params
0         Non-trainable params
21.6 M    Total params
86.495    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.057
Metric Loss/val improved by 0.006 >= min_delta = 0.0. New best score: 1.051
Metric Loss/val improved by 0.022 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                          Best trial: 4. Best value: 0.969465:  35%|███▌      | 35/100 [1:29:21<2:36:45, 144.70s/it]Best trial: 4. Best value: 0.969465:  35%|███▌      | 35/100 [1:29:21<2:36:45, 144.70s/it]Best trial: 4. Best value: 0.969465:  36%|███▌      | 36/100 [1:29:21<2:38:35, 148.69s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                          Best trial: 4. Best value: 0.969465:  36%|███▌      | 36/100 [1:29:21<2:38:35, 148.69s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=8, input_dim=84, dec_in=84, output_dim=1, d_model=192, n_heads=4, e_layers=8, hidden_d_model=192, seq_layers=2, last_d_model=1216, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=64, pos_d_model=96, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.22, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=224), exp_settings='seq_len-12-lr-0.0135-d-192-hid_d-192-last_d-1216-tok_d-64-time_d-64-pos_d-96-e_layers-8-tok_conv_k-8-dropout-0.22-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.013515964367858088, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 01:05:01,539] Trial 70 finished with value: 2.160101103782654 and parameters: {'num_heads': 36, 'hidden_d_model': 192, 'token_conv_kernel': 8, 'last_d_model': 1216, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 64, 'pos_d_model': 96, 'd_model': 192, 'conv_out_dim': 224, 'e_layers': 8, 'learning_rate': 0.013515964367858088, 'dropout': 0.22, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 01:05:01,823] The parameter 'norm_type' in trial#71 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=8, input_dim=84, dec_in=84, output_dim=1, d_model=320, n_heads=4, e_layers=8, hidden_d_model=160, seq_layers=2, last_d_model=1056, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=112, pos_d_model=64, combine_type='add', seq_len=14, pred_len=1, min_y_value=0.0, dropout=0.24000000000000002, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=320), exp_settings='seq_len-14-lr-0.0086-d-320-hid_d-160-last_d-1056-tok_d-64-time_d-112-pos_d-64-e_layers-8-tok_conv_k-8-dropout-0.24000000000000002-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.008626614829876743, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 01:07:39,525] Trial 71 finished with value: 1.0706692934036255 and parameters: {'num_heads': 36, 'hidden_d_model': 160, 'token_conv_kernel': 8, 'last_d_model': 1056, 'seq_len': 14, 'token_d_model': 64, 'time_d_model': 112, 'pos_d_model': 64, 'd_model': 320, 'conv_out_dim': 320, 'e_layers': 8, 'learning_rate': 0.008626614829876743, 'dropout': 0.24000000000000002, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 01:07:39,823] The parameter 'norm_type' in trial#73 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 21.5 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
21.5 M    Trainable params
0         Non-trainable params
21.5 M    Total params
85.949    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.055
Metric Loss/val improved by 0.008 >= min_delta = 0.0. New best score: 1.047
Metric Loss/val improved by 0.018 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                          Best trial: 4. Best value: 0.969465:  36%|███▌      | 36/100 [1:31:58<2:38:35, 148.69s/it]Best trial: 4. Best value: 0.969465:  36%|███▌      | 36/100 [1:31:58<2:38:35, 148.69s/it]Best trial: 4. Best value: 0.969465:  37%|███▋      | 37/100 [1:31:58<2:38:43, 151.17s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                          Best trial: 4. Best value: 0.969465:  37%|███▋      | 37/100 [1:31:58<2:38:43, 151.17s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 22.2 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
22.2 M    Trainable params
0         Non-trainable params
22.2 M    Total params
88.696    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                          Best trial: 4. Best value: 0.969465:  37%|███▋      | 37/100 [1:33:48<2:38:43, 151.17s/it]Best trial: 4. Best value: 0.969465:  37%|███▋      | 37/100 [1:33:48<2:38:43, 151.17s/it]Best trial: 4. Best value: 0.969465:  38%|███▊      | 38/100 [1:33:48<2:23:35, 138.96s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                          Best trial: 4. Best value: 0.969465:  38%|███▊      | 38/100 [1:33:48<2:23:35, 138.96s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=320, n_heads=4, e_layers=8, hidden_d_model=176, seq_layers=2, last_d_model=576, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=96, pos_d_model=80, combine_type='add', seq_len=10, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=256), exp_settings='seq_len-10-lr-0.0103-d-320-hid_d-176-last_d-576-tok_d-64-time_d-96-pos_d-80-e_layers-8-tok_conv_k-10-dropout-0.14-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.010276029917481756, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 01:10:16,509] Trial 73 finished with value: 0.9963407039642334 and parameters: {'num_heads': 36, 'hidden_d_model': 176, 'token_conv_kernel': 10, 'last_d_model': 576, 'seq_len': 10, 'token_d_model': 64, 'time_d_model': 96, 'pos_d_model': 80, 'd_model': 320, 'conv_out_dim': 256, 'e_layers': 8, 'learning_rate': 0.010276029917481756, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 01:10:16,817] The parameter 'norm_type' in trial#75 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=8, input_dim=84, dec_in=84, output_dim=1, d_model=448, n_heads=4, e_layers=6, hidden_d_model=192, seq_layers=2, last_d_model=576, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=48, time_d_model=96, pos_d_model=80, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.12000000000000001, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=352), exp_settings='seq_len-12-lr-0.0037-d-448-hid_d-192-last_d-576-tok_d-48-time_d-96-pos_d-80-e_layers-6-tok_conv_k-8-dropout-0.12000000000000001-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0036628324217297063, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 01:12:06,963] Trial 75 finished with value: 0.9931733369827271 and parameters: {'num_heads': 36, 'hidden_d_model': 192, 'token_conv_kernel': 8, 'last_d_model': 576, 'seq_len': 12, 'token_d_model': 48, 'time_d_model': 96, 'pos_d_model': 80, 'd_model': 448, 'conv_out_dim': 352, 'e_layers': 6, 'learning_rate': 0.0036628324217297063, 'dropout': 0.12000000000000001, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 01:12:07,198] The parameter 'norm_type' in trial#77 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 27.3 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
27.3 M    Trainable params
0         Non-trainable params
27.3 M    Total params
109.393   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.100
Metric Loss/val improved by 0.051 >= min_delta = 0.0. New best score: 1.049
Metric Loss/val improved by 0.016 >= min_delta = 0.0. New best score: 1.033
Metric Loss/val improved by 0.003 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                          Best trial: 4. Best value: 0.969465:  38%|███▊      | 38/100 [1:36:56<2:23:35, 138.96s/it]Best trial: 4. Best value: 0.969465:  38%|███▊      | 38/100 [1:36:56<2:23:35, 138.96s/it]Best trial: 4. Best value: 0.969465:  39%|███▉      | 39/100 [1:36:56<2:36:10, 153.62s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                          Best trial: 4. Best value: 0.969465:  39%|███▉      | 39/100 [1:36:56<2:36:10, 153.62s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 26.1 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
26.1 M    Trainable params
0         Non-trainable params
26.1 M    Total params
104.576   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.612
Metric Loss/val improved by 0.567 >= min_delta = 0.0. New best score: 1.044
Metric Loss/val improved by 0.015 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                          Best trial: 4. Best value: 0.969465:  39%|███▉      | 39/100 [1:40:18<2:36:10, 153.62s/it]Best trial: 4. Best value: 0.969465:  39%|███▉      | 39/100 [1:40:18<2:36:10, 153.62s/it]Best trial: 4. Best value: 0.969465:  40%|████      | 40/100 [1:40:18<2:48:05, 168.09s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                          Best trial: 4. Best value: 0.969465:  40%|████      | 40/100 [1:40:18<2:48:05, 168.09s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=8, input_dim=84, dec_in=84, output_dim=1, d_model=416, n_heads=4, e_layers=6, hidden_d_model=192, seq_layers=2, last_d_model=256, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=80, pos_d_model=80, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.24000000000000002, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=68, fc_layer_type='mha', conv_out_dim=416), exp_settings='seq_len-12-lr-0.0106-d-416-hid_d-192-last_d-256-tok_d-64-time_d-80-pos_d-80-e_layers-6-tok_conv_k-8-dropout-0.24000000000000002-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-68-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.01055189905903297, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 01:15:14,779] Trial 77 finished with value: 1.001915729045868 and parameters: {'num_heads': 68, 'hidden_d_model': 192, 'token_conv_kernel': 8, 'last_d_model': 256, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 80, 'pos_d_model': 80, 'd_model': 416, 'conv_out_dim': 416, 'e_layers': 6, 'learning_rate': 0.01055189905903297, 'dropout': 0.24000000000000002, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 01:15:15,227] The parameter 'norm_type' in trial#79 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=8, input_dim=84, dec_in=84, output_dim=1, d_model=448, n_heads=4, e_layers=6, hidden_d_model=208, seq_layers=2, last_d_model=640, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=80, pos_d_model=112, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.18, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=352), exp_settings='seq_len-8-lr-0.0124-d-448-hid_d-208-last_d-640-tok_d-64-time_d-80-pos_d-112-e_layers-6-tok_conv_k-8-dropout-0.18-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.012403548401070746, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 01:18:36,648] Trial 79 finished with value: 0.9955759882926942 and parameters: {'num_heads': 36, 'hidden_d_model': 208, 'token_conv_kernel': 8, 'last_d_model': 640, 'seq_len': 8, 'token_d_model': 64, 'time_d_model': 80, 'pos_d_model': 112, 'd_model': 448, 'conv_out_dim': 352, 'e_layers': 6, 'learning_rate': 0.012403548401070746, 'dropout': 0.18, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 01:18:36,943] The parameter 'norm_type' in trial#81 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 18.5 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
18.5 M    Trainable params
0         Non-trainable params
18.5 M    Total params
74.047    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.562
Metric Loss/val improved by 0.502 >= min_delta = 0.0. New best score: 1.060
Metric Loss/val improved by 0.028 >= min_delta = 0.0. New best score: 1.032
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.031
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                          Best trial: 4. Best value: 0.969465:  40%|████      | 40/100 [1:43:14<2:48:05, 168.09s/it]Best trial: 4. Best value: 0.969465:  40%|████      | 40/100 [1:43:14<2:48:05, 168.09s/it]Best trial: 4. Best value: 0.969465:  41%|████      | 41/100 [1:43:14<2:47:35, 170.43s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                          Best trial: 4. Best value: 0.969465:  41%|████      | 41/100 [1:43:14<2:47:35, 170.43s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 18.1 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
18.1 M    Trainable params
0         Non-trainable params
18.1 M    Total params
72.429    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.080
Metric Loss/val improved by 0.037 >= min_delta = 0.0. New best score: 1.044
Metric Loss/val improved by 0.006 >= min_delta = 0.0. New best score: 1.037
Metric Loss/val improved by 0.007 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                          Best trial: 4. Best value: 0.969465:  41%|████      | 41/100 [1:46:11<2:47:35, 170.43s/it]Best trial: 4. Best value: 0.969465:  41%|████      | 41/100 [1:46:11<2:47:35, 170.43s/it]Best trial: 4. Best value: 0.969465:  42%|████▏     | 42/100 [1:46:11<2:46:41, 172.44s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                          Best trial: 4. Best value: 0.969465:  42%|████▏     | 42/100 [1:46:11<2:46:41, 172.44s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=320, n_heads=4, e_layers=6, hidden_d_model=224, seq_layers=2, last_d_model=992, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=80, pos_d_model=80, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.18, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=256), exp_settings='seq_len-12-lr-0.0274-d-320-hid_d-224-last_d-992-tok_d-64-time_d-80-pos_d-80-e_layers-6-tok_conv_k-9-dropout-0.18-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.027366636994432467, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 01:21:32,551] Trial 81 finished with value: 1.0568965435028077 and parameters: {'num_heads': 36, 'hidden_d_model': 224, 'token_conv_kernel': 9, 'last_d_model': 992, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 80, 'pos_d_model': 80, 'd_model': 320, 'conv_out_dim': 256, 'e_layers': 6, 'learning_rate': 0.027366636994432467, 'dropout': 0.18, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 01:21:32,825] The parameter 'norm_type' in trial#84 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=256, n_heads=4, e_layers=6, hidden_d_model=208, seq_layers=2, last_d_model=992, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=64, pos_d_model=112, combine_type='add', seq_len=10, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=256), exp_settings='seq_len-10-lr-0.0167-d-256-hid_d-208-last_d-992-tok_d-64-time_d-64-pos_d-112-e_layers-6-tok_conv_k-10-dropout-0.16-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.016663823071646662, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 01:24:29,656] Trial 84 finished with value: 0.9964043378829956 and parameters: {'num_heads': 36, 'hidden_d_model': 208, 'token_conv_kernel': 10, 'last_d_model': 992, 'seq_len': 10, 'token_d_model': 64, 'time_d_model': 64, 'pos_d_model': 112, 'd_model': 256, 'conv_out_dim': 256, 'e_layers': 6, 'learning_rate': 0.016663823071646662, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 01:24:29,967] The parameter 'norm_type' in trial#86 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 20.0 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
20.0 M    Trainable params
0         Non-trainable params
20.0 M    Total params
80.017    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.140
Metric Loss/val improved by 0.031 >= min_delta = 0.0. New best score: 1.109
Metric Loss/val improved by 0.075 >= min_delta = 0.0. New best score: 1.034
Metric Loss/val improved by 0.005 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                          Best trial: 4. Best value: 0.969465:  42%|████▏     | 42/100 [1:49:27<2:46:41, 172.44s/it]Best trial: 4. Best value: 0.969465:  42%|████▏     | 42/100 [1:49:27<2:46:41, 172.44s/it]Best trial: 4. Best value: 0.969465:  43%|████▎     | 43/100 [1:49:27<2:50:40, 179.67s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                          Best trial: 4. Best value: 0.969465:  43%|████▎     | 43/100 [1:49:28<2:50:40, 179.67s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 21.1 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
21.1 M    Trainable params
0         Non-trainable params
21.1 M    Total params
84.246    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.091
Metric Loss/val improved by 0.010 >= min_delta = 0.0. New best score: 1.082
Metric Loss/val improved by 0.035 >= min_delta = 0.0. New best score: 1.047
Metric Loss/val improved by 0.017 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                          Best trial: 4. Best value: 0.969465:  43%|████▎     | 43/100 [1:52:17<2:50:40, 179.67s/it]Best trial: 4. Best value: 0.969465:  43%|████▎     | 43/100 [1:52:17<2:50:40, 179.67s/it]Best trial: 4. Best value: 0.969465:  44%|████▍     | 44/100 [1:52:17<2:44:51, 176.63s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                          Best trial: 4. Best value: 0.969465:  44%|████▍     | 44/100 [1:52:17<2:44:51, 176.63s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=320, n_heads=4, e_layers=6, hidden_d_model=176, seq_layers=2, last_d_model=960, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=32, pos_d_model=80, combine_type='add', seq_len=10, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=288), exp_settings='seq_len-10-lr-0.0122-d-320-hid_d-176-last_d-960-tok_d-64-time_d-32-pos_d-80-e_layers-6-tok_conv_k-9-dropout-0.16-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.012236210533731386, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 01:27:46,195] Trial 86 finished with value: 1.1074894666671753 and parameters: {'num_heads': 36, 'hidden_d_model': 176, 'token_conv_kernel': 9, 'last_d_model': 960, 'seq_len': 10, 'token_d_model': 64, 'time_d_model': 32, 'pos_d_model': 80, 'd_model': 320, 'conv_out_dim': 288, 'e_layers': 6, 'learning_rate': 0.012236210533731386, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 01:27:46,485] The parameter 'norm_type' in trial#88 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=384, n_heads=4, e_layers=6, hidden_d_model=224, seq_layers=2, last_d_model=416, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=48, time_d_model=80, pos_d_model=96, combine_type='add', seq_len=10, pred_len=1, min_y_value=0.0, dropout=0.22, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=352), exp_settings='seq_len-10-lr-0.0121-d-384-hid_d-224-last_d-416-tok_d-48-time_d-80-pos_d-96-e_layers-6-tok_conv_k-9-dropout-0.22-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.01212716064108686, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 01:30:35,734] Trial 88 finished with value: 0.9962685585021973 and parameters: {'num_heads': 36, 'hidden_d_model': 224, 'token_conv_kernel': 9, 'last_d_model': 416, 'seq_len': 10, 'token_d_model': 48, 'time_d_model': 80, 'pos_d_model': 96, 'd_model': 384, 'conv_out_dim': 352, 'e_layers': 6, 'learning_rate': 0.01212716064108686, 'dropout': 0.22, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 01:30:36,004] The parameter 'norm_type' in trial#90 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 23.3 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
23.3 M    Trainable params
0         Non-trainable params
23.3 M    Total params
93.079    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.054
Metric Loss/val improved by 0.014 >= min_delta = 0.0. New best score: 1.040
Metric Loss/val improved by 0.010 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                          Best trial: 4. Best value: 0.969465:  44%|████▍     | 44/100 [1:55:46<2:44:51, 176.63s/it]Best trial: 4. Best value: 0.969465:  44%|████▍     | 44/100 [1:55:46<2:44:51, 176.63s/it]Best trial: 4. Best value: 0.969465:  45%|████▌     | 45/100 [1:55:46<2:50:54, 186.44s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                          Best trial: 4. Best value: 0.969465:  45%|████▌     | 45/100 [1:55:46<2:50:54, 186.44s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 15.2 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
15.2 M    Trainable params
0         Non-trainable params
15.2 M    Total params
60.898    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 7.214
Metric Loss/val improved by 6.025 >= min_delta = 0.0. New best score: 1.190
Metric Loss/val improved by 0.135 >= min_delta = 0.0. New best score: 1.054
Metric Loss/val improved by 0.008 >= min_delta = 0.0. New best score: 1.046
Metric Loss/val improved by 0.004 >= min_delta = 0.0. New best score: 1.041
Metric Loss/val improved by 0.012 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                          Best trial: 4. Best value: 0.969465:  45%|████▌     | 45/100 [1:59:11<2:50:54, 186.44s/it]Best trial: 4. Best value: 0.969465:  45%|████▌     | 45/100 [1:59:11<2:50:54, 186.44s/it]Best trial: 4. Best value: 0.969465:  46%|████▌     | 46/100 [1:59:11<2:52:52, 192.08s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                          Best trial: 4. Best value: 0.969465:  46%|████▌     | 46/100 [1:59:12<2:52:52, 192.08s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=11, input_dim=84, dec_in=84, output_dim=1, d_model=480, n_heads=4, e_layers=6, hidden_d_model=192, seq_layers=2, last_d_model=672, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=80, pos_d_model=80, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=192), exp_settings='seq_len-12-lr-0.0059-d-480-hid_d-192-last_d-672-tok_d-64-time_d-80-pos_d-80-e_layers-6-tok_conv_k-11-dropout-0.16-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.005852840238703093, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 01:34:05,074] Trial 90 finished with value: 0.9914110779762269 and parameters: {'num_heads': 36, 'hidden_d_model': 192, 'token_conv_kernel': 11, 'last_d_model': 672, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 80, 'pos_d_model': 80, 'd_model': 480, 'conv_out_dim': 192, 'e_layers': 6, 'learning_rate': 0.005852840238703093, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 01:34:05,372] The parameter 'norm_type' in trial#92 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=7, input_dim=84, dec_in=84, output_dim=1, d_model=384, n_heads=4, e_layers=6, hidden_d_model=224, seq_layers=2, last_d_model=896, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=80, pos_d_model=80, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=192), exp_settings='seq_len-12-lr-0.0346-d-384-hid_d-224-last_d-896-tok_d-64-time_d-80-pos_d-80-e_layers-6-tok_conv_k-7-dropout-0.16-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.03455634865527529, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 01:37:30,301] Trial 92 finished with value: 0.9980485439300537 and parameters: {'num_heads': 36, 'hidden_d_model': 224, 'token_conv_kernel': 7, 'last_d_model': 896, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 80, 'pos_d_model': 80, 'd_model': 384, 'conv_out_dim': 192, 'e_layers': 6, 'learning_rate': 0.03455634865527529, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 4 with value: 0.9694645166397096.
[W 2024-08-14 01:37:30,608] The parameter 'norm_type' in trial#94 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 19.0 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
19.0 M    Trainable params
0         Non-trainable params
19.0 M    Total params
75.839    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.056
Metric Loss/val improved by 0.026 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                          Best trial: 4. Best value: 0.969465:  46%|████▌     | 46/100 [2:02:19<2:52:52, 192.08s/it]Best trial: 93. Best value: 0.969243:  46%|████▌     | 46/100 [2:02:19<2:52:52, 192.08s/it]Best trial: 93. Best value: 0.969243:  47%|████▋     | 47/100 [2:02:19<2:48:33, 190.82s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  47%|████▋     | 47/100 [2:02:20<2:48:33, 190.82s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 26.4 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
26.4 M    Trainable params
0         Non-trainable params
26.4 M    Total params
105.789   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.068
Metric Loss/val improved by 0.003 >= min_delta = 0.0. New best score: 1.065
Metric Loss/val improved by 0.035 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                           Best trial: 93. Best value: 0.969243:  47%|████▋     | 47/100 [2:05:02<2:48:33, 190.82s/it]Best trial: 93. Best value: 0.969243:  47%|████▋     | 47/100 [2:05:02<2:48:33, 190.82s/it]Best trial: 93. Best value: 0.969243:  48%|████▊     | 48/100 [2:05:02<2:38:11, 182.54s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  48%|████▊     | 48/100 [2:05:03<2:38:11, 182.54s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=7, input_dim=84, dec_in=84, output_dim=1, d_model=448, n_heads=4, e_layers=6, hidden_d_model=192, seq_layers=2, last_d_model=800, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=64, pos_d_model=80, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=68, fc_layer_type='mha', conv_out_dim=224), exp_settings='seq_len-12-lr-0.0099-d-448-hid_d-192-last_d-800-tok_d-64-time_d-64-pos_d-80-e_layers-6-tok_conv_k-7-dropout-0.16-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-68-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.009933396614806553, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 01:40:38,185] Trial 94 finished with value: 0.9959906935691833 and parameters: {'num_heads': 68, 'hidden_d_model': 192, 'token_conv_kernel': 7, 'last_d_model': 800, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 64, 'pos_d_model': 80, 'd_model': 448, 'conv_out_dim': 224, 'e_layers': 6, 'learning_rate': 0.009933396614806553, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 01:40:38,451] The parameter 'norm_type' in trial#97 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=8, input_dim=84, dec_in=84, output_dim=1, d_model=448, n_heads=4, e_layers=8, hidden_d_model=192, seq_layers=2, last_d_model=736, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=80, pos_d_model=112, combine_type='add', seq_len=10, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=4, fc_layer_type='mha', conv_out_dim=288), exp_settings='seq_len-10-lr-0.0138-d-448-hid_d-192-last_d-736-tok_d-64-time_d-80-pos_d-112-e_layers-8-tok_conv_k-8-dropout-0.14-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-4-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.013810149985340928, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 01:43:21,398] Trial 97 finished with value: 1.000610876083374 and parameters: {'num_heads': 4, 'hidden_d_model': 192, 'token_conv_kernel': 8, 'last_d_model': 736, 'seq_len': 10, 'token_d_model': 64, 'time_d_model': 80, 'pos_d_model': 112, 'd_model': 448, 'conv_out_dim': 288, 'e_layers': 8, 'learning_rate': 0.013810149985340928, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 01:43:21,705] The parameter 'norm_type' in trial#98 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 12.1 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
12.1 M    Trainable params
0         Non-trainable params
12.1 M    Total params
48.268    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 4.262
Metric Loss/val improved by 3.231 >= min_delta = 0.0. New best score: 1.031
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                           Best trial: 93. Best value: 0.969243:  48%|████▊     | 48/100 [2:07:42<2:38:11, 182.54s/it]Best trial: 93. Best value: 0.969243:  48%|████▊     | 48/100 [2:07:42<2:38:11, 182.54s/it]Best trial: 93. Best value: 0.969243:  49%|████▉     | 49/100 [2:07:42<2:29:22, 175.73s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  49%|████▉     | 49/100 [2:07:43<2:29:22, 175.73s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 20.3 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
20.3 M    Trainable params
0         Non-trainable params
20.3 M    Total params
81.197    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 8.329
Metric Loss/val improved by 5.606 >= min_delta = 0.0. New best score: 2.723
Metric Loss/val improved by 1.690 >= min_delta = 0.0. New best score: 1.032
Metric Loss/val improved by 0.004 >= min_delta = 0.0. New best score: 1.028
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.028. Signaling Trainer to stop.
                                                                                           Best trial: 93. Best value: 0.969243:  49%|████▉     | 49/100 [2:09:27<2:29:22, 175.73s/it]Best trial: 93. Best value: 0.969243:  49%|████▉     | 49/100 [2:09:27<2:29:22, 175.73s/it]Best trial: 93. Best value: 0.969243:  50%|█████     | 50/100 [2:09:27<2:08:38, 154.37s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  50%|█████     | 50/100 [2:09:27<2:08:38, 154.37s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=7, input_dim=84, dec_in=84, output_dim=1, d_model=320, n_heads=4, e_layers=6, hidden_d_model=176, seq_layers=2, last_d_model=736, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=48, time_d_model=80, pos_d_model=64, combine_type='add', seq_len=10, pred_len=1, min_y_value=0.0, dropout=0.18, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=224), exp_settings='seq_len-10-lr-0.0301-d-320-hid_d-176-last_d-736-tok_d-48-time_d-80-pos_d-64-e_layers-6-tok_conv_k-7-dropout-0.18-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.03005988555158175, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 01:46:01,247] Trial 98 finished with value: 0.9966306686401367 and parameters: {'num_heads': 36, 'hidden_d_model': 176, 'token_conv_kernel': 7, 'last_d_model': 736, 'seq_len': 10, 'token_d_model': 48, 'time_d_model': 80, 'pos_d_model': 64, 'd_model': 320, 'conv_out_dim': 224, 'e_layers': 6, 'learning_rate': 0.03005988555158175, 'dropout': 0.18, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 01:46:01,499] The parameter 'norm_type' in trial#100 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=480, n_heads=4, e_layers=4, hidden_d_model=192, seq_layers=2, last_d_model=512, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=96, pos_d_model=80, combine_type='add', seq_len=10, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=224), exp_settings='seq_len-10-lr-0.0269-d-480-hid_d-192-last_d-512-tok_d-64-time_d-96-pos_d-80-e_layers-4-tok_conv_k-10-dropout-0.14-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.02694402774133318, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 01:47:45,772] Trial 100 finished with value: 0.9987376928329468 and parameters: {'num_heads': 36, 'hidden_d_model': 192, 'token_conv_kernel': 10, 'last_d_model': 512, 'seq_len': 10, 'token_d_model': 64, 'time_d_model': 96, 'pos_d_model': 80, 'd_model': 480, 'conv_out_dim': 224, 'e_layers': 4, 'learning_rate': 0.02694402774133318, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 01:47:46,058] The parameter 'norm_type' in trial#102 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 26.2 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
26.2 M    Trainable params
0         Non-trainable params
26.2 M    Total params
104.781   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.028
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.028. Signaling Trainer to stop.
                                                                                           Best trial: 93. Best value: 0.969243:  50%|█████     | 50/100 [2:11:32<2:08:38, 154.37s/it]Best trial: 93. Best value: 0.969243:  50%|█████     | 50/100 [2:11:32<2:08:38, 154.37s/it]Best trial: 93. Best value: 0.969243:  51%|█████     | 51/100 [2:11:32<1:58:54, 145.59s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  51%|█████     | 51/100 [2:11:32<1:58:54, 145.59s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 21.9 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
21.9 M    Trainable params
0         Non-trainable params
21.9 M    Total params
87.584    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.050
Metric Loss/val improved by 0.014 >= min_delta = 0.0. New best score: 1.036
Metric Loss/val improved by 0.004 >= min_delta = 0.0. New best score: 1.032
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                           Best trial: 93. Best value: 0.969243:  51%|█████     | 51/100 [2:15:41<1:58:54, 145.59s/it]Best trial: 93. Best value: 0.969243:  51%|█████     | 51/100 [2:15:41<1:58:54, 145.59s/it]Best trial: 93. Best value: 0.969243:  52%|█████▏    | 52/100 [2:15:41<2:21:18, 176.64s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  52%|█████▏    | 52/100 [2:15:42<2:21:18, 176.64s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=8, input_dim=84, dec_in=84, output_dim=1, d_model=512, n_heads=4, e_layers=6, hidden_d_model=160, seq_layers=2, last_d_model=736, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=128, pos_d_model=96, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=288), exp_settings='seq_len-12-lr-0.0042-d-512-hid_d-160-last_d-736-tok_d-64-time_d-128-pos_d-96-e_layers-6-tok_conv_k-8-dropout-0.16-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.004235665300985824, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 01:49:50,882] Trial 102 finished with value: 0.996979308128357 and parameters: {'num_heads': 36, 'hidden_d_model': 160, 'token_conv_kernel': 8, 'last_d_model': 736, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 128, 'pos_d_model': 96, 'd_model': 512, 'conv_out_dim': 288, 'e_layers': 6, 'learning_rate': 0.004235665300985824, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 01:49:51,166] The parameter 'norm_type' in trial#103 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=416, n_heads=4, e_layers=6, hidden_d_model=208, seq_layers=2, last_d_model=896, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=112, pos_d_model=64, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.18, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=68, fc_layer_type='mha', conv_out_dim=256), exp_settings='seq_len-12-lr-0.0093-d-416-hid_d-208-last_d-896-tok_d-64-time_d-112-pos_d-64-e_layers-6-tok_conv_k-9-dropout-0.18-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-68-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.009337049396403315, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 01:53:59,981] Trial 103 finished with value: 0.9886790037155152 and parameters: {'num_heads': 68, 'hidden_d_model': 208, 'token_conv_kernel': 9, 'last_d_model': 896, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 112, 'pos_d_model': 64, 'd_model': 416, 'conv_out_dim': 256, 'e_layers': 6, 'learning_rate': 0.009337049396403315, 'dropout': 0.18, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 01:54:00,541] The parameter 'norm_type' in trial#106 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 18.1 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
18.1 M    Trainable params
0         Non-trainable params
18.1 M    Total params
72.364    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 10.000
Metric Loss/val improved by 6.768 >= min_delta = 0.0. New best score: 3.232
Metric Loss/val improved by 2.199 >= min_delta = 0.0. New best score: 1.033
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 1.031
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                           Best trial: 93. Best value: 0.969243:  52%|█████▏    | 52/100 [2:18:12<2:21:18, 176.64s/it]Best trial: 93. Best value: 0.969243:  52%|█████▏    | 52/100 [2:18:12<2:21:18, 176.64s/it]Best trial: 93. Best value: 0.969243:  53%|█████▎    | 53/100 [2:18:12<2:12:22, 168.98s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  53%|█████▎    | 53/100 [2:18:12<2:12:22, 168.98s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 17.7 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
17.7 M    Trainable params
0         Non-trainable params
17.7 M    Total params
70.779    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.038
Metric Loss/val improved by 0.007 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                           Best trial: 93. Best value: 0.969243:  53%|█████▎    | 53/100 [2:21:08<2:12:22, 168.98s/it]Best trial: 93. Best value: 0.969243:  53%|█████▎    | 53/100 [2:21:08<2:12:22, 168.98s/it]Best trial: 93. Best value: 0.969243:  54%|█████▍    | 54/100 [2:21:08<2:11:10, 171.10s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  54%|█████▍    | 54/100 [2:21:09<2:11:10, 171.10s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=384, n_heads=4, e_layers=8, hidden_d_model=176, seq_layers=2, last_d_model=736, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=112, pos_d_model=96, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=160), exp_settings='seq_len-12-lr-0.0509-d-384-hid_d-176-last_d-736-tok_d-64-time_d-112-pos_d-96-e_layers-8-tok_conv_k-9-dropout-0.16-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.05087395178763718, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 01:56:31,090] Trial 106 finished with value: 0.9976067543029785 and parameters: {'num_heads': 36, 'hidden_d_model': 176, 'token_conv_kernel': 9, 'last_d_model': 736, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 112, 'pos_d_model': 96, 'd_model': 384, 'conv_out_dim': 160, 'e_layers': 8, 'learning_rate': 0.05087395178763718, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 01:56:31,346] The parameter 'norm_type' in trial#108 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=8, input_dim=84, dec_in=84, output_dim=1, d_model=384, n_heads=4, e_layers=6, hidden_d_model=176, seq_layers=2, last_d_model=736, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=96, pos_d_model=64, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.12000000000000001, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=224), exp_settings='seq_len-12-lr-0.0057-d-384-hid_d-176-last_d-736-tok_d-64-time_d-96-pos_d-64-e_layers-6-tok_conv_k-8-dropout-0.12000000000000001-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.00574969606023434, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 01:59:27,137] Trial 108 finished with value: 0.9968772172927858 and parameters: {'num_heads': 36, 'hidden_d_model': 176, 'token_conv_kernel': 8, 'last_d_model': 736, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 96, 'pos_d_model': 64, 'd_model': 384, 'conv_out_dim': 224, 'e_layers': 6, 'learning_rate': 0.00574969606023434, 'dropout': 0.12000000000000001, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 01:59:27,438] The parameter 'norm_type' in trial#110 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 10.9 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
10.9 M    Trainable params
0         Non-trainable params
10.9 M    Total params
43.613    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                           Best trial: 93. Best value: 0.969243:  54%|█████▍    | 54/100 [2:23:16<2:11:10, 171.10s/it]Best trial: 93. Best value: 0.969243:  54%|█████▍    | 54/100 [2:23:16<2:11:10, 171.10s/it]Best trial: 93. Best value: 0.969243:  55%|█████▌    | 55/100 [2:23:16<1:58:28, 157.98s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  55%|█████▌    | 55/100 [2:23:16<1:58:28, 157.98s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 20.1 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
20.1 M    Trainable params
0         Non-trainable params
20.1 M    Total params
80.435    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.105
Metric Loss/val improved by 0.046 >= min_delta = 0.0. New best score: 1.059
Metric Loss/val improved by 0.023 >= min_delta = 0.0. New best score: 1.036
Metric Loss/val improved by 0.006 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                           Best trial: 93. Best value: 0.969243:  55%|█████▌    | 55/100 [2:25:32<1:58:28, 157.98s/it]Best trial: 93. Best value: 0.969243:  55%|█████▌    | 55/100 [2:25:32<1:58:28, 157.98s/it]Best trial: 93. Best value: 0.969243:  56%|█████▌    | 56/100 [2:25:32<1:51:09, 151.58s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  56%|█████▌    | 56/100 [2:25:32<1:51:09, 151.58s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=7, input_dim=84, dec_in=84, output_dim=1, d_model=288, n_heads=4, e_layers=4, hidden_d_model=176, seq_layers=2, last_d_model=864, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=48, time_d_model=128, pos_d_model=64, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.22, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=68, fc_layer_type='mha', conv_out_dim=256), exp_settings='seq_len-12-lr-0.0007-d-288-hid_d-176-last_d-864-tok_d-48-time_d-128-pos_d-64-e_layers-4-tok_conv_k-7-dropout-0.22-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-68-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0007461043662646484, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 02:01:34,487] Trial 110 finished with value: 1.9450018167495728 and parameters: {'num_heads': 68, 'hidden_d_model': 176, 'token_conv_kernel': 7, 'last_d_model': 864, 'seq_len': 12, 'token_d_model': 48, 'time_d_model': 128, 'pos_d_model': 64, 'd_model': 288, 'conv_out_dim': 256, 'e_layers': 4, 'learning_rate': 0.0007461043662646484, 'dropout': 0.22, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 02:01:34,756] The parameter 'norm_type' in trial#112 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=320, n_heads=4, e_layers=6, hidden_d_model=208, seq_layers=2, last_d_model=736, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=96, pos_d_model=80, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=68, fc_layer_type='mha', conv_out_dim=288), exp_settings='seq_len-12-lr-0.0072-d-320-hid_d-208-last_d-736-tok_d-64-time_d-96-pos_d-80-e_layers-6-tok_conv_k-9-dropout-0.16-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-68-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.007229298313193291, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 02:03:51,125] Trial 112 finished with value: 0.9949067115783692 and parameters: {'num_heads': 68, 'hidden_d_model': 208, 'token_conv_kernel': 9, 'last_d_model': 736, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 96, 'pos_d_model': 80, 'd_model': 320, 'conv_out_dim': 288, 'e_layers': 6, 'learning_rate': 0.007229298313193291, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 02:03:51,428] The parameter 'norm_type' in trial#114 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 33.5 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
33.5 M    Trainable params
0         Non-trainable params
33.5 M    Total params
133.942   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.785
Metric Loss/val improved by 0.753 >= min_delta = 0.0. New best score: 1.032
Metric Loss/val improved by 0.004 >= min_delta = 0.0. New best score: 1.028
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.028. Signaling Trainer to stop.
                                                                                           Best trial: 93. Best value: 0.969243:  56%|█████▌    | 56/100 [2:28:08<1:51:09, 151.58s/it]Best trial: 93. Best value: 0.969243:  56%|█████▌    | 56/100 [2:28:08<1:51:09, 151.58s/it]Best trial: 93. Best value: 0.969243:  57%|█████▋    | 57/100 [2:28:08<1:49:31, 152.82s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  57%|█████▋    | 57/100 [2:28:08<1:49:31, 152.82s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 19.8 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
19.8 M    Trainable params
0         Non-trainable params
19.8 M    Total params
79.320    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.034
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 1.032
Metric Loss/val improved by 0.003 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                           Best trial: 93. Best value: 0.969243:  57%|█████▋    | 57/100 [2:31:41<1:49:31, 152.82s/it]Best trial: 93. Best value: 0.969243:  57%|█████▋    | 57/100 [2:31:41<1:49:31, 152.82s/it]Best trial: 93. Best value: 0.969243:  58%|█████▊    | 58/100 [2:31:41<1:59:42, 171.01s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  58%|█████▊    | 58/100 [2:31:42<1:59:42, 171.01s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=480, n_heads=4, e_layers=8, hidden_d_model=208, seq_layers=2, last_d_model=1088, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=128, pos_d_model=80, combine_type='add', seq_len=16, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=68, fc_layer_type='mha', conv_out_dim=352), exp_settings='seq_len-16-lr-0.0136-d-480-hid_d-208-last_d-1088-tok_d-64-time_d-128-pos_d-80-e_layers-8-tok_conv_k-9-dropout-0.16-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-68-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0136204746822407, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 02:06:26,868] Trial 114 finished with value: 0.9960978507995606 and parameters: {'num_heads': 68, 'hidden_d_model': 208, 'token_conv_kernel': 9, 'last_d_model': 1088, 'seq_len': 16, 'token_d_model': 64, 'time_d_model': 128, 'pos_d_model': 80, 'd_model': 480, 'conv_out_dim': 352, 'e_layers': 8, 'learning_rate': 0.0136204746822407, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 02:06:27,125] The parameter 'norm_type' in trial#115 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=384, n_heads=4, e_layers=8, hidden_d_model=192, seq_layers=2, last_d_model=1248, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=80, pos_d_model=64, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=68, fc_layer_type='mha', conv_out_dim=192), exp_settings='seq_len-12-lr-0.0037-d-384-hid_d-192-last_d-1248-tok_d-64-time_d-80-pos_d-64-e_layers-8-tok_conv_k-9-dropout-0.14-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-68-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.003724688221504262, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 02:10:00,325] Trial 115 finished with value: 1.138503932952881 and parameters: {'num_heads': 68, 'hidden_d_model': 192, 'token_conv_kernel': 9, 'last_d_model': 1248, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 80, 'pos_d_model': 64, 'd_model': 384, 'conv_out_dim': 192, 'e_layers': 8, 'learning_rate': 0.003724688221504262, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 02:10:00,625] The parameter 'norm_type' in trial#117 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 28.0 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
28.0 M    Trainable params
0         Non-trainable params
28.0 M    Total params
112.044   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.073
Metric Loss/val improved by 0.043 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                           Best trial: 93. Best value: 0.969243:  58%|█████▊    | 58/100 [2:35:24<1:59:42, 171.01s/it]Best trial: 93. Best value: 0.969243:  58%|█████▊    | 58/100 [2:35:24<1:59:42, 171.01s/it]Best trial: 93. Best value: 0.969243:  59%|█████▉    | 59/100 [2:35:24<2:07:21, 186.38s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  59%|█████▉    | 59/100 [2:35:24<2:07:21, 186.38s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 20.0 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
20.0 M    Trainable params
0         Non-trainable params
20.0 M    Total params
79.909    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.042
Metric Loss/val improved by 0.011 >= min_delta = 0.0. New best score: 1.031
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                           Best trial: 93. Best value: 0.969243:  59%|█████▉    | 59/100 [2:38:23<2:07:21, 186.38s/it]Best trial: 93. Best value: 0.969243:  59%|█████▉    | 59/100 [2:38:23<2:07:21, 186.38s/it]Best trial: 93. Best value: 0.969243:  60%|██████    | 60/100 [2:38:23<2:02:53, 184.35s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  60%|██████    | 60/100 [2:38:24<2:02:53, 184.35s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=416, n_heads=4, e_layers=8, hidden_d_model=192, seq_layers=2, last_d_model=864, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=128, pos_d_model=80, combine_type='add', seq_len=10, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=68, fc_layer_type='mha', conv_out_dim=288), exp_settings='seq_len-10-lr-0.0084-d-416-hid_d-192-last_d-864-tok_d-64-time_d-128-pos_d-80-e_layers-8-tok_conv_k-10-dropout-0.16-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-68-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.008387488914946088, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 02:13:42,560] Trial 117 finished with value: 1.035167932510376 and parameters: {'num_heads': 68, 'hidden_d_model': 192, 'token_conv_kernel': 10, 'last_d_model': 864, 'seq_len': 10, 'token_d_model': 64, 'time_d_model': 128, 'pos_d_model': 80, 'd_model': 416, 'conv_out_dim': 288, 'e_layers': 8, 'learning_rate': 0.008387488914946088, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 02:13:42,851] The parameter 'norm_type' in trial#119 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=448, n_heads=4, e_layers=6, hidden_d_model=208, seq_layers=2, last_d_model=1024, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=112, pos_d_model=80, combine_type='add', seq_len=10, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=192), exp_settings='seq_len-10-lr-0.0062-d-448-hid_d-208-last_d-1024-tok_d-64-time_d-112-pos_d-80-e_layers-6-tok_conv_k-9-dropout-0.14-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.006220264913035845, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 02:16:42,156] Trial 119 finished with value: 0.9951433539390564 and parameters: {'num_heads': 36, 'hidden_d_model': 208, 'token_conv_kernel': 9, 'last_d_model': 1024, 'seq_len': 10, 'token_d_model': 64, 'time_d_model': 112, 'pos_d_model': 80, 'd_model': 448, 'conv_out_dim': 192, 'e_layers': 6, 'learning_rate': 0.006220264913035845, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 02:16:42,433] The parameter 'norm_type' in trial#121 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 15.3 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
15.3 M    Trainable params
0         Non-trainable params
15.3 M    Total params
61.046    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.050
Metric Loss/val improved by 0.019 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                           Best trial: 93. Best value: 0.969243:  60%|██████    | 60/100 [2:41:14<2:02:53, 184.35s/it]Best trial: 93. Best value: 0.969243:  60%|██████    | 60/100 [2:41:14<2:02:53, 184.35s/it]Best trial: 93. Best value: 0.969243:  61%|██████    | 61/100 [2:41:14<1:57:05, 180.15s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  61%|██████    | 61/100 [2:41:14<1:57:05, 180.15s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 26.5 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
26.5 M    Trainable params
0         Non-trainable params
26.5 M    Total params
106.152   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.083
Metric Loss/val improved by 0.042 >= min_delta = 0.0. New best score: 1.041
Metric Loss/val improved by 0.011 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                           Best trial: 93. Best value: 0.969243:  61%|██████    | 61/100 [2:44:25<1:57:05, 180.15s/it]Best trial: 93. Best value: 0.969243:  61%|██████    | 61/100 [2:44:25<1:57:05, 180.15s/it]Best trial: 93. Best value: 0.969243:  62%|██████▏   | 62/100 [2:44:25<1:56:17, 183.61s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  62%|██████▏   | 62/100 [2:44:26<1:56:17, 183.61s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=7, input_dim=84, dec_in=84, output_dim=1, d_model=448, n_heads=4, e_layers=6, hidden_d_model=192, seq_layers=2, last_d_model=1056, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=128, pos_d_model=80, combine_type='add', seq_len=10, pred_len=1, min_y_value=0.0, dropout=0.12000000000000001, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=68, fc_layer_type='mha', conv_out_dim=128), exp_settings='seq_len-10-lr-0.0054-d-448-hid_d-192-last_d-1056-tok_d-64-time_d-128-pos_d-80-e_layers-6-tok_conv_k-7-dropout-0.12000000000000001-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-68-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0054191919388519895, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 02:19:32,515] Trial 121 finished with value: 1.0551697254180907 and parameters: {'num_heads': 68, 'hidden_d_model': 192, 'token_conv_kernel': 7, 'last_d_model': 1056, 'seq_len': 10, 'token_d_model': 64, 'time_d_model': 128, 'pos_d_model': 80, 'd_model': 448, 'conv_out_dim': 128, 'e_layers': 6, 'learning_rate': 0.0054191919388519895, 'dropout': 0.12000000000000001, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 02:19:32,835] The parameter 'norm_type' in trial#123 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=448, n_heads=4, e_layers=6, hidden_d_model=208, seq_layers=2, last_d_model=1056, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=80, pos_d_model=80, combine_type='add', seq_len=14, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=68, fc_layer_type='mha', conv_out_dim=320), exp_settings='seq_len-14-lr-0.0086-d-448-hid_d-208-last_d-1056-tok_d-64-time_d-80-pos_d-80-e_layers-6-tok_conv_k-9-dropout-0.16-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-68-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.00858634446747221, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 02:22:44,190] Trial 123 finished with value: 0.9963863134384155 and parameters: {'num_heads': 68, 'hidden_d_model': 208, 'token_conv_kernel': 9, 'last_d_model': 1056, 'seq_len': 14, 'token_d_model': 64, 'time_d_model': 80, 'pos_d_model': 80, 'd_model': 448, 'conv_out_dim': 320, 'e_layers': 6, 'learning_rate': 0.00858634446747221, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 02:22:44,459] The parameter 'norm_type' in trial#125 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 24.3 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
24.3 M    Trainable params
0         Non-trainable params
24.3 M    Total params
97.270    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.155
Metric Loss/val improved by 0.100 >= min_delta = 0.0. New best score: 1.055
Metric Loss/val improved by 0.025 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                           Best trial: 93. Best value: 0.969243:  62%|██████▏   | 62/100 [2:47:31<1:56:17, 183.61s/it]Best trial: 93. Best value: 0.969243:  62%|██████▏   | 62/100 [2:47:31<1:56:17, 183.61s/it]Best trial: 93. Best value: 0.969243:  63%|██████▎   | 63/100 [2:47:31<1:53:34, 184.17s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  63%|██████▎   | 63/100 [2:47:31<1:53:34, 184.17s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 17.7 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
17.7 M    Trainable params
0         Non-trainable params
17.7 M    Total params
70.829    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.036
Metric Loss/val improved by 0.009 >= min_delta = 0.0. New best score: 1.028
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.028. Signaling Trainer to stop.
                                                                                           Best trial: 93. Best value: 0.969243:  63%|██████▎   | 63/100 [2:50:39<1:53:34, 184.17s/it]Best trial: 93. Best value: 0.969243:  63%|██████▎   | 63/100 [2:50:39<1:53:34, 184.17s/it]Best trial: 93. Best value: 0.969243:  64%|██████▍   | 64/100 [2:50:39<1:51:13, 185.37s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  64%|██████▍   | 64/100 [2:50:39<1:51:13, 185.37s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=480, n_heads=4, e_layers=4, hidden_d_model=160, seq_layers=2, last_d_model=1216, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=112, pos_d_model=64, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=68, fc_layer_type='mha', conv_out_dim=288), exp_settings='seq_len-12-lr-0.0119-d-480-hid_d-160-last_d-1216-tok_d-64-time_d-112-pos_d-64-e_layers-4-tok_conv_k-10-dropout-0.14-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-68-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.01189447176260167, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 02:25:49,687] Trial 125 finished with value: 1.0970912694931032 and parameters: {'num_heads': 68, 'hidden_d_model': 160, 'token_conv_kernel': 10, 'last_d_model': 1216, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 112, 'pos_d_model': 64, 'd_model': 480, 'conv_out_dim': 288, 'e_layers': 4, 'learning_rate': 0.01189447176260167, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 02:25:49,973] The parameter 'norm_type' in trial#127 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=8, input_dim=84, dec_in=84, output_dim=1, d_model=416, n_heads=4, e_layers=4, hidden_d_model=208, seq_layers=2, last_d_model=768, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=128, pos_d_model=80, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=68, fc_layer_type='mha', conv_out_dim=256), exp_settings='seq_len-12-lr-0.0069-d-416-hid_d-208-last_d-768-tok_d-64-time_d-128-pos_d-80-e_layers-4-tok_conv_k-8-dropout-0.16-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-68-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.006935090834959754, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 02:28:57,851] Trial 127 finished with value: 1.0035773396492005 and parameters: {'num_heads': 68, 'hidden_d_model': 208, 'token_conv_kernel': 8, 'last_d_model': 768, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 128, 'pos_d_model': 80, 'd_model': 416, 'conv_out_dim': 256, 'e_layers': 4, 'learning_rate': 0.006935090834959754, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 02:28:58,112] The parameter 'norm_type' in trial#130 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 17.3 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
17.3 M    Trainable params
0         Non-trainable params
17.3 M    Total params
69.287    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.114
Metric Loss/val improved by 0.066 >= min_delta = 0.0. New best score: 1.048
Metric Loss/val improved by 0.018 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                           Best trial: 93. Best value: 0.969243:  64%|██████▍   | 64/100 [2:53:15<1:51:13, 185.37s/it]Best trial: 93. Best value: 0.969243:  64%|██████▍   | 64/100 [2:53:15<1:51:13, 185.37s/it]Best trial: 93. Best value: 0.969243:  65%|██████▌   | 65/100 [2:53:15<1:43:01, 176.61s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  65%|██████▌   | 65/100 [2:53:15<1:43:01, 176.61s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 14.5 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
14.5 M    Trainable params
0         Non-trainable params
14.5 M    Total params
57.981    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.040
Metric Loss/val improved by 0.007 >= min_delta = 0.0. New best score: 1.033
Metric Loss/val improved by 0.003 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                           Best trial: 93. Best value: 0.969243:  65%|██████▌   | 65/100 [2:56:11<1:43:01, 176.61s/it]Best trial: 93. Best value: 0.969243:  65%|██████▌   | 65/100 [2:56:11<1:43:01, 176.61s/it]Best trial: 93. Best value: 0.969243:  66%|██████▌   | 66/100 [2:56:11<1:40:02, 176.54s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  66%|██████▌   | 66/100 [2:56:12<1:40:02, 176.54s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=352, n_heads=4, e_layers=6, hidden_d_model=208, seq_layers=2, last_d_model=864, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=112, pos_d_model=64, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.18, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=68, fc_layer_type='mha', conv_out_dim=192), exp_settings='seq_len-12-lr-0.0138-d-352-hid_d-208-last_d-864-tok_d-64-time_d-112-pos_d-64-e_layers-6-tok_conv_k-10-dropout-0.18-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-68-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.01381620814535569, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 02:31:34,023] Trial 130 finished with value: 0.9977081298828125 and parameters: {'num_heads': 68, 'hidden_d_model': 208, 'token_conv_kernel': 10, 'last_d_model': 864, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 112, 'pos_d_model': 64, 'd_model': 352, 'conv_out_dim': 192, 'e_layers': 6, 'learning_rate': 0.01381620814535569, 'dropout': 0.18, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 02:31:34,335] The parameter 'norm_type' in trial#132 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=352, n_heads=4, e_layers=6, hidden_d_model=208, seq_layers=2, last_d_model=608, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=96, pos_d_model=64, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=160), exp_settings='seq_len-12-lr-0.009-d-352-hid_d-208-last_d-608-tok_d-64-time_d-96-pos_d-64-e_layers-6-tok_conv_k-9-dropout-0.16-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.009003776463730719, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 02:34:30,381] Trial 132 finished with value: 0.9971528768539428 and parameters: {'num_heads': 36, 'hidden_d_model': 208, 'token_conv_kernel': 9, 'last_d_model': 608, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 96, 'pos_d_model': 64, 'd_model': 352, 'conv_out_dim': 160, 'e_layers': 6, 'learning_rate': 0.009003776463730719, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 02:34:30,715] The parameter 'norm_type' in trial#134 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 25.3 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
25.3 M    Trainable params
0         Non-trainable params
25.3 M    Total params
101.031   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.218
Metric Loss/val improved by 0.184 >= min_delta = 0.0. New best score: 1.034
Metric Loss/val improved by 0.006 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                           Best trial: 93. Best value: 0.969243:  66%|██████▌   | 66/100 [2:58:15<1:40:02, 176.54s/it]Best trial: 93. Best value: 0.969243:  66%|██████▌   | 66/100 [2:58:15<1:40:02, 176.54s/it]Best trial: 93. Best value: 0.969243:  67%|██████▋   | 67/100 [2:58:15<1:28:22, 160.67s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  67%|██████▋   | 67/100 [2:58:15<1:28:22, 160.67s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 24.6 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
24.6 M    Trainable params
0         Non-trainable params
24.6 M    Total params
98.228    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 7.902
Metric Loss/val improved by 0.768 >= min_delta = 0.0. New best score: 7.134
Metric Loss/val improved by 2.539 >= min_delta = 0.0. New best score: 4.595
Metric Loss/val improved by 0.484 >= min_delta = 0.0. New best score: 4.111
Metric Loss/val improved by 2.332 >= min_delta = 0.0. New best score: 1.780
Metric Loss/val improved by 0.614 >= min_delta = 0.0. New best score: 1.166
Metric Loss/val improved by 0.135 >= min_delta = 0.0. New best score: 1.031
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                           Best trial: 93. Best value: 0.969243:  67%|██████▋   | 67/100 [3:01:31<1:28:22, 160.67s/it]Best trial: 93. Best value: 0.969243:  67%|██████▋   | 67/100 [3:01:31<1:28:22, 160.67s/it]Best trial: 93. Best value: 0.969243:  68%|██████▊   | 68/100 [3:01:31<1:31:19, 171.23s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  68%|██████▊   | 68/100 [3:01:31<1:31:19, 171.23s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=416, n_heads=4, e_layers=6, hidden_d_model=192, seq_layers=2, last_d_model=1056, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=96, pos_d_model=96, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=288), exp_settings='seq_len-12-lr-0.0251-d-416-hid_d-192-last_d-1056-tok_d-64-time_d-96-pos_d-96-e_layers-6-tok_conv_k-10-dropout-0.14-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.025119721382919676, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 02:36:34,041] Trial 134 finished with value: 0.9948640584945679 and parameters: {'num_heads': 36, 'hidden_d_model': 192, 'token_conv_kernel': 10, 'last_d_model': 1056, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 96, 'pos_d_model': 96, 'd_model': 416, 'conv_out_dim': 288, 'e_layers': 6, 'learning_rate': 0.025119721382919676, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 02:36:34,325] The parameter 'norm_type' in trial#136 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=384, n_heads=4, e_layers=8, hidden_d_model=208, seq_layers=2, last_d_model=896, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=128, pos_d_model=80, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=256), exp_settings='seq_len-12-lr-0.0192-d-384-hid_d-208-last_d-896-tok_d-64-time_d-128-pos_d-80-e_layers-8-tok_conv_k-10-dropout-0.16-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.019155093021016527, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 02:39:49,892] Trial 136 finished with value: 1.006984329223633 and parameters: {'num_heads': 36, 'hidden_d_model': 208, 'token_conv_kernel': 10, 'last_d_model': 896, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 128, 'pos_d_model': 80, 'd_model': 384, 'conv_out_dim': 256, 'e_layers': 8, 'learning_rate': 0.019155093021016527, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 02:39:50,177] The parameter 'norm_type' in trial#138 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 21.1 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
21.1 M    Trainable params
0         Non-trainable params
21.1 M    Total params
84.479    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.232
Metric Loss/val improved by 0.185 >= min_delta = 0.0. New best score: 1.046
Metric Loss/val improved by 0.014 >= min_delta = 0.0. New best score: 1.032
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 1.031
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                           Best trial: 93. Best value: 0.969243:  68%|██████▊   | 68/100 [3:04:28<1:31:19, 171.23s/it]Best trial: 93. Best value: 0.969243:  68%|██████▊   | 68/100 [3:04:29<1:31:19, 171.23s/it]Best trial: 93. Best value: 0.969243:  69%|██████▉   | 69/100 [3:04:29<1:29:26, 173.12s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  69%|██████▉   | 69/100 [3:04:29<1:29:26, 173.12s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 25.2 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
25.2 M    Trainable params
0         Non-trainable params
25.2 M    Total params
100.739   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.703
Metric Loss/val improved by 0.314 >= min_delta = 0.0. New best score: 1.389
Metric Loss/val improved by 0.196 >= min_delta = 0.0. New best score: 1.194
Metric Loss/val improved by 0.164 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                           Best trial: 93. Best value: 0.969243:  69%|██████▉   | 69/100 [3:07:41<1:29:26, 173.12s/it]Best trial: 93. Best value: 0.969243:  69%|██████▉   | 69/100 [3:07:41<1:29:26, 173.12s/it]Best trial: 93. Best value: 0.969243:  70%|███████   | 70/100 [3:07:41<1:29:26, 178.87s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  70%|███████   | 70/100 [3:07:41<1:29:26, 178.87s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=480, n_heads=4, e_layers=4, hidden_d_model=208, seq_layers=2, last_d_model=1344, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=112, pos_d_model=64, combine_type='add', seq_len=10, pred_len=1, min_y_value=0.0, dropout=0.12000000000000001, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=256), exp_settings='seq_len-10-lr-0.0172-d-480-hid_d-208-last_d-1344-tok_d-64-time_d-112-pos_d-64-e_layers-4-tok_conv_k-9-dropout-0.12000000000000001-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0171855168154891, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 02:42:47,427] Trial 138 finished with value: 0.9963664054870607 and parameters: {'num_heads': 36, 'hidden_d_model': 208, 'token_conv_kernel': 9, 'last_d_model': 1344, 'seq_len': 10, 'token_d_model': 64, 'time_d_model': 112, 'pos_d_model': 64, 'd_model': 480, 'conv_out_dim': 256, 'e_layers': 4, 'learning_rate': 0.0171855168154891, 'dropout': 0.12000000000000001, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 02:42:47,677] The parameter 'norm_type' in trial#140 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=416, n_heads=4, e_layers=6, hidden_d_model=160, seq_layers=2, last_d_model=992, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=96, pos_d_model=80, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=288), exp_settings='seq_len-12-lr-0.015-d-416-hid_d-160-last_d-992-tok_d-64-time_d-96-pos_d-80-e_layers-6-tok_conv_k-10-dropout-0.16-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0149684157562982, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 02:45:59,710] Trial 140 finished with value: 1.1286171197891237 and parameters: {'num_heads': 36, 'hidden_d_model': 160, 'token_conv_kernel': 10, 'last_d_model': 992, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 96, 'pos_d_model': 80, 'd_model': 416, 'conv_out_dim': 288, 'e_layers': 6, 'learning_rate': 0.0149684157562982, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 02:45:59,976] The parameter 'norm_type' in trial#142 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 31.8 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
31.8 M    Trainable params
0         Non-trainable params
31.8 M    Total params
127.394   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.055
Metric Loss/val improved by 0.007 >= min_delta = 0.0. New best score: 1.048
Metric Loss/val improved by 0.017 >= min_delta = 0.0. New best score: 1.031
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                           Best trial: 93. Best value: 0.969243:  70%|███████   | 70/100 [3:11:02<1:29:26, 178.87s/it]Best trial: 93. Best value: 0.969243:  70%|███████   | 70/100 [3:11:02<1:29:26, 178.87s/it]Best trial: 93. Best value: 0.969243:  71%|███████   | 71/100 [3:11:02<1:29:43, 185.62s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  71%|███████   | 71/100 [3:11:02<1:29:43, 185.62s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 33.7 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
33.7 M    Trainable params
0         Non-trainable params
33.7 M    Total params
134.986   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.142
Metric Loss/val improved by 0.018 >= min_delta = 0.0. New best score: 1.123
Metric Loss/val improved by 0.093 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                           Best trial: 93. Best value: 0.969243:  71%|███████   | 71/100 [3:13:55<1:29:43, 185.62s/it]Best trial: 93. Best value: 0.969243:  71%|███████   | 71/100 [3:13:55<1:29:43, 185.62s/it]Best trial: 93. Best value: 0.969243:  72%|███████▏  | 72/100 [3:13:55<1:24:46, 181.64s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  72%|███████▏  | 72/100 [3:13:55<1:24:46, 181.64s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=448, n_heads=4, e_layers=6, hidden_d_model=160, seq_layers=2, last_d_model=1440, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=96, pos_d_model=112, combine_type='add', seq_len=10, pred_len=1, min_y_value=0.0, dropout=0.12000000000000001, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=416), exp_settings='seq_len-10-lr-0.0097-d-448-hid_d-160-last_d-1440-tok_d-64-time_d-96-pos_d-112-e_layers-6-tok_conv_k-9-dropout-0.12000000000000001-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.009696326650111706, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 02:49:21,094] Trial 142 finished with value: 0.9959258913993836 and parameters: {'num_heads': 36, 'hidden_d_model': 160, 'token_conv_kernel': 9, 'last_d_model': 1440, 'seq_len': 10, 'token_d_model': 64, 'time_d_model': 96, 'pos_d_model': 112, 'd_model': 448, 'conv_out_dim': 416, 'e_layers': 6, 'learning_rate': 0.009696326650111706, 'dropout': 0.12000000000000001, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 02:49:21,359] The parameter 'norm_type' in trial#144 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=480, n_heads=4, e_layers=8, hidden_d_model=160, seq_layers=2, last_d_model=1504, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=96, pos_d_model=96, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.2, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=320), exp_settings='seq_len-12-lr-0.0101-d-480-hid_d-160-last_d-1504-tok_d-64-time_d-96-pos_d-96-e_layers-8-tok_conv_k-10-dropout-0.2-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.010091992020711588, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 02:52:13,448] Trial 144 finished with value: 0.9961809873580934 and parameters: {'num_heads': 36, 'hidden_d_model': 160, 'token_conv_kernel': 10, 'last_d_model': 1504, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 96, 'pos_d_model': 96, 'd_model': 480, 'conv_out_dim': 320, 'e_layers': 8, 'learning_rate': 0.010091992020711588, 'dropout': 0.2, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 02:52:13,778] The parameter 'norm_type' in trial#146 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 28.0 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
28.0 M    Trainable params
0         Non-trainable params
28.0 M    Total params
111.973   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.498
Metric Loss/val improved by 0.348 >= min_delta = 0.0. New best score: 1.149
Metric Loss/val improved by 0.120 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                           Best trial: 93. Best value: 0.969243:  72%|███████▏  | 72/100 [3:16:37<1:24:46, 181.64s/it]Best trial: 93. Best value: 0.969243:  72%|███████▏  | 72/100 [3:16:37<1:24:46, 181.64s/it]Best trial: 93. Best value: 0.969243:  73%|███████▎  | 73/100 [3:16:37<1:19:10, 175.94s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  73%|███████▎  | 73/100 [3:16:37<1:19:10, 175.94s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 21.3 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
21.3 M    Trainable params
0         Non-trainable params
21.3 M    Total params
85.246    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.103
Metric Loss/val improved by 0.049 >= min_delta = 0.0. New best score: 1.054
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 1.052
Metric Loss/val improved by 0.021 >= min_delta = 0.0. New best score: 1.031
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                           Best trial: 93. Best value: 0.969243:  73%|███████▎  | 73/100 [3:19:51<1:19:10, 175.94s/it]Best trial: 93. Best value: 0.969243:  73%|███████▎  | 73/100 [3:19:51<1:19:10, 175.94s/it]Best trial: 93. Best value: 0.969243:  74%|███████▍  | 74/100 [3:19:51<1:18:33, 181.27s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  74%|███████▍  | 74/100 [3:19:51<1:18:33, 181.27s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=11, input_dim=84, dec_in=84, output_dim=1, d_model=384, n_heads=4, e_layers=8, hidden_d_model=176, seq_layers=2, last_d_model=896, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=80, pos_d_model=128, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.12000000000000001, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=288), exp_settings='seq_len-12-lr-0.0115-d-384-hid_d-176-last_d-896-tok_d-64-time_d-80-pos_d-128-e_layers-8-tok_conv_k-11-dropout-0.12000000000000001-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.011503856297489509, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 02:54:56,080] Trial 146 finished with value: 0.9970229625701905 and parameters: {'num_heads': 36, 'hidden_d_model': 176, 'token_conv_kernel': 11, 'last_d_model': 896, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 80, 'pos_d_model': 128, 'd_model': 384, 'conv_out_dim': 288, 'e_layers': 8, 'learning_rate': 0.011503856297489509, 'dropout': 0.12000000000000001, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 02:54:56,383] The parameter 'norm_type' in trial#148 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=448, n_heads=4, e_layers=6, hidden_d_model=176, seq_layers=2, last_d_model=1248, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=48, time_d_model=112, pos_d_model=96, combine_type='add', seq_len=10, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=256), exp_settings='seq_len-10-lr-0.0112-d-448-hid_d-176-last_d-1248-tok_d-48-time_d-112-pos_d-96-e_layers-6-tok_conv_k-10-dropout-0.16-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.01116689272481795, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 02:58:09,808] Trial 148 finished with value: 1.0568830847740174 and parameters: {'num_heads': 36, 'hidden_d_model': 176, 'token_conv_kernel': 10, 'last_d_model': 1248, 'seq_len': 10, 'token_d_model': 48, 'time_d_model': 112, 'pos_d_model': 96, 'd_model': 448, 'conv_out_dim': 256, 'e_layers': 6, 'learning_rate': 0.01116689272481795, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 02:58:10,073] The parameter 'norm_type' in trial#150 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 22.8 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
22.8 M    Trainable params
0         Non-trainable params
22.8 M    Total params
91.231    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.051
Metric Loss/val improved by 0.020 >= min_delta = 0.0. New best score: 1.031
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                           Best trial: 93. Best value: 0.969243:  74%|███████▍  | 74/100 [3:23:13<1:18:33, 181.27s/it]Best trial: 93. Best value: 0.969243:  74%|███████▍  | 74/100 [3:23:13<1:18:33, 181.27s/it]Best trial: 93. Best value: 0.969243:  75%|███████▌  | 75/100 [3:23:13<1:18:08, 187.53s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  75%|███████▌  | 75/100 [3:23:13<1:18:08, 187.53s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 25.9 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
25.9 M    Trainable params
0         Non-trainable params
25.9 M    Total params
103.612   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.581
Metric Loss/val improved by 0.437 >= min_delta = 0.0. New best score: 1.144
Metric Loss/val improved by 0.006 >= min_delta = 0.0. New best score: 1.138
Metric Loss/val improved by 0.108 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                           Best trial: 93. Best value: 0.969243:  75%|███████▌  | 75/100 [3:25:36<1:18:08, 187.53s/it]Best trial: 93. Best value: 0.969243:  75%|███████▌  | 75/100 [3:25:36<1:18:08, 187.53s/it]Best trial: 93. Best value: 0.969243:  76%|███████▌  | 76/100 [3:25:36<1:09:37, 174.05s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  76%|███████▌  | 76/100 [3:25:36<1:09:37, 174.05s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=352, n_heads=4, e_layers=6, hidden_d_model=192, seq_layers=2, last_d_model=800, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=96, pos_d_model=96, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=68, fc_layer_type='mha', conv_out_dim=288), exp_settings='seq_len-12-lr-0.0097-d-352-hid_d-192-last_d-800-tok_d-64-time_d-96-pos_d-96-e_layers-6-tok_conv_k-10-dropout-0.14-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-68-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.009745636314085458, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 03:01:31,919] Trial 150 finished with value: 0.994077706336975 and parameters: {'num_heads': 68, 'hidden_d_model': 192, 'token_conv_kernel': 10, 'last_d_model': 800, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 96, 'pos_d_model': 96, 'd_model': 352, 'conv_out_dim': 288, 'e_layers': 6, 'learning_rate': 0.009745636314085458, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 03:01:32,214] The parameter 'norm_type' in trial#152 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=11, input_dim=84, dec_in=84, output_dim=1, d_model=480, n_heads=4, e_layers=4, hidden_d_model=192, seq_layers=2, last_d_model=1088, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=96, pos_d_model=96, combine_type='add', seq_len=10, pred_len=1, min_y_value=0.0, dropout=0.12000000000000001, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=288), exp_settings='seq_len-10-lr-0.0192-d-480-hid_d-192-last_d-1088-tok_d-64-time_d-96-pos_d-96-e_layers-4-tok_conv_k-11-dropout-0.12000000000000001-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.01921981027837748, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 03:03:54,515] Trial 152 finished with value: 0.9944945812225342 and parameters: {'num_heads': 36, 'hidden_d_model': 192, 'token_conv_kernel': 11, 'last_d_model': 1088, 'seq_len': 10, 'token_d_model': 64, 'time_d_model': 96, 'pos_d_model': 96, 'd_model': 480, 'conv_out_dim': 288, 'e_layers': 4, 'learning_rate': 0.01921981027837748, 'dropout': 0.12000000000000001, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 03:03:54,793] The parameter 'norm_type' in trial#154 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 22.3 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
22.3 M    Trainable params
0         Non-trainable params
22.3 M    Total params
89.357    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.274
Metric Loss/val improved by 0.098 >= min_delta = 0.0. New best score: 1.176
Metric Loss/val improved by 0.119 >= min_delta = 0.0. New best score: 1.057
Metric Loss/val improved by 0.026 >= min_delta = 0.0. New best score: 1.031
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                           Best trial: 93. Best value: 0.969243:  76%|███████▌  | 76/100 [3:29:08<1:09:37, 174.05s/it]Best trial: 93. Best value: 0.969243:  76%|███████▌  | 76/100 [3:29:08<1:09:37, 174.05s/it]Best trial: 93. Best value: 0.969243:  77%|███████▋  | 77/100 [3:29:08<1:11:06, 185.52s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  77%|███████▋  | 77/100 [3:29:08<1:11:06, 185.52s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 28.2 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
28.2 M    Trainable params
0         Non-trainable params
28.2 M    Total params
112.876   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.130
Metric Loss/val improved by 0.022 >= min_delta = 0.0. New best score: 1.108
Metric Loss/val improved by 0.026 >= min_delta = 0.0. New best score: 1.082
Metric Loss/val improved by 0.008 >= min_delta = 0.0. New best score: 1.074
Metric Loss/val improved by 0.040 >= min_delta = 0.0. New best score: 1.034
Metric Loss/val improved by 0.005 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                           Best trial: 93. Best value: 0.969243:  77%|███████▋  | 77/100 [3:32:15<1:11:06, 185.52s/it]Best trial: 93. Best value: 0.969243:  77%|███████▋  | 77/100 [3:32:15<1:11:06, 185.52s/it]Best trial: 93. Best value: 0.969243:  78%|███████▊  | 78/100 [3:32:15<1:08:13, 186.05s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  78%|███████▊  | 78/100 [3:32:15<1:08:13, 186.05s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=384, n_heads=4, e_layers=6, hidden_d_model=176, seq_layers=2, last_d_model=832, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=80, pos_d_model=112, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=288), exp_settings='seq_len-12-lr-0.0156-d-384-hid_d-176-last_d-832-tok_d-64-time_d-80-pos_d-112-e_layers-6-tok_conv_k-9-dropout-0.16-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.015649471861298656, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 03:07:26,787] Trial 154 finished with value: 0.9989208936691285 and parameters: {'num_heads': 36, 'hidden_d_model': 176, 'token_conv_kernel': 9, 'last_d_model': 832, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 80, 'pos_d_model': 112, 'd_model': 384, 'conv_out_dim': 288, 'e_layers': 6, 'learning_rate': 0.015649471861298656, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 03:07:27,089] The parameter 'norm_type' in trial#156 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=480, n_heads=4, e_layers=6, hidden_d_model=176, seq_layers=2, last_d_model=1216, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=112, pos_d_model=112, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=4, fc_layer_type='mha', conv_out_dim=320), exp_settings='seq_len-8-lr-0.0113-d-480-hid_d-176-last_d-1216-tok_d-64-time_d-112-pos_d-112-e_layers-6-tok_conv_k-9-dropout-0.14-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-4-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.011341945751449511, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 03:10:34,067] Trial 156 finished with value: 0.995384931564331 and parameters: {'num_heads': 4, 'hidden_d_model': 176, 'token_conv_kernel': 9, 'last_d_model': 1216, 'seq_len': 8, 'token_d_model': 64, 'time_d_model': 112, 'pos_d_model': 112, 'd_model': 480, 'conv_out_dim': 320, 'e_layers': 6, 'learning_rate': 0.011341945751449511, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 03:10:34,390] The parameter 'norm_type' in trial#158 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 15.5 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
15.5 M    Trainable params
0         Non-trainable params
15.5 M    Total params
61.920    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.049
Metric Loss/val improved by 0.013 >= min_delta = 0.0. New best score: 1.036
Metric Loss/val improved by 0.006 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                           Best trial: 93. Best value: 0.969243:  78%|███████▊  | 78/100 [3:35:39<1:08:13, 186.05s/it]Best trial: 93. Best value: 0.969243:  78%|███████▊  | 78/100 [3:35:39<1:08:13, 186.05s/it]Best trial: 93. Best value: 0.969243:  79%|███████▉  | 79/100 [3:35:39<1:06:56, 191.25s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  79%|███████▉  | 79/100 [3:35:41<1:06:56, 191.25s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 27.7 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
27.7 M    Trainable params
0         Non-trainable params
27.7 M    Total params
110.765   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.073
Metric Loss/val improved by 0.031 >= min_delta = 0.0. New best score: 1.042
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 1.040
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.040
Metric Loss/val improved by 0.010 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                           Best trial: 93. Best value: 0.969243:  79%|███████▉  | 79/100 [3:38:20<1:06:56, 191.25s/it]Best trial: 93. Best value: 0.969243:  79%|███████▉  | 79/100 [3:38:20<1:06:56, 191.25s/it]Best trial: 93. Best value: 0.969243:  80%|████████  | 80/100 [3:38:20<1:00:47, 182.37s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                           Best trial: 93. Best value: 0.969243:  80%|████████  | 80/100 [3:38:20<1:00:47, 182.37s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=416, n_heads=4, e_layers=4, hidden_d_model=208, seq_layers=2, last_d_model=576, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=48, time_d_model=96, pos_d_model=96, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=224), exp_settings='seq_len-12-lr-0.0175-d-416-hid_d-208-last_d-576-tok_d-48-time_d-96-pos_d-96-e_layers-4-tok_conv_k-10-dropout-0.14-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.017539458969106386, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 03:13:57,455] Trial 158 finished with value: 0.9966324329376222 and parameters: {'num_heads': 36, 'hidden_d_model': 208, 'token_conv_kernel': 10, 'last_d_model': 576, 'seq_len': 12, 'token_d_model': 48, 'time_d_model': 96, 'pos_d_model': 96, 'd_model': 416, 'conv_out_dim': 224, 'e_layers': 4, 'learning_rate': 0.017539458969106386, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 03:14:00,040] The parameter 'norm_type' in trial#161 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=11, input_dim=84, dec_in=84, output_dim=1, d_model=384, n_heads=4, e_layers=6, hidden_d_model=208, seq_layers=2, last_d_model=1024, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=112, pos_d_model=96, combine_type='add', seq_len=10, pred_len=1, min_y_value=0.0, dropout=0.12000000000000001, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=320), exp_settings='seq_len-10-lr-0.0067-d-384-hid_d-208-last_d-1024-tok_d-64-time_d-112-pos_d-96-e_layers-6-tok_conv_k-11-dropout-0.12000000000000001-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.006691997838812846, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 03:16:39,101] Trial 161 finished with value: 0.9960958242416382 and parameters: {'num_heads': 36, 'hidden_d_model': 208, 'token_conv_kernel': 11, 'last_d_model': 1024, 'seq_len': 10, 'token_d_model': 64, 'time_d_model': 112, 'pos_d_model': 96, 'd_model': 384, 'conv_out_dim': 320, 'e_layers': 6, 'learning_rate': 0.006691997838812846, 'dropout': 0.12000000000000001, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 03:16:39,396] The parameter 'norm_type' in trial#163 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 22.3 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
22.3 M    Trainable params
0         Non-trainable params
22.3 M    Total params
89.184    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.075
Metric Loss/val improved by 0.037 >= min_delta = 0.0. New best score: 1.037
Metric Loss/val improved by 0.008 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                           Best trial: 93. Best value: 0.969243:  80%|████████  | 80/100 [3:40:24<1:00:47, 182.37s/it]Best trial: 93. Best value: 0.969243:  80%|████████  | 80/100 [3:40:24<1:00:47, 182.37s/it]Best trial: 93. Best value: 0.969243:  81%|████████  | 81/100 [3:40:24<52:12, 164.89s/it]  /home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                         Best trial: 93. Best value: 0.969243:  81%|████████  | 81/100 [3:40:25<52:12, 164.89s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 30.7 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
30.7 M    Trainable params
0         Non-trainable params
30.7 M    Total params
122.791   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 3.743
Metric Loss/val improved by 1.340 >= min_delta = 0.0. New best score: 2.403
Metric Loss/val improved by 1.368 >= min_delta = 0.0. New best score: 1.035
Metric Loss/val improved by 0.007 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                         Best trial: 93. Best value: 0.969243:  81%|████████  | 81/100 [3:42:32<52:12, 164.89s/it]Best trial: 93. Best value: 0.969243:  81%|████████  | 81/100 [3:42:32<52:12, 164.89s/it]Best trial: 93. Best value: 0.969243:  82%|████████▏ | 82/100 [3:42:32<46:06, 153.68s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                         Best trial: 93. Best value: 0.969243:  82%|████████▏ | 82/100 [3:42:32<46:06, 153.68s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=11, input_dim=84, dec_in=84, output_dim=1, d_model=448, n_heads=4, e_layers=4, hidden_d_model=160, seq_layers=2, last_d_model=1184, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=48, time_d_model=96, pos_d_model=128, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=68, fc_layer_type='mha', conv_out_dim=320), exp_settings='seq_len-12-lr-0.0121-d-448-hid_d-160-last_d-1184-tok_d-48-time_d-96-pos_d-128-e_layers-4-tok_conv_k-11-dropout-0.16-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-68-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.012089439375499238, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 03:18:43,223] Trial 163 finished with value: 1.057757592201233 and parameters: {'num_heads': 68, 'hidden_d_model': 160, 'token_conv_kernel': 11, 'last_d_model': 1184, 'seq_len': 12, 'token_d_model': 48, 'time_d_model': 96, 'pos_d_model': 128, 'd_model': 448, 'conv_out_dim': 320, 'e_layers': 4, 'learning_rate': 0.012089439375499238, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 03:18:43,488] The parameter 'norm_type' in trial#164 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=11, input_dim=84, dec_in=84, output_dim=1, d_model=416, n_heads=4, e_layers=6, hidden_d_model=208, seq_layers=2, last_d_model=608, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=80, pos_d_model=112, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=68, fc_layer_type='mha', conv_out_dim=352), exp_settings='seq_len-12-lr-0.0234-d-416-hid_d-208-last_d-608-tok_d-64-time_d-80-pos_d-112-e_layers-6-tok_conv_k-11-dropout-0.16-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-68-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.023405804303492185, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 03:20:50,752] Trial 164 finished with value: 0.9964129567146303 and parameters: {'num_heads': 68, 'hidden_d_model': 208, 'token_conv_kernel': 11, 'last_d_model': 608, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 80, 'pos_d_model': 112, 'd_model': 416, 'conv_out_dim': 352, 'e_layers': 6, 'learning_rate': 0.023405804303492185, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 03:20:51,010] The parameter 'norm_type' in trial#166 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 20.4 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
20.4 M    Trainable params
0         Non-trainable params
20.4 M    Total params
81.727    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.098
Metric Loss/val improved by 0.054 >= min_delta = 0.0. New best score: 1.045
Metric Loss/val improved by 0.014 >= min_delta = 0.0. New best score: 1.031
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                         Best trial: 93. Best value: 0.969243:  82%|████████▏ | 82/100 [3:45:14<46:06, 153.68s/it]Best trial: 93. Best value: 0.969243:  82%|████████▏ | 82/100 [3:45:14<46:06, 153.68s/it]Best trial: 93. Best value: 0.969243:  83%|████████▎ | 83/100 [3:45:14<44:14, 156.15s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                         Best trial: 93. Best value: 0.969243:  83%|████████▎ | 83/100 [3:45:14<44:14, 156.15s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 23.5 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
23.5 M    Trainable params
0         Non-trainable params
23.5 M    Total params
94.071    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.097
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.095
Metric Loss/val improved by 0.057 >= min_delta = 0.0. New best score: 1.038
Metric Loss/val improved by 0.009 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                         Best trial: 93. Best value: 0.969243:  83%|████████▎ | 83/100 [3:47:54<44:14, 156.15s/it]Best trial: 93. Best value: 0.969243:  83%|████████▎ | 83/100 [3:47:54<44:14, 156.15s/it]Best trial: 93. Best value: 0.969243:  84%|████████▍ | 84/100 [3:47:54<42:00, 157.53s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                         Best trial: 93. Best value: 0.969243:  84%|████████▍ | 84/100 [3:47:55<42:00, 157.53s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=384, n_heads=4, e_layers=6, hidden_d_model=192, seq_layers=2, last_d_model=1216, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=96, pos_d_model=96, combine_type='add', seq_len=10, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=224), exp_settings='seq_len-10-lr-0.0265-d-384-hid_d-192-last_d-1216-tok_d-64-time_d-96-pos_d-96-e_layers-6-tok_conv_k-10-dropout-0.14-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.02648621017655146, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 03:23:32,661] Trial 166 finished with value: 0.9966449975967409 and parameters: {'num_heads': 36, 'hidden_d_model': 192, 'token_conv_kernel': 10, 'last_d_model': 1216, 'seq_len': 10, 'token_d_model': 64, 'time_d_model': 96, 'pos_d_model': 96, 'd_model': 384, 'conv_out_dim': 224, 'e_layers': 6, 'learning_rate': 0.02648621017655146, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 03:23:32,931] The parameter 'norm_type' in trial#168 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=384, n_heads=4, e_layers=4, hidden_d_model=176, seq_layers=2, last_d_model=992, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=96, pos_d_model=112, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=320), exp_settings='seq_len-12-lr-0.0139-d-384-hid_d-176-last_d-992-tok_d-64-time_d-96-pos_d-112-e_layers-4-tok_conv_k-10-dropout-0.14-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.013901601301297146, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 03:26:13,403] Trial 168 finished with value: 4.6123099565505985 and parameters: {'num_heads': 36, 'hidden_d_model': 176, 'token_conv_kernel': 10, 'last_d_model': 992, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 96, 'pos_d_model': 112, 'd_model': 384, 'conv_out_dim': 320, 'e_layers': 4, 'learning_rate': 0.013901601301297146, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 03:26:13,712] The parameter 'norm_type' in trial#170 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 31.1 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
31.1 M    Trainable params
0         Non-trainable params
31.1 M    Total params
124.319   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.062
Metric Loss/val improved by 0.033 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                         Best trial: 93. Best value: 0.969243:  84%|████████▍ | 84/100 [3:50:57<42:00, 157.53s/it]Best trial: 93. Best value: 0.969243:  84%|████████▍ | 84/100 [3:50:57<42:00, 157.53s/it]Best trial: 93. Best value: 0.969243:  85%|████████▌ | 85/100 [3:50:57<41:16, 165.08s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                         Best trial: 93. Best value: 0.969243:  85%|████████▌ | 85/100 [3:50:57<41:16, 165.08s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 28.7 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
28.7 M    Trainable params
0         Non-trainable params
28.7 M    Total params
114.780   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.050
Metric Loss/val improved by 0.010 >= min_delta = 0.0. New best score: 1.040
Metric Loss/val improved by 0.008 >= min_delta = 0.0. New best score: 1.032
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                         Best trial: 93. Best value: 0.969243:  85%|████████▌ | 85/100 [3:53:57<41:16, 165.08s/it]Best trial: 93. Best value: 0.969243:  85%|████████▌ | 85/100 [3:53:57<41:16, 165.08s/it]Best trial: 93. Best value: 0.969243:  86%|████████▌ | 86/100 [3:53:57<39:34, 169.63s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                         Best trial: 93. Best value: 0.969243:  86%|████████▌ | 86/100 [3:53:58<39:34, 169.63s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=11, input_dim=84, dec_in=84, output_dim=1, d_model=512, n_heads=4, e_layers=4, hidden_d_model=176, seq_layers=2, last_d_model=1344, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=96, pos_d_model=96, combine_type='add', seq_len=8, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=352), exp_settings='seq_len-8-lr-0.0094-d-512-hid_d-176-last_d-1344-tok_d-64-time_d-96-pos_d-96-e_layers-4-tok_conv_k-11-dropout-0.14-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.009400331429136419, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 03:29:16,090] Trial 170 finished with value: 0.9957600593566895 and parameters: {'num_heads': 36, 'hidden_d_model': 176, 'token_conv_kernel': 11, 'last_d_model': 1344, 'seq_len': 8, 'token_d_model': 64, 'time_d_model': 96, 'pos_d_model': 96, 'd_model': 512, 'conv_out_dim': 352, 'e_layers': 4, 'learning_rate': 0.009400331429136419, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 03:29:16,368] The parameter 'norm_type' in trial#172 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=11, input_dim=84, dec_in=84, output_dim=1, d_model=416, n_heads=4, e_layers=6, hidden_d_model=224, seq_layers=2, last_d_model=768, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=80, pos_d_model=96, combine_type='add', seq_len=10, pred_len=1, min_y_value=0.0, dropout=0.2, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=320), exp_settings='seq_len-10-lr-0.0077-d-416-hid_d-224-last_d-768-tok_d-64-time_d-80-pos_d-96-e_layers-6-tok_conv_k-11-dropout-0.2-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0077492969176184, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 03:32:16,360] Trial 172 finished with value: 0.9958180665969849 and parameters: {'num_heads': 36, 'hidden_d_model': 224, 'token_conv_kernel': 11, 'last_d_model': 768, 'seq_len': 10, 'token_d_model': 64, 'time_d_model': 80, 'pos_d_model': 96, 'd_model': 416, 'conv_out_dim': 320, 'e_layers': 6, 'learning_rate': 0.0077492969176184, 'dropout': 0.2, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 03:32:16,664] The parameter 'norm_type' in trial#174 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 25.5 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
25.5 M    Trainable params
0         Non-trainable params
25.5 M    Total params
101.991   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.106
Metric Loss/val improved by 0.058 >= min_delta = 0.0. New best score: 1.048
Metric Loss/val improved by 0.018 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                         Best trial: 93. Best value: 0.969243:  86%|████████▌ | 86/100 [3:56:12<39:34, 169.63s/it]Best trial: 93. Best value: 0.969243:  86%|████████▌ | 86/100 [3:56:12<39:34, 169.63s/it]Best trial: 93. Best value: 0.969243:  87%|████████▋ | 87/100 [3:56:12<34:28, 159.10s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                         Best trial: 93. Best value: 0.969243:  87%|████████▋ | 87/100 [3:56:12<34:28, 159.10s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 24.7 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
24.7 M    Trainable params
0         Non-trainable params
24.7 M    Total params
98.969    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.069
Metric Loss/val improved by 0.004 >= min_delta = 0.0. New best score: 1.065
Metric Loss/val improved by 0.021 >= min_delta = 0.0. New best score: 1.044
Metric Loss/val improved by 0.012 >= min_delta = 0.0. New best score: 1.032
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                         Best trial: 93. Best value: 0.969243:  87%|████████▋ | 87/100 [3:59:36<34:28, 159.10s/it]Best trial: 93. Best value: 0.969243:  87%|████████▋ | 87/100 [3:59:36<34:28, 159.10s/it]Best trial: 93. Best value: 0.969243:  88%|████████▊ | 88/100 [3:59:36<34:29, 172.50s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                         Best trial: 93. Best value: 0.969243:  88%|████████▊ | 88/100 [3:59:36<34:29, 172.50s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=384, n_heads=4, e_layers=4, hidden_d_model=192, seq_layers=2, last_d_model=1504, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=112, pos_d_model=80, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=352), exp_settings='seq_len-12-lr-0.0117-d-384-hid_d-192-last_d-1504-tok_d-64-time_d-112-pos_d-80-e_layers-4-tok_conv_k-10-dropout-0.16-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.01172895291021251, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 03:34:30,878] Trial 174 finished with value: 3.660809648036957 and parameters: {'num_heads': 36, 'hidden_d_model': 192, 'token_conv_kernel': 10, 'last_d_model': 1504, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 112, 'pos_d_model': 80, 'd_model': 384, 'conv_out_dim': 352, 'e_layers': 4, 'learning_rate': 0.01172895291021251, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 03:34:31,127] The parameter 'norm_type' in trial#176 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=352, n_heads=4, e_layers=6, hidden_d_model=192, seq_layers=2, last_d_model=960, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=96, pos_d_model=112, combine_type='add', seq_len=14, pred_len=1, min_y_value=0.0, dropout=0.2, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=320), exp_settings='seq_len-14-lr-0.0074-d-352-hid_d-192-last_d-960-tok_d-64-time_d-96-pos_d-112-e_layers-6-tok_conv_k-10-dropout-0.2-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.007370441127627579, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 03:37:54,634] Trial 176 finished with value: 0.9919558405876161 and parameters: {'num_heads': 36, 'hidden_d_model': 192, 'token_conv_kernel': 10, 'last_d_model': 960, 'seq_len': 14, 'token_d_model': 64, 'time_d_model': 96, 'pos_d_model': 112, 'd_model': 352, 'conv_out_dim': 320, 'e_layers': 6, 'learning_rate': 0.007370441127627579, 'dropout': 0.2, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 03:37:54,995] The parameter 'norm_type' in trial#179 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 26.3 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
26.3 M    Trainable params
0         Non-trainable params
26.3 M    Total params
105.251   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.280
Metric Loss/val improved by 0.009 >= min_delta = 0.0. New best score: 1.271
Metric Loss/val improved by 0.209 >= min_delta = 0.0. New best score: 1.062
Metric Loss/val improved by 0.031 >= min_delta = 0.0. New best score: 1.031
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 1.029
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                         Best trial: 93. Best value: 0.969243:  88%|████████▊ | 88/100 [4:02:28<34:29, 172.50s/it]Best trial: 93. Best value: 0.969243:  88%|████████▊ | 88/100 [4:02:28<34:29, 172.50s/it]Best trial: 93. Best value: 0.969243:  89%|████████▉ | 89/100 [4:02:28<31:36, 172.38s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                         Best trial: 93. Best value: 0.969243:  89%|████████▉ | 89/100 [4:02:28<31:36, 172.38s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 21.2 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
21.2 M    Trainable params
0         Non-trainable params
21.2 M    Total params
84.684    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.039
Metric Loss/val improved by 0.006 >= min_delta = 0.0. New best score: 1.033
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.032
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                         Best trial: 93. Best value: 0.969243:  89%|████████▉ | 89/100 [4:05:27<31:36, 172.38s/it]Best trial: 93. Best value: 0.969243:  89%|████████▉ | 89/100 [4:05:27<31:36, 172.38s/it]Best trial: 93. Best value: 0.969243:  90%|█████████ | 90/100 [4:05:27<29:04, 174.48s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                         Best trial: 93. Best value: 0.969243:  90%|█████████ | 90/100 [4:05:28<29:04, 174.48s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=11, input_dim=84, dec_in=84, output_dim=1, d_model=352, n_heads=4, e_layers=4, hidden_d_model=208, seq_layers=2, last_d_model=1184, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=80, pos_d_model=80, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.18, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=352), exp_settings='seq_len-12-lr-0.0086-d-352-hid_d-208-last_d-1184-tok_d-64-time_d-80-pos_d-80-e_layers-4-tok_conv_k-11-dropout-0.18-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.008629934099266484, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 03:40:46,741] Trial 179 finished with value: 1.0670934200286866 and parameters: {'num_heads': 36, 'hidden_d_model': 208, 'token_conv_kernel': 11, 'last_d_model': 1184, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 80, 'pos_d_model': 80, 'd_model': 352, 'conv_out_dim': 352, 'e_layers': 4, 'learning_rate': 0.008629934099266484, 'dropout': 0.18, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 03:40:47,005] The parameter 'norm_type' in trial#181 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=11, input_dim=84, dec_in=84, output_dim=1, d_model=384, n_heads=4, e_layers=4, hidden_d_model=208, seq_layers=2, last_d_model=832, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=112, pos_d_model=80, combine_type='add', seq_len=10, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=256), exp_settings='seq_len-10-lr-0.0066-d-384-hid_d-208-last_d-832-tok_d-64-time_d-112-pos_d-80-e_layers-4-tok_conv_k-11-dropout-0.16-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.006550467256368785, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 03:43:46,118] Trial 181 finished with value: 0.9973549604415894 and parameters: {'num_heads': 36, 'hidden_d_model': 208, 'token_conv_kernel': 11, 'last_d_model': 832, 'seq_len': 10, 'token_d_model': 64, 'time_d_model': 112, 'pos_d_model': 80, 'd_model': 384, 'conv_out_dim': 256, 'e_layers': 4, 'learning_rate': 0.006550467256368785, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 03:43:46,452] The parameter 'norm_type' in trial#183 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 32.2 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
32.2 M    Trainable params
0         Non-trainable params
32.2 M    Total params
128.970   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.034
Metric Loss/val improved by 0.003 >= min_delta = 0.0. New best score: 1.031
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.031. Signaling Trainer to stop.
                                                                                         Best trial: 93. Best value: 0.969243:  90%|█████████ | 90/100 [4:09:09<29:04, 174.48s/it]Best trial: 93. Best value: 0.969243:  90%|█████████ | 90/100 [4:09:09<29:04, 174.48s/it]Best trial: 93. Best value: 0.969243:  91%|█████████ | 91/100 [4:09:09<28:16, 188.56s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                         Best trial: 93. Best value: 0.969243:  91%|█████████ | 91/100 [4:09:09<28:16, 188.56s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 27.7 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
27.7 M    Trainable params
0         Non-trainable params
27.7 M    Total params
110.864   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.052
Metric Loss/val improved by 0.023 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                         Best trial: 93. Best value: 0.969243:  91%|█████████ | 91/100 [4:11:49<28:16, 188.56s/it]Best trial: 93. Best value: 0.969243:  91%|█████████ | 91/100 [4:11:49<28:16, 188.56s/it]Best trial: 93. Best value: 0.969243:  92%|█████████▏| 92/100 [4:11:49<24:00, 180.07s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                         Best trial: 93. Best value: 0.969243:  92%|█████████▏| 92/100 [4:11:49<24:00, 180.07s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=11, input_dim=84, dec_in=84, output_dim=1, d_model=448, n_heads=4, e_layers=6, hidden_d_model=208, seq_layers=2, last_d_model=1120, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=96, pos_d_model=96, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.18, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=352), exp_settings='seq_len-12-lr-0.0038-d-448-hid_d-208-last_d-1120-tok_d-64-time_d-96-pos_d-96-e_layers-6-tok_conv_k-11-dropout-0.18-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.003767196849323027, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 03:47:27,521] Trial 183 finished with value: 1.712529790401459 and parameters: {'num_heads': 36, 'hidden_d_model': 208, 'token_conv_kernel': 11, 'last_d_model': 1120, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 96, 'pos_d_model': 96, 'd_model': 448, 'conv_out_dim': 352, 'e_layers': 6, 'learning_rate': 0.003767196849323027, 'dropout': 0.18, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 03:47:27,778] The parameter 'norm_type' in trial#186 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=11, input_dim=84, dec_in=84, output_dim=1, d_model=384, n_heads=4, e_layers=6, hidden_d_model=192, seq_layers=2, last_d_model=1248, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=80, pos_d_model=112, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=320), exp_settings='seq_len-12-lr-0.0073-d-384-hid_d-192-last_d-1248-tok_d-64-time_d-80-pos_d-112-e_layers-6-tok_conv_k-11-dropout-0.16-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.007312936499444074, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 03:50:07,782] Trial 186 finished with value: 0.9945468664169312 and parameters: {'num_heads': 36, 'hidden_d_model': 192, 'token_conv_kernel': 11, 'last_d_model': 1248, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 80, 'pos_d_model': 112, 'd_model': 384, 'conv_out_dim': 320, 'e_layers': 6, 'learning_rate': 0.007312936499444074, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 03:50:08,026] The parameter 'norm_type' in trial#188 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 25.4 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
25.4 M    Trainable params
0         Non-trainable params
25.4 M    Total params
101.536   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.032
Metric Loss/val improved by 0.003 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.028
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.028. Signaling Trainer to stop.
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                         Best trial: 93. Best value: 0.969243:  92%|█████████▏| 92/100 [4:15:08<24:00, 180.07s/it]Best trial: 93. Best value: 0.969243:  92%|█████████▏| 92/100 [4:15:08<24:00, 180.07s/it]Best trial: 93. Best value: 0.969243:  93%|█████████▎| 93/100 [4:15:08<21:40, 185.72s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                         Best trial: 93. Best value: 0.969243:  93%|█████████▎| 93/100 [4:15:08<21:40, 185.72s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 24.0 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
24.0 M    Trainable params
0         Non-trainable params
24.0 M    Total params
96.107    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.036
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 1.034
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 1.031
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                         Best trial: 93. Best value: 0.969243:  93%|█████████▎| 93/100 [4:18:29<21:40, 185.72s/it]Best trial: 93. Best value: 0.969243:  93%|█████████▎| 93/100 [4:18:29<21:40, 185.72s/it]Best trial: 93. Best value: 0.969243:  94%|█████████▍| 94/100 [4:18:29<19:02, 190.37s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                         Best trial: 93. Best value: 0.969243:  94%|█████████▍| 94/100 [4:18:29<19:02, 190.37s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=11, input_dim=84, dec_in=84, output_dim=1, d_model=320, n_heads=4, e_layers=6, hidden_d_model=176, seq_layers=2, last_d_model=960, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=96, pos_d_model=112, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=320), exp_settings='seq_len-12-lr-0.007-d-320-hid_d-176-last_d-960-tok_d-64-time_d-96-pos_d-112-e_layers-6-tok_conv_k-11-dropout-0.14-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.007000344397965164, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 03:53:26,698] Trial 188 finished with value: 3.3402656555175785 and parameters: {'num_heads': 36, 'hidden_d_model': 176, 'token_conv_kernel': 11, 'last_d_model': 960, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 96, 'pos_d_model': 112, 'd_model': 320, 'conv_out_dim': 320, 'e_layers': 6, 'learning_rate': 0.007000344397965164, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 03:53:27,115] The parameter 'norm_type' in trial#190 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=11, input_dim=84, dec_in=84, output_dim=1, d_model=416, n_heads=4, e_layers=6, hidden_d_model=208, seq_layers=2, last_d_model=1056, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=48, time_d_model=80, pos_d_model=112, combine_type='add', seq_len=14, pred_len=1, min_y_value=0.0, dropout=0.18, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=36, fc_layer_type='mha', conv_out_dim=320), exp_settings='seq_len-14-lr-0.005-d-416-hid_d-208-last_d-1056-tok_d-48-time_d-80-pos_d-112-e_layers-6-tok_conv_k-11-dropout-0.18-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.004968406131535042, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 03:56:47,918] Trial 190 finished with value: 0.9930260062217713 and parameters: {'num_heads': 36, 'hidden_d_model': 208, 'token_conv_kernel': 11, 'last_d_model': 1056, 'seq_len': 14, 'token_d_model': 48, 'time_d_model': 80, 'pos_d_model': 112, 'd_model': 416, 'conv_out_dim': 320, 'e_layers': 6, 'learning_rate': 0.004968406131535042, 'dropout': 0.18, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 03:56:48,188] The parameter 'norm_type' in trial#191 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 28.5 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
28.5 M    Trainable params
0         Non-trainable params
28.5 M    Total params
114.183   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.433
Metric Loss/val improved by 0.108 >= min_delta = 0.0. New best score: 1.324
Metric Loss/val improved by 0.264 >= min_delta = 0.0. New best score: 1.060
Metric Loss/val improved by 0.028 >= min_delta = 0.0. New best score: 1.032
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                         Best trial: 93. Best value: 0.969243:  94%|█████████▍| 94/100 [4:21:10<19:02, 190.37s/it]Best trial: 93. Best value: 0.969243:  94%|█████████▍| 94/100 [4:21:10<19:02, 190.37s/it]Best trial: 93. Best value: 0.969243:  95%|█████████▌| 95/100 [4:21:10<15:07, 181.50s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                         Best trial: 93. Best value: 0.969243:  95%|█████████▌| 95/100 [4:21:10<15:07, 181.50s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 20.5 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
20.5 M    Trainable params
0         Non-trainable params
20.5 M    Total params
81.935    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 2.318
Metric Loss/val improved by 1.256 >= min_delta = 0.0. New best score: 1.062
Metric Loss/val improved by 0.025 >= min_delta = 0.0. New best score: 1.037
Metric Loss/val improved by 0.004 >= min_delta = 0.0. New best score: 1.032
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                         Best trial: 93. Best value: 0.969243:  95%|█████████▌| 95/100 [4:23:53<15:07, 181.50s/it]Best trial: 93. Best value: 0.969243:  95%|█████████▌| 95/100 [4:23:53<15:07, 181.50s/it]Best trial: 93. Best value: 0.969243:  96%|█████████▌| 96/100 [4:23:53<11:44, 176.02s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                         Best trial: 93. Best value: 0.969243:  96%|█████████▌| 96/100 [4:23:53<11:44, 176.02s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=352, n_heads=4, e_layers=8, hidden_d_model=224, seq_layers=2, last_d_model=1248, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=96, pos_d_model=112, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=352), exp_settings='seq_len-12-lr-0.0128-d-352-hid_d-224-last_d-1248-tok_d-64-time_d-96-pos_d-112-e_layers-8-tok_conv_k-10-dropout-0.16-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.01284869961232461, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 03:59:28,705] Trial 191 finished with value: 1.0184834957122804 and parameters: {'num_heads': 36, 'hidden_d_model': 224, 'token_conv_kernel': 10, 'last_d_model': 1248, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 96, 'pos_d_model': 112, 'd_model': 352, 'conv_out_dim': 352, 'e_layers': 8, 'learning_rate': 0.01284869961232461, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 03:59:28,985] The parameter 'norm_type' in trial#193 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=11, input_dim=84, dec_in=84, output_dim=1, d_model=352, n_heads=4, e_layers=6, hidden_d_model=224, seq_layers=2, last_d_model=1152, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=80, pos_d_model=96, combine_type='add', seq_len=12, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=224), exp_settings='seq_len-12-lr-0.0245-d-352-hid_d-224-last_d-1152-tok_d-64-time_d-80-pos_d-96-e_layers-6-tok_conv_k-11-dropout-0.16-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.024453327867822464, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 04:02:11,957] Trial 193 finished with value: 0.9962277412414551 and parameters: {'num_heads': 36, 'hidden_d_model': 224, 'token_conv_kernel': 11, 'last_d_model': 1152, 'seq_len': 12, 'token_d_model': 64, 'time_d_model': 80, 'pos_d_model': 96, 'd_model': 352, 'conv_out_dim': 224, 'e_layers': 6, 'learning_rate': 0.024453327867822464, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 04:02:12,265] The parameter 'norm_type' in trial#195 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 23.7 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
23.7 M    Trainable params
0         Non-trainable params
23.7 M    Total params
94.802    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.035
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.035
Metric Loss/val improved by 0.004 >= min_delta = 0.0. New best score: 1.031
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.031. Signaling Trainer to stop.
                                                                                         Best trial: 93. Best value: 0.969243:  96%|█████████▌| 96/100 [4:27:25<11:44, 176.02s/it]Best trial: 93. Best value: 0.969243:  96%|█████████▌| 96/100 [4:27:25<11:44, 176.02s/it]Best trial: 93. Best value: 0.969243:  97%|█████████▋| 97/100 [4:27:25<09:20, 186.81s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                         Best trial: 93. Best value: 0.969243:  97%|█████████▋| 97/100 [4:27:25<09:20, 186.81s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 18.8 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
18.8 M    Trainable params
0         Non-trainable params
18.8 M    Total params
75.124    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.041
Metric Loss/val improved by 0.010 >= min_delta = 0.0. New best score: 1.031
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                         Best trial: 93. Best value: 0.969243:  97%|█████████▋| 97/100 [4:31:08<09:20, 186.81s/it]Best trial: 93. Best value: 0.969243:  97%|█████████▋| 97/100 [4:31:08<09:20, 186.81s/it]Best trial: 93. Best value: 0.969243:  98%|█████████▊| 98/100 [4:31:08<06:35, 197.77s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                         Best trial: 93. Best value: 0.969243:  98%|█████████▊| 98/100 [4:31:09<06:35, 197.77s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=11, input_dim=84, dec_in=84, output_dim=1, d_model=448, n_heads=4, e_layers=6, hidden_d_model=176, seq_layers=2, last_d_model=1056, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=48, time_d_model=64, pos_d_model=96, combine_type='add', seq_len=14, pred_len=1, min_y_value=0.0, dropout=0.18, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=288), exp_settings='seq_len-14-lr-0.0025-d-448-hid_d-176-last_d-1056-tok_d-48-time_d-64-pos_d-96-e_layers-6-tok_conv_k-11-dropout-0.18-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.002478185950894608, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 04:05:43,945] Trial 195 finished with value: 2.1278445720672607 and parameters: {'num_heads': 36, 'hidden_d_model': 176, 'token_conv_kernel': 11, 'last_d_model': 1056, 'seq_len': 14, 'token_d_model': 48, 'time_d_model': 64, 'pos_d_model': 96, 'd_model': 448, 'conv_out_dim': 288, 'e_layers': 6, 'learning_rate': 0.002478185950894608, 'dropout': 0.18, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 04:05:44,219] The parameter 'norm_type' in trial#197 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=352, n_heads=4, e_layers=6, hidden_d_model=208, seq_layers=2, last_d_model=1088, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=48, time_d_model=64, pos_d_model=80, combine_type='add', seq_len=16, pred_len=1, min_y_value=0.0, dropout=0.24000000000000002, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=288), exp_settings='seq_len-16-lr-0.006-d-352-hid_d-208-last_d-1088-tok_d-48-time_d-64-pos_d-80-e_layers-6-tok_conv_k-10-dropout-0.24000000000000002-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.006006196577716916, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 04:09:27,297] Trial 197 finished with value: 0.9990136861801148 and parameters: {'num_heads': 36, 'hidden_d_model': 208, 'token_conv_kernel': 10, 'last_d_model': 1088, 'seq_len': 16, 'token_d_model': 48, 'time_d_model': 64, 'pos_d_model': 80, 'd_model': 352, 'conv_out_dim': 288, 'e_layers': 6, 'learning_rate': 0.006006196577716916, 'dropout': 0.24000000000000002, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 04:09:27,572] The parameter 'norm_type' in trial#198 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 20.1 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
20.1 M    Trainable params
0         Non-trainable params
20.1 M    Total params
80.338    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.030
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.029
Monitored metric Loss/val did not improve in the last 15 records. Best score: 1.029. Signaling Trainer to stop.
                                                                                         Best trial: 93. Best value: 0.969243:  98%|█████████▊| 98/100 [4:33:56<06:35, 197.77s/it]Best trial: 93. Best value: 0.969243:  98%|█████████▊| 98/100 [4:33:56<06:35, 197.77s/it]Best trial: 93. Best value: 0.969243:  99%|█████████▉| 99/100 [4:33:56<03:08, 188.65s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [4, 96] and step=32, but the range is not divisible by `step`. It will be replaced by [4, 68].
  warnings.warn(
                                                                                         Best trial: 93. Best value: 0.969243:  99%|█████████▉| 99/100 [4:33:56<03:08, 188.65s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 19.1 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
19.1 M    Trainable params
0         Non-trainable params
19.1 M    Total params
76.252    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.064
Metric Loss/val improved by 0.033 >= min_delta = 0.0. New best score: 1.031
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 1.030
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 1.029
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                         Best trial: 93. Best value: 0.969243:  99%|█████████▉| 99/100 [4:37:07<03:08, 188.65s/it]Best trial: 93. Best value: 0.969243:  99%|█████████▉| 99/100 [4:37:07<03:08, 188.65s/it]Best trial: 93. Best value: 0.969243: 100%|██████████| 100/100 [4:37:07<00:00, 189.40s/it]Best trial: 93. Best value: 0.969243: 100%|██████████| 100/100 [4:37:07<00:00, 166.27s/it]

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=11, input_dim=84, dec_in=84, output_dim=1, d_model=352, n_heads=4, e_layers=6, hidden_d_model=192, seq_layers=2, last_d_model=928, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=48, time_d_model=96, pos_d_model=96, combine_type='add', seq_len=14, pred_len=1, min_y_value=0.0, dropout=0.2, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=288), exp_settings='seq_len-14-lr-0.0045-d-352-hid_d-192-last_d-928-tok_d-48-time_d-96-pos_d-96-e_layers-6-tok_conv_k-11-dropout-0.2-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.004526332553441206, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 04:12:14,670] Trial 198 finished with value: 1.415476417541504 and parameters: {'num_heads': 36, 'hidden_d_model': 192, 'token_conv_kernel': 11, 'last_d_model': 928, 'seq_len': 14, 'token_d_model': 48, 'time_d_model': 96, 'pos_d_model': 96, 'd_model': 352, 'conv_out_dim': 288, 'e_layers': 6, 'learning_rate': 0.004526332553441206, 'dropout': 0.2, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
[W 2024-08-14 04:12:14,939] The parameter 'norm_type' in trial#199 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['num_heads', 'hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-13-mlp_v3-search', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-13-mlp_v3-search', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-13-mlp_v3-search.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-13-mlp_v3-search', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-13-mlp_v3-search'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_98_withTime.csv', test_path='test_farm_98_withTime.csv', random_split=True), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(task_name='wind_power_forecasting with single farm station', name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=384, n_heads=4, e_layers=6, hidden_d_model=192, seq_layers=2, last_d_model=992, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=64, time_d_model=80, pos_d_model=96, combine_type='add', seq_len=16, pred_len=1, min_y_value=0.0, dropout=0.2, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=36, fc_layer_type='mha', conv_out_dim=224), exp_settings='seq_len-16-lr-0.008-d-384-hid_d-192-last_d-992-tok_d-64-time_d-80-pos_d-96-e_layers-6-tok_conv_k-9-dropout-0.2-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-36-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.00803020186054875, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-14 04:15:25,804] Trial 199 finished with value: 0.9887681245803833 and parameters: {'num_heads': 36, 'hidden_d_model': 192, 'token_conv_kernel': 9, 'last_d_model': 992, 'seq_len': 16, 'token_d_model': 64, 'time_d_model': 80, 'pos_d_model': 96, 'd_model': 384, 'conv_out_dim': 224, 'e_layers': 6, 'learning_rate': 0.00803020186054875, 'dropout': 0.2, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 93 with value: 0.969242835044861.
Number of finished trials: 200
Best trial:
Value: 0.969242835044861
Params: 
    num_heads: 36
    hidden_d_model: 176
    token_conv_kernel: 7
    last_d_model: 672
    seq_len: 12
    token_d_model: 64
    time_d_model: 96
    pos_d_model: 64
    d_model: 384
    conv_out_dim: 288
    e_layers: 6
    learning_rate: 0.017511033836363155
    dropout: 0.18
    combine_type: add
    use_pos_enc: True
    norm_type: layer
    fc_layer_type: mha
    batch_size: 1024
    train_epochs: 30
Top 10 trials saved to /data3/lsf/Pein/Power-Prediction/optuna_results/24-08-13-mlp_v3-search/24-08-13-mlp_v3-search-farm_89_top10_params.json
Total time taken:  16628.0352 seconds,  277.13 minutes
Done!
