[I 2024-08-12 14:28:19,782] Using an existing study with name '24-08-12-farm_98-mlpv3-search-time=hour' instead of creating a new one.
Creating study "24-08-12-farm_98-mlpv3-search-time=hour" with storage "sqlite:////data3/lsf/Pein/Power-Prediction/optuna_results/24-08-12/24-08-12-farm_98-mlpv3-search-time=hour.db?mode=wal"...
  0%|          | 0/100 [00:00<?, ?it/s]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA RTX 6000 Ada Generation') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 5.3 M  | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
5.3 M     Trainable params
0         Non-trainable params
5.3 M     Total params
21.318    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.203
Metric Loss/val improved by 0.068 >= min_delta = 0.0. New best score: 1.136
Metric Loss/val improved by 0.189 >= min_delta = 0.0. New best score: 0.947
Metric Loss/val improved by 0.191 >= min_delta = 0.0. New best score: 0.756
`Trainer.fit` stopped: `max_epochs=30` reached.
                                         0%|          | 0/100 [02:59<?, ?it/s]Best trial: 1. Best value: 0.770539:   0%|          | 0/100 [02:59<?, ?it/s]Best trial: 1. Best value: 0.770539:   1%|          | 1/100 [02:59<4:56:20, 179.60s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                       Best trial: 1. Best value: 0.770539:   1%|          | 1/100 [03:00<4:56:20, 179.60s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 14.2 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
14.2 M    Trainable params
0         Non-trainable params
14.2 M    Total params
56.629    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.214
Metric Loss/val improved by 0.018 >= min_delta = 0.0. New best score: 1.196
Metric Loss/val improved by 0.137 >= min_delta = 0.0. New best score: 1.059
Metric Loss/val improved by 0.072 >= min_delta = 0.0. New best score: 0.987
Metric Loss/val improved by 0.093 >= min_delta = 0.0. New best score: 0.894
Metric Loss/val improved by 0.059 >= min_delta = 0.0. New best score: 0.835
Metric Loss/val improved by 0.049 >= min_delta = 0.0. New best score: 0.786
Metric Loss/val improved by 0.016 >= min_delta = 0.0. New best score: 0.770
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                       Best trial: 1. Best value: 0.770539:   1%|          | 1/100 [07:24<4:56:20, 179.60s/it]Best trial: 2. Best value: 0.623499:   1%|          | 1/100 [07:24<4:56:20, 179.60s/it]Best trial: 2. Best value: 0.623499:   2%|▏         | 2/100 [07:24<6:15:25, 229.85s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                       Best trial: 2. Best value: 0.623499:   2%|▏         | 2/100 [07:25<6:15:25, 229.85s/it]Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=7, input_dim=84, dec_in=84, output_dim=1, d_model=416, n_heads=4, e_layers=2, hidden_d_model=816, seq_layers=2, last_d_model=512, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=8, time_d_model=240, pos_d_model=80, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.3, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=8, fc_layer_type='mha', conv_out_dim=320), exp_settings='seq_len-42-lr-0.0005-d-416-hid_d-816-last_d-512-time_d-240-e_layers-2-tok_conv_k-7-dropout-0.3-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=20, learning_rate=0.000544328334635811, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 14:31:19,385] Trial 1 finished with value: 0.7705389145761729 and parameters: {'hidden_d_model': 816, 'token_conv_kernel': 7, 'last_d_model': 512, 'seq_len': 42, 'token_d_model': 8, 'time_d_model': 240, 'pos_d_model': 80, 'd_model': 416, 'conv_out_dim': 320, 'e_layers': 2, 'learning_rate': 0.000544328334635811, 'dropout': 0.3, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 1 with value: 0.7705389145761729.
[W 2024-08-12 14:31:20,233] The parameter 'norm_type' in trial#2 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=576, n_heads=4, e_layers=4, hidden_d_model=736, seq_layers=2, last_d_model=480, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=8, time_d_model=176, pos_d_model=96, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.2, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=8, fc_layer_type='mha', conv_out_dim=352), exp_settings='seq_len-42-lr-0.0005-d-576-hid_d-736-last_d-480-time_d-176-e_layers-4-tok_conv_k-10-dropout-0.2-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=20, learning_rate=0.0005044283216862631, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 14:35:44,400] Trial 2 finished with value: 0.6234989974647761 and parameters: {'hidden_d_model': 736, 'token_conv_kernel': 10, 'last_d_model': 480, 'seq_len': 42, 'token_d_model': 8, 'time_d_model': 176, 'pos_d_model': 96, 'd_model': 576, 'conv_out_dim': 352, 'e_layers': 4, 'learning_rate': 0.0005044283216862631, 'dropout': 0.2, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 2 with value: 0.6234989974647761.
[W 2024-08-12 14:35:45,289] The parameter 'norm_type' in trial#4 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 32.4 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
32.4 M    Trainable params
0         Non-trainable params
32.4 M    Total params
129.429   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.230
Metric Loss/val improved by 0.015 >= min_delta = 0.0. New best score: 1.215
Metric Loss/val improved by 0.046 >= min_delta = 0.0. New best score: 1.169
Metric Loss/val improved by 0.035 >= min_delta = 0.0. New best score: 1.133
Metric Loss/val improved by 0.036 >= min_delta = 0.0. New best score: 1.098
Metric Loss/val improved by 0.116 >= min_delta = 0.0. New best score: 0.982
Metric Loss/val improved by 0.061 >= min_delta = 0.0. New best score: 0.920
Metric Loss/val improved by 0.029 >= min_delta = 0.0. New best score: 0.891
Metric Loss/val improved by 0.042 >= min_delta = 0.0. New best score: 0.849
Metric Loss/val improved by 0.003 >= min_delta = 0.0. New best score: 0.846
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                       Best trial: 2. Best value: 0.623499:   2%|▏         | 2/100 [14:38<6:15:25, 229.85s/it]Best trial: 3. Best value: 0.595208:   2%|▏         | 2/100 [14:38<6:15:25, 229.85s/it]Best trial: 3. Best value: 0.595208:   3%|▎         | 3/100 [14:38<8:42:00, 322.89s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                       Best trial: 3. Best value: 0.595208:   3%|▎         | 3/100 [14:39<8:42:00, 322.89s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 36.7 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
36.7 M    Trainable params
0         Non-trainable params
36.7 M    Total params
146.983   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.209
Metric Loss/val improved by 0.021 >= min_delta = 0.0. New best score: 1.188
Metric Loss/val improved by 0.099 >= min_delta = 0.0. New best score: 1.088
Metric Loss/val improved by 0.227 >= min_delta = 0.0. New best score: 0.861
Metric Loss/val improved by 0.017 >= min_delta = 0.0. New best score: 0.844
Metric Loss/val improved by 0.087 >= min_delta = 0.0. New best score: 0.757
Metric Loss/val improved by 0.062 >= min_delta = 0.0. New best score: 0.695
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 0.694
Metric Loss/val improved by 0.009 >= min_delta = 0.0. New best score: 0.685
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 0.685
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 0.683
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                       Best trial: 3. Best value: 0.595208:   3%|▎         | 3/100 [22:14<8:42:00, 322.89s/it]Best trial: 6. Best value: 0.590104:   3%|▎         | 3/100 [22:14<8:42:00, 322.89s/it]Best trial: 6. Best value: 0.590104:   4%|▍         | 4/100 [22:14<10:01:07, 375.70s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                        Best trial: 6. Best value: 0.590104:   4%|▍         | 4/100 [22:15<10:01:07, 375.70s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=13, input_dim=84, dec_in=84, output_dim=1, d_model=800, n_heads=4, e_layers=6, hidden_d_model=672, seq_layers=2, last_d_model=480, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=8, time_d_model=160, pos_d_model=128, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.2, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=8, fc_layer_type='mha', conv_out_dim=192), exp_settings='seq_len-42-lr-0.0004-d-800-hid_d-672-last_d-480-time_d-160-e_layers-6-tok_conv_k-13-dropout-0.2-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=20, learning_rate=0.0003930201830633981, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 14:42:58,002] Trial 4 finished with value: 0.6471409240737558 and parameters: {'hidden_d_model': 672, 'token_conv_kernel': 13, 'last_d_model': 480, 'seq_len': 42, 'token_d_model': 8, 'time_d_model': 160, 'pos_d_model': 128, 'd_model': 800, 'conv_out_dim': 192, 'e_layers': 6, 'learning_rate': 0.0003930201830633981, 'dropout': 0.2, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 3 with value: 0.5952083382755519.
[W 2024-08-12 14:42:58,850] The parameter 'norm_type' in trial#6 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=704, n_heads=4, e_layers=8, hidden_d_model=704, seq_layers=2, last_d_model=384, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=144, pos_d_model=112, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.14, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=8, fc_layer_type='mha', conv_out_dim=256), exp_settings='seq_len-42-lr-0.001-d-704-hid_d-704-last_d-384-time_d-144-e_layers-8-tok_conv_k-9-dropout-0.14-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=20, learning_rate=0.0010180589130511797, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 14:50:34,677] Trial 6 finished with value: 0.5901038590818644 and parameters: {'hidden_d_model': 704, 'token_conv_kernel': 9, 'last_d_model': 384, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 144, 'pos_d_model': 112, 'd_model': 704, 'conv_out_dim': 256, 'e_layers': 8, 'learning_rate': 0.0010180589130511797, 'dropout': 0.14, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 6 with value: 0.5901038590818644.
[W 2024-08-12 14:50:35,520] The parameter 'norm_type' in trial#9 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 30.5 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
30.5 M    Trainable params
0         Non-trainable params
30.5 M    Total params
122.054   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.106
Metric Loss/val improved by 0.149 >= min_delta = 0.0. New best score: 0.957
Monitored metric Loss/val did not improve in the last 15 records. Best score: 0.957. Signaling Trainer to stop.
                                                                                        Best trial: 6. Best value: 0.590104:   4%|▍         | 4/100 [25:42<10:01:07, 375.70s/it]Best trial: 6. Best value: 0.590104:   4%|▍         | 4/100 [25:42<10:01:07, 375.70s/it]Best trial: 6. Best value: 0.590104:   5%|▌         | 5/100 [25:42<8:19:10, 315.26s/it] /home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                       Best trial: 6. Best value: 0.590104:   5%|▌         | 5/100 [25:43<8:19:10, 315.26s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 35.5 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
35.5 M    Trainable params
0         Non-trainable params
35.5 M    Total params
141.830   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.227
Metric Loss/val improved by 0.004 >= min_delta = 0.0. New best score: 1.224
Metric Loss/val improved by 0.050 >= min_delta = 0.0. New best score: 1.173
Metric Loss/val improved by 0.228 >= min_delta = 0.0. New best score: 0.946
Metric Loss/val improved by 0.183 >= min_delta = 0.0. New best score: 0.762
Metric Loss/val improved by 0.053 >= min_delta = 0.0. New best score: 0.709
Metric Loss/val improved by 0.041 >= min_delta = 0.0. New best score: 0.668
Monitored metric Loss/val did not improve in the last 15 records. Best score: 0.668. Signaling Trainer to stop.
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                       Best trial: 6. Best value: 0.590104:   5%|▌         | 5/100 [34:35<8:19:10, 315.26s/it]Best trial: 6. Best value: 0.590104:   5%|▌         | 5/100 [34:35<8:19:10, 315.26s/it]Best trial: 6. Best value: 0.590104:   6%|▌         | 6/100 [34:35<10:09:45, 389.21s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                        Best trial: 6. Best value: 0.590104:   6%|▌         | 6/100 [34:36<10:09:45, 389.21s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=704, n_heads=4, e_layers=6, hidden_d_model=384, seq_layers=2, last_d_model=512, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=160, pos_d_model=144, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.22, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=8, fc_layer_type='mha', conv_out_dim=320), exp_settings='seq_len-42-lr-0.0017-d-704-hid_d-384-last_d-512-time_d-160-e_layers-6-tok_conv_k-9-dropout-0.22-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0017024194169585804, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 14:54:02,777] Trial 9 finished with value: 0.9574044063687326 and parameters: {'hidden_d_model': 384, 'token_conv_kernel': 9, 'last_d_model': 512, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 160, 'pos_d_model': 144, 'd_model': 704, 'conv_out_dim': 320, 'e_layers': 6, 'learning_rate': 0.0017024194169585804, 'dropout': 0.22, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 6 with value: 0.5901038590818644.
[W 2024-08-12 14:54:03,017] The parameter 'norm_type' in trial#10 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=11, input_dim=84, dec_in=84, output_dim=1, d_model=704, n_heads=4, e_layers=8, hidden_d_model=656, seq_layers=2, last_d_model=480, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=8, time_d_model=176, pos_d_model=112, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.18, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=8, fc_layer_type='mha', conv_out_dim=352), exp_settings='seq_len-42-lr-0.0003-d-704-hid_d-656-last_d-480-time_d-176-e_layers-8-tok_conv_k-11-dropout-0.18-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.00034867661579763236, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 15:02:55,533] Trial 10 finished with value: 0.6021412447094918 and parameters: {'hidden_d_model': 656, 'token_conv_kernel': 11, 'last_d_model': 480, 'seq_len': 42, 'token_d_model': 8, 'time_d_model': 176, 'pos_d_model': 112, 'd_model': 704, 'conv_out_dim': 352, 'e_layers': 8, 'learning_rate': 0.00034867661579763236, 'dropout': 0.18, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 6 with value: 0.5901038590818644.
[W 2024-08-12 15:02:55,798] The parameter 'norm_type' in trial#12 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 16.6 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
16.6 M    Trainable params
0         Non-trainable params
16.6 M    Total params
66.345    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.246
Metric Loss/val improved by 0.038 >= min_delta = 0.0. New best score: 1.208
Metric Loss/val improved by 0.082 >= min_delta = 0.0. New best score: 1.125
Metric Loss/val improved by 0.237 >= min_delta = 0.0. New best score: 0.888
Metric Loss/val improved by 0.012 >= min_delta = 0.0. New best score: 0.876
Metric Loss/val improved by 0.038 >= min_delta = 0.0. New best score: 0.838
Metric Loss/val improved by 0.010 >= min_delta = 0.0. New best score: 0.828
Metric Loss/val improved by 0.004 >= min_delta = 0.0. New best score: 0.824
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                        Best trial: 6. Best value: 0.590104:   6%|▌         | 6/100 [40:14<10:09:45, 389.21s/it]Best trial: 6. Best value: 0.590104:   6%|▌         | 6/100 [40:14<10:09:45, 389.21s/it]Best trial: 6. Best value: 0.590104:   7%|▋         | 7/100 [40:14<9:37:39, 372.68s/it] /home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                       Best trial: 6. Best value: 0.590104:   7%|▋         | 7/100 [40:14<9:37:39, 372.68s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 35.0 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
35.0 M    Trainable params
0         Non-trainable params
35.0 M    Total params
139.822   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.245
Metric Loss/val improved by 0.061 >= min_delta = 0.0. New best score: 1.184
Metric Loss/val improved by 0.269 >= min_delta = 0.0. New best score: 0.915
Monitored metric Loss/val did not improve in the last 15 records. Best score: 0.915. Signaling Trainer to stop.
                                                                                       Best trial: 6. Best value: 0.590104:   7%|▋         | 7/100 [45:07<9:37:39, 372.68s/it]Best trial: 6. Best value: 0.590104:   7%|▋         | 7/100 [45:07<9:37:39, 372.68s/it]Best trial: 6. Best value: 0.590104:   8%|▊         | 8/100 [45:07<8:52:33, 347.32s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                       Best trial: 6. Best value: 0.590104:   8%|▊         | 8/100 [45:07<8:52:33, 347.32s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=14, input_dim=84, dec_in=84, output_dim=1, d_model=640, n_heads=4, e_layers=4, hidden_d_model=640, seq_layers=2, last_d_model=672, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=8, time_d_model=144, pos_d_model=160, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.18, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=8, fc_layer_type='mha', conv_out_dim=256), exp_settings='seq_len-42-lr-0.0007-d-640-hid_d-640-last_d-672-time_d-144-e_layers-4-tok_conv_k-14-dropout-0.18-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0006779573987374036, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 15:08:34,188] Trial 12 finished with value: 0.6146017400547863 and parameters: {'hidden_d_model': 640, 'token_conv_kernel': 14, 'last_d_model': 672, 'seq_len': 42, 'token_d_model': 8, 'time_d_model': 144, 'pos_d_model': 160, 'd_model': 640, 'conv_out_dim': 256, 'e_layers': 4, 'learning_rate': 0.0006779573987374036, 'dropout': 0.18, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 6 with value: 0.5901038590818644.
[W 2024-08-12 15:08:34,437] The parameter 'norm_type' in trial#13 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=11, input_dim=84, dec_in=84, output_dim=1, d_model=576, n_heads=4, e_layers=10, hidden_d_model=800, seq_layers=2, last_d_model=384, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=112, pos_d_model=192, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.18, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=8, fc_layer_type='mha', conv_out_dim=320), exp_settings='seq_len-42-lr-0.001-d-576-hid_d-800-last_d-384-time_d-112-e_layers-10-tok_conv_k-11-dropout-0.18-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.00098418556928527, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 15:13:27,190] Trial 13 finished with value: 0.7591603592038155 and parameters: {'hidden_d_model': 800, 'token_conv_kernel': 11, 'last_d_model': 384, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 112, 'pos_d_model': 192, 'd_model': 576, 'conv_out_dim': 320, 'e_layers': 10, 'learning_rate': 0.00098418556928527, 'dropout': 0.18, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 6 with value: 0.5901038590818644.
[W 2024-08-12 15:13:27,534] The parameter 'norm_type' in trial#15 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 49.0 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
49.0 M    Trainable params
0         Non-trainable params
49.0 M    Total params
195.890   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.215
Metric Loss/val improved by 0.003 >= min_delta = 0.0. New best score: 1.212
Metric Loss/val improved by 0.031 >= min_delta = 0.0. New best score: 1.181
Metric Loss/val improved by 0.276 >= min_delta = 0.0. New best score: 0.905
Metric Loss/val improved by 0.107 >= min_delta = 0.0. New best score: 0.798
Metric Loss/val improved by 0.027 >= min_delta = 0.0. New best score: 0.772
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                       Best trial: 6. Best value: 0.590104:   8%|▊         | 8/100 [57:15<8:52:33, 347.32s/it]Best trial: 6. Best value: 0.590104:   8%|▊         | 8/100 [57:15<8:52:33, 347.32s/it]Best trial: 6. Best value: 0.590104:   9%|▉         | 9/100 [57:15<11:47:16, 466.34s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                        Best trial: 6. Best value: 0.590104:   9%|▉         | 9/100 [57:15<11:47:16, 466.34s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 59.9 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
59.9 M    Trainable params
0         Non-trainable params
59.9 M    Total params
239.714   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.215
Metric Loss/val improved by 0.009 >= min_delta = 0.0. New best score: 1.206
Metric Loss/val improved by 0.108 >= min_delta = 0.0. New best score: 1.098
Metric Loss/val improved by 0.135 >= min_delta = 0.0. New best score: 0.962
Metric Loss/val improved by 0.076 >= min_delta = 0.0. New best score: 0.886
Metric Loss/val improved by 0.221 >= min_delta = 0.0. New best score: 0.665
Monitored metric Loss/val did not improve in the last 15 records. Best score: 0.665. Signaling Trainer to stop.
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                        Best trial: 6. Best value: 0.590104:   9%|▉         | 9/100 [1:11:35<11:47:16, 466.34s/it]Best trial: 17. Best value: 0.562415:   9%|▉         | 9/100 [1:11:35<11:47:16, 466.34s/it]Best trial: 17. Best value: 0.562415:  10%|█         | 10/100 [1:11:35<14:41:49, 587.89s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                            Best trial: 17. Best value: 0.562415:  10%|█         | 10/100 [1:11:35<14:41:49, 587.89s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=832, n_heads=4, e_layers=8, hidden_d_model=784, seq_layers=2, last_d_model=160, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=160, pos_d_model=192, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.26, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=8, fc_layer_type='mha', conv_out_dim=256), exp_settings='seq_len-42-lr-0.0004-d-832-hid_d-784-last_d-160-time_d-160-e_layers-8-tok_conv_k-9-dropout-0.26-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0003537472468955695, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 15:25:35,235] Trial 15 finished with value: 0.6181559449061752 and parameters: {'hidden_d_model': 784, 'token_conv_kernel': 9, 'last_d_model': 160, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 160, 'pos_d_model': 192, 'd_model': 832, 'conv_out_dim': 256, 'e_layers': 8, 'learning_rate': 0.0003537472468955695, 'dropout': 0.26, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 6 with value: 0.5901038590818644.
[W 2024-08-12 15:25:35,512] The parameter 'norm_type' in trial#17 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=832, n_heads=4, e_layers=10, hidden_d_model=688, seq_layers=2, last_d_model=512, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=144, pos_d_model=128, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.22, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=8, fc_layer_type='mha', conv_out_dim=224), exp_settings='seq_len-42-lr-0.0005-d-832-hid_d-688-last_d-512-time_d-144-e_layers-10-tok_conv_k-10-dropout-0.22-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0005253522666365598, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 15:39:55,293] Trial 17 finished with value: 0.562415336444974 and parameters: {'hidden_d_model': 688, 'token_conv_kernel': 10, 'last_d_model': 512, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 144, 'pos_d_model': 128, 'd_model': 832, 'conv_out_dim': 224, 'e_layers': 10, 'learning_rate': 0.0005253522666365598, 'dropout': 0.22, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 17 with value: 0.562415336444974.
[W 2024-08-12 15:39:55,560] The parameter 'norm_type' in trial#20 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 33.8 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
33.8 M    Trainable params
0         Non-trainable params
33.8 M    Total params
135.094   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.257
Metric Loss/val improved by 0.216 >= min_delta = 0.0. New best score: 1.042
Metric Loss/val improved by 0.093 >= min_delta = 0.0. New best score: 0.949
Metric Loss/val improved by 0.110 >= min_delta = 0.0. New best score: 0.839
Monitored metric Loss/val did not improve in the last 15 records. Best score: 0.839. Signaling Trainer to stop.
                                                                                            Best trial: 17. Best value: 0.562415:  10%|█         | 10/100 [1:20:24<14:41:49, 587.89s/it]Best trial: 17. Best value: 0.562415:  10%|█         | 10/100 [1:20:24<14:41:49, 587.89s/it]Best trial: 17. Best value: 0.562415:  11%|█         | 11/100 [1:20:24<14:05:29, 570.00s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                            Best trial: 17. Best value: 0.562415:  11%|█         | 11/100 [1:20:25<14:05:29, 570.00s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 20.8 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
20.8 M    Trainable params
0         Non-trainable params
20.8 M    Total params
83.224    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.208
Metric Loss/val improved by 0.056 >= min_delta = 0.0. New best score: 1.152
Metric Loss/val improved by 0.172 >= min_delta = 0.0. New best score: 0.980
Metric Loss/val improved by 0.115 >= min_delta = 0.0. New best score: 0.865
Metric Loss/val improved by 0.192 >= min_delta = 0.0. New best score: 0.673
Monitored metric Loss/val did not improve in the last 15 records. Best score: 0.673. Signaling Trainer to stop.
                                                                                            Best trial: 17. Best value: 0.562415:  11%|█         | 11/100 [1:26:20<14:05:29, 570.00s/it]Best trial: 17. Best value: 0.562415:  11%|█         | 11/100 [1:26:20<14:05:29, 570.00s/it]Best trial: 17. Best value: 0.562415:  12%|█▏        | 12/100 [1:26:20<12:20:19, 504.76s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                            Best trial: 17. Best value: 0.562415:  12%|█▏        | 12/100 [1:26:20<12:20:19, 504.76s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=13, input_dim=84, dec_in=84, output_dim=1, d_model=768, n_heads=4, e_layers=6, hidden_d_model=896, seq_layers=2, last_d_model=640, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=192, pos_d_model=144, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.22, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=8, fc_layer_type='mha', conv_out_dim=192), exp_settings='seq_len-42-lr-0.0008-d-768-hid_d-896-last_d-640-time_d-192-e_layers-6-tok_conv_k-13-dropout-0.22-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0007803924648251736, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 15:48:44,730] Trial 20 finished with value: 0.6531516006216407 and parameters: {'hidden_d_model': 896, 'token_conv_kernel': 13, 'last_d_model': 640, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 192, 'pos_d_model': 144, 'd_model': 768, 'conv_out_dim': 192, 'e_layers': 6, 'learning_rate': 0.0007803924648251736, 'dropout': 0.22, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 17 with value: 0.562415336444974.
[W 2024-08-12 15:48:45,007] The parameter 'norm_type' in trial#23 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=704, n_heads=4, e_layers=4, hidden_d_model=752, seq_layers=2, last_d_model=512, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=160, pos_d_model=144, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.2, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=8, fc_layer_type='mha', conv_out_dim=224), exp_settings='seq_len-42-lr-0.0007-d-704-hid_d-752-last_d-512-time_d-160-e_layers-4-tok_conv_k-10-dropout-0.2-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0006792184893195017, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 15:54:40,285] Trial 23 finished with value: 0.6760878985747696 and parameters: {'hidden_d_model': 752, 'token_conv_kernel': 10, 'last_d_model': 512, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 160, 'pos_d_model': 144, 'd_model': 704, 'conv_out_dim': 224, 'e_layers': 4, 'learning_rate': 0.0006792184893195017, 'dropout': 0.2, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 17 with value: 0.562415336444974.
[W 2024-08-12 15:54:40,533] The parameter 'norm_type' in trial#24 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 26.5 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
26.5 M    Trainable params
0         Non-trainable params
26.5 M    Total params
105.893   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.232
Metric Loss/val improved by 0.022 >= min_delta = 0.0. New best score: 1.210
Metric Loss/val improved by 0.133 >= min_delta = 0.0. New best score: 1.076
Metric Loss/val improved by 0.150 >= min_delta = 0.0. New best score: 0.927
Metric Loss/val improved by 0.078 >= min_delta = 0.0. New best score: 0.849
Metric Loss/val improved by 0.009 >= min_delta = 0.0. New best score: 0.840
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                            Best trial: 17. Best value: 0.562415:  12%|█▏        | 12/100 [1:34:13<12:20:19, 504.76s/it]Best trial: 17. Best value: 0.562415:  12%|█▏        | 12/100 [1:34:13<12:20:19, 504.76s/it]Best trial: 17. Best value: 0.562415:  13%|█▎        | 13/100 [1:34:13<11:57:46, 495.02s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                            Best trial: 17. Best value: 0.562415:  13%|█▎        | 13/100 [1:34:13<11:57:46, 495.02s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 42.0 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
42.0 M    Trainable params
0         Non-trainable params
42.0 M    Total params
167.824   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.245
Metric Loss/val improved by 0.040 >= min_delta = 0.0. New best score: 1.205
Metric Loss/val improved by 0.240 >= min_delta = 0.0. New best score: 0.965
Metric Loss/val improved by 0.029 >= min_delta = 0.0. New best score: 0.936
Metric Loss/val improved by 0.115 >= min_delta = 0.0. New best score: 0.821
Metric Loss/val improved by 0.011 >= min_delta = 0.0. New best score: 0.810
Metric Loss/val improved by 0.099 >= min_delta = 0.0. New best score: 0.711
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                            Best trial: 17. Best value: 0.562415:  13%|█▎        | 13/100 [1:44:23<11:57:46, 495.02s/it]Best trial: 26. Best value: 0.544069:  13%|█▎        | 13/100 [1:44:23<11:57:46, 495.02s/it]Best trial: 26. Best value: 0.544069:  14%|█▍        | 14/100 [1:44:23<12:39:25, 529.83s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                            Best trial: 26. Best value: 0.544069:  14%|█▍        | 14/100 [1:44:23<12:39:25, 529.83s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=608, n_heads=4, e_layers=8, hidden_d_model=784, seq_layers=2, last_d_model=672, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=144, pos_d_model=128, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.16, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=8, fc_layer_type='mha', conv_out_dim=128), exp_settings='seq_len-42-lr-0.0005-d-608-hid_d-784-last_d-672-time_d-144-e_layers-8-tok_conv_k-10-dropout-0.16-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0005151253067396795, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 16:02:32,877] Trial 24 finished with value: 0.642329403758049 and parameters: {'hidden_d_model': 784, 'token_conv_kernel': 10, 'last_d_model': 672, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 144, 'pos_d_model': 128, 'd_model': 608, 'conv_out_dim': 128, 'e_layers': 8, 'learning_rate': 0.0005151253067396795, 'dropout': 0.16, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 17 with value: 0.562415336444974.
[W 2024-08-12 16:02:33,215] The parameter 'norm_type' in trial#26 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=736, n_heads=4, e_layers=8, hidden_d_model=736, seq_layers=2, last_d_model=416, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=144, pos_d_model=144, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.24000000000000002, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=8, fc_layer_type='mha', conv_out_dim=320), exp_settings='seq_len-42-lr-0.0005-d-736-hid_d-736-last_d-416-time_d-144-e_layers-8-tok_conv_k-10-dropout-0.24000000000000002-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0005314744906339955, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 16:12:43,165] Trial 26 finished with value: 0.5440689027309418 and parameters: {'hidden_d_model': 736, 'token_conv_kernel': 10, 'last_d_model': 416, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 144, 'pos_d_model': 144, 'd_model': 736, 'conv_out_dim': 320, 'e_layers': 8, 'learning_rate': 0.0005314744906339955, 'dropout': 0.24000000000000002, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 16:12:43,412] The parameter 'norm_type' in trial#28 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 64.3 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
64.3 M    Trainable params
0         Non-trainable params
64.3 M    Total params
257.230   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.220
Metric Loss/val improved by 0.008 >= min_delta = 0.0. New best score: 1.212
Metric Loss/val improved by 0.333 >= min_delta = 0.0. New best score: 0.879
Metric Loss/val improved by 0.147 >= min_delta = 0.0. New best score: 0.732
Metric Loss/val improved by 0.039 >= min_delta = 0.0. New best score: 0.693
Metric Loss/val improved by 0.004 >= min_delta = 0.0. New best score: 0.688
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                            Best trial: 26. Best value: 0.544069:  14%|█▍        | 14/100 [1:55:49<12:39:25, 529.83s/it]Best trial: 26. Best value: 0.544069:  14%|█▍        | 14/100 [1:55:49<12:39:25, 529.83s/it]Best trial: 26. Best value: 0.544069:  15%|█▌        | 15/100 [1:55:49<13:37:25, 577.00s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                            Best trial: 26. Best value: 0.544069:  15%|█▌        | 15/100 [1:55:49<13:37:25, 577.00s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 63.9 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
63.9 M    Trainable params
0         Non-trainable params
63.9 M    Total params
255.579   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.227
Metric Loss/val improved by 0.053 >= min_delta = 0.0. New best score: 1.174
Metric Loss/val improved by 0.120 >= min_delta = 0.0. New best score: 1.054
Metric Loss/val improved by 0.073 >= min_delta = 0.0. New best score: 0.981
Metric Loss/val improved by 0.076 >= min_delta = 0.0. New best score: 0.905
Metric Loss/val improved by 0.063 >= min_delta = 0.0. New best score: 0.842
Metric Loss/val improved by 0.172 >= min_delta = 0.0. New best score: 0.670
Metric Loss/val improved by 0.010 >= min_delta = 0.0. New best score: 0.660
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                            Best trial: 26. Best value: 0.544069:  15%|█▌        | 15/100 [2:07:13<13:37:25, 577.00s/it]Best trial: 26. Best value: 0.544069:  15%|█▌        | 15/100 [2:07:13<13:37:25, 577.00s/it]Best trial: 26. Best value: 0.544069:  16%|█▌        | 16/100 [2:07:13<14:12:37, 609.02s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                            Best trial: 26. Best value: 0.544069:  16%|█▌        | 16/100 [2:07:13<14:12:37, 609.02s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=864, n_heads=4, e_layers=10, hidden_d_model=384, seq_layers=2, last_d_model=352, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=112, pos_d_model=176, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.2, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=8, fc_layer_type='mha', conv_out_dim=256), exp_settings='seq_len-42-lr-0.0005-d-864-hid_d-384-last_d-352-time_d-112-e_layers-10-tok_conv_k-9-dropout-0.2-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0005494240255426967, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 16:24:09,471] Trial 28 finished with value: 0.57463438808918 and parameters: {'hidden_d_model': 384, 'token_conv_kernel': 9, 'last_d_model': 352, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 112, 'pos_d_model': 176, 'd_model': 864, 'conv_out_dim': 256, 'e_layers': 10, 'learning_rate': 0.0005494240255426967, 'dropout': 0.2, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 16:24:09,744] The parameter 'norm_type' in trial#30 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=8, input_dim=84, dec_in=84, output_dim=1, d_model=864, n_heads=4, e_layers=10, hidden_d_model=576, seq_layers=2, last_d_model=544, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=176, pos_d_model=112, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.22, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=8, fc_layer_type='mha', conv_out_dim=256), exp_settings='seq_len-42-lr-0.0003-d-864-hid_d-576-last_d-544-time_d-176-e_layers-10-tok_conv_k-8-dropout-0.22-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.00029614546868207215, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 16:35:32,853] Trial 30 finished with value: 0.5978953525424004 and parameters: {'hidden_d_model': 576, 'token_conv_kernel': 8, 'last_d_model': 544, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 176, 'pos_d_model': 112, 'd_model': 864, 'conv_out_dim': 256, 'e_layers': 10, 'learning_rate': 0.00029614546868207215, 'dropout': 0.22, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 16:35:33,114] The parameter 'norm_type' in trial#33 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 32.8 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
32.8 M    Trainable params
0         Non-trainable params
32.8 M    Total params
131.030   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.254
Metric Loss/val improved by 0.003 >= min_delta = 0.0. New best score: 1.251
Metric Loss/val improved by 0.136 >= min_delta = 0.0. New best score: 1.114
Metric Loss/val improved by 0.109 >= min_delta = 0.0. New best score: 1.005
Metric Loss/val improved by 0.114 >= min_delta = 0.0. New best score: 0.891
Metric Loss/val improved by 0.068 >= min_delta = 0.0. New best score: 0.823
Metric Loss/val improved by 0.091 >= min_delta = 0.0. New best score: 0.732
Metric Loss/val improved by 0.041 >= min_delta = 0.0. New best score: 0.691
Monitored metric Loss/val did not improve in the last 15 records. Best score: 0.691. Signaling Trainer to stop.
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                            Best trial: 26. Best value: 0.544069:  16%|█▌        | 16/100 [2:14:15<14:12:37, 609.02s/it]Best trial: 26. Best value: 0.544069:  16%|█▌        | 16/100 [2:14:15<14:12:37, 609.02s/it]Best trial: 26. Best value: 0.544069:  17%|█▋        | 17/100 [2:14:15<12:44:50, 552.89s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                            Best trial: 26. Best value: 0.544069:  17%|█▋        | 17/100 [2:14:15<12:44:50, 552.89s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 55.0 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
55.0 M    Trainable params
0         Non-trainable params
55.0 M    Total params
219.885   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.244
Metric Loss/val improved by 0.022 >= min_delta = 0.0. New best score: 1.222
Metric Loss/val improved by 0.209 >= min_delta = 0.0. New best score: 1.013
Metric Loss/val improved by 0.144 >= min_delta = 0.0. New best score: 0.869
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                            Best trial: 26. Best value: 0.544069:  17%|█▋        | 17/100 [2:24:26<12:44:50, 552.89s/it]Best trial: 26. Best value: 0.544069:  17%|█▋        | 17/100 [2:24:26<12:44:50, 552.89s/it]Best trial: 26. Best value: 0.544069:  18%|█▊        | 18/100 [2:24:26<12:59:27, 570.33s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                            Best trial: 26. Best value: 0.544069:  18%|█▊        | 18/100 [2:24:26<12:59:27, 570.33s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=8, input_dim=84, dec_in=84, output_dim=1, d_model=768, n_heads=4, e_layers=6, hidden_d_model=688, seq_layers=2, last_d_model=352, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=176, pos_d_model=192, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.24000000000000002, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=8, fc_layer_type='mha', conv_out_dim=256), exp_settings='seq_len-42-lr-0.0005-d-768-hid_d-688-last_d-352-time_d-176-e_layers-6-tok_conv_k-8-dropout-0.24000000000000002-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.000543060501157583, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 16:42:35,212] Trial 33 finished with value: 0.5933957453817129 and parameters: {'hidden_d_model': 688, 'token_conv_kernel': 8, 'last_d_model': 352, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 176, 'pos_d_model': 192, 'd_model': 768, 'conv_out_dim': 256, 'e_layers': 6, 'learning_rate': 0.000543060501157583, 'dropout': 0.24000000000000002, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 16:42:35,474] The parameter 'norm_type' in trial#34 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=896, n_heads=4, e_layers=8, hidden_d_model=784, seq_layers=2, last_d_model=256, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=208, pos_d_model=80, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.26, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=8, fc_layer_type='mha', conv_out_dim=224), exp_settings='seq_len-42-lr-0.0005-d-896-hid_d-784-last_d-256-time_d-208-e_layers-8-tok_conv_k-9-dropout-0.26-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0004672851988167594, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 16:52:46,142] Trial 34 finished with value: 0.768236067891121 and parameters: {'hidden_d_model': 784, 'token_conv_kernel': 9, 'last_d_model': 256, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 208, 'pos_d_model': 80, 'd_model': 896, 'conv_out_dim': 224, 'e_layers': 8, 'learning_rate': 0.0004672851988167594, 'dropout': 0.26, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 16:52:46,399] The parameter 'norm_type' in trial#36 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 44.7 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
44.7 M    Trainable params
0         Non-trainable params
44.7 M    Total params
178.899   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.213
Metric Loss/val improved by 0.015 >= min_delta = 0.0. New best score: 1.198
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 1.196
Metric Loss/val improved by 0.280 >= min_delta = 0.0. New best score: 0.915
Metric Loss/val improved by 0.015 >= min_delta = 0.0. New best score: 0.901
Metric Loss/val improved by 0.006 >= min_delta = 0.0. New best score: 0.895
Metric Loss/val improved by 0.087 >= min_delta = 0.0. New best score: 0.808
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                            Best trial: 26. Best value: 0.544069:  18%|█▊        | 18/100 [2:32:59<12:59:27, 570.33s/it]Best trial: 26. Best value: 0.544069:  18%|█▊        | 18/100 [2:32:59<12:59:27, 570.33s/it]Best trial: 26. Best value: 0.544069:  19%|█▉        | 19/100 [2:32:59<12:26:52, 553.24s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                            Best trial: 26. Best value: 0.544069:  19%|█▉        | 19/100 [2:33:00<12:26:52, 553.24s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 62.8 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
62.8 M    Trainable params
0         Non-trainable params
62.8 M    Total params
251.013   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.247
Metric Loss/val improved by 0.223 >= min_delta = 0.0. New best score: 1.025
Metric Loss/val improved by 0.066 >= min_delta = 0.0. New best score: 0.958
Metric Loss/val improved by 0.100 >= min_delta = 0.0. New best score: 0.858
Monitored metric Loss/val did not improve in the last 15 records. Best score: 0.858. Signaling Trainer to stop.
                                                                                            Best trial: 26. Best value: 0.544069:  19%|█▉        | 19/100 [2:43:15<12:26:52, 553.24s/it]Best trial: 26. Best value: 0.544069:  19%|█▉        | 19/100 [2:43:15<12:26:52, 553.24s/it]Best trial: 26. Best value: 0.544069:  20%|██        | 20/100 [2:43:15<12:42:40, 572.01s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                            Best trial: 26. Best value: 0.544069:  20%|██        | 20/100 [2:43:15<12:42:40, 572.01s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=11, input_dim=84, dec_in=84, output_dim=1, d_model=768, n_heads=4, e_layers=8, hidden_d_model=624, seq_layers=2, last_d_model=416, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=128, pos_d_model=144, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.2, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=8, fc_layer_type='mha', conv_out_dim=288), exp_settings='seq_len-42-lr-0.0005-d-768-hid_d-624-last_d-416-time_d-128-e_layers-8-tok_conv_k-11-dropout-0.2-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0004688694332590358, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 17:01:19,560] Trial 36 finished with value: 0.6531546046957374 and parameters: {'hidden_d_model': 624, 'token_conv_kernel': 11, 'last_d_model': 416, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 128, 'pos_d_model': 144, 'd_model': 768, 'conv_out_dim': 288, 'e_layers': 8, 'learning_rate': 0.0004688694332590358, 'dropout': 0.2, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 17:01:19,836] The parameter 'norm_type' in trial#37 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=11, input_dim=84, dec_in=84, output_dim=1, d_model=864, n_heads=4, e_layers=10, hidden_d_model=720, seq_layers=2, last_d_model=352, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=128, pos_d_model=176, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.22, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=8, fc_layer_type='mha', conv_out_dim=160), exp_settings='seq_len-42-lr-0.0004-d-864-hid_d-720-last_d-352-time_d-128-e_layers-10-tok_conv_k-11-dropout-0.22-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.00040157454249664627, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 17:11:35,326] Trial 37 finished with value: 0.6565201513469219 and parameters: {'hidden_d_model': 720, 'token_conv_kernel': 11, 'last_d_model': 352, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 128, 'pos_d_model': 176, 'd_model': 864, 'conv_out_dim': 160, 'e_layers': 10, 'learning_rate': 0.00040157454249664627, 'dropout': 0.22, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 17:11:35,611] The parameter 'norm_type' in trial#40 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 24.1 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
24.1 M    Trainable params
0         Non-trainable params
24.1 M    Total params
96.260    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.256
Metric Loss/val improved by 0.022 >= min_delta = 0.0. New best score: 1.234
Metric Loss/val improved by 0.167 >= min_delta = 0.0. New best score: 1.067
Metric Loss/val improved by 0.125 >= min_delta = 0.0. New best score: 0.942
Metric Loss/val improved by 0.015 >= min_delta = 0.0. New best score: 0.927
Metric Loss/val improved by 0.072 >= min_delta = 0.0. New best score: 0.855
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                            Best trial: 26. Best value: 0.544069:  20%|██        | 20/100 [2:49:32<12:42:40, 572.01s/it]Best trial: 26. Best value: 0.544069:  20%|██        | 20/100 [2:49:32<12:42:40, 572.01s/it]Best trial: 26. Best value: 0.544069:  21%|██        | 21/100 [2:49:32<11:16:09, 513.53s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                            Best trial: 26. Best value: 0.544069:  21%|██        | 21/100 [2:49:32<11:16:09, 513.53s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 55.7 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
55.7 M    Trainable params
0         Non-trainable params
55.7 M    Total params
222.849   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.227
Metric Loss/val improved by 0.139 >= min_delta = 0.0. New best score: 1.088
Metric Loss/val improved by 0.104 >= min_delta = 0.0. New best score: 0.984
Metric Loss/val improved by 0.206 >= min_delta = 0.0. New best score: 0.778
Metric Loss/val improved by 0.017 >= min_delta = 0.0. New best score: 0.761
Metric Loss/val improved by 0.003 >= min_delta = 0.0. New best score: 0.758
Metric Loss/val improved by 0.003 >= min_delta = 0.0. New best score: 0.755
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                            Best trial: 26. Best value: 0.544069:  21%|██        | 21/100 [2:59:34<11:16:09, 513.53s/it]Best trial: 26. Best value: 0.544069:  21%|██        | 21/100 [2:59:34<11:16:09, 513.53s/it]Best trial: 26. Best value: 0.544069:  22%|██▏       | 22/100 [2:59:34<11:42:05, 540.08s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                            Best trial: 26. Best value: 0.544069:  22%|██▏       | 22/100 [2:59:34<11:42:05, 540.08s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=640, n_heads=4, e_layers=6, hidden_d_model=608, seq_layers=2, last_d_model=416, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=64, pos_d_model=176, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.28, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=8, fc_layer_type='mha', conv_out_dim=224), exp_settings='seq_len-42-lr-0.0005-d-640-hid_d-608-last_d-416-time_d-64-e_layers-6-tok_conv_k-9-dropout-0.28-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.00047890819015437494, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 17:17:52,520] Trial 40 finished with value: 0.6529286529868841 and parameters: {'hidden_d_model': 608, 'token_conv_kernel': 9, 'last_d_model': 416, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 64, 'pos_d_model': 176, 'd_model': 640, 'conv_out_dim': 224, 'e_layers': 6, 'learning_rate': 0.00047890819015437494, 'dropout': 0.28, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 17:17:52,781] The parameter 'norm_type' in trial#42 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=768, n_heads=4, e_layers=10, hidden_d_model=752, seq_layers=2, last_d_model=576, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=128, pos_d_model=32, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.18, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=8, fc_layer_type='mha', conv_out_dim=384), exp_settings='seq_len-42-lr-0.0007-d-768-hid_d-752-last_d-576-time_d-128-e_layers-10-tok_conv_k-10-dropout-0.18-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0006861163140422066, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 17:27:54,492] Trial 42 finished with value: 0.6176034659147263 and parameters: {'hidden_d_model': 752, 'token_conv_kernel': 10, 'last_d_model': 576, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 128, 'pos_d_model': 32, 'd_model': 768, 'conv_out_dim': 384, 'e_layers': 10, 'learning_rate': 0.0006861163140422066, 'dropout': 0.18, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 17:27:54,752] The parameter 'norm_type' in trial#44 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 45.1 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
45.1 M    Trainable params
0         Non-trainable params
45.1 M    Total params
180.248   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.186
Metric Loss/val improved by 0.124 >= min_delta = 0.0. New best score: 1.063
Metric Loss/val improved by 0.067 >= min_delta = 0.0. New best score: 0.995
Metric Loss/val improved by 0.029 >= min_delta = 0.0. New best score: 0.966
Monitored metric Loss/val did not improve in the last 15 records. Best score: 0.966. Signaling Trainer to stop.
                                                                                            Best trial: 26. Best value: 0.544069:  22%|██▏       | 22/100 [3:07:16<11:42:05, 540.08s/it]Best trial: 26. Best value: 0.544069:  22%|██▏       | 22/100 [3:07:16<11:42:05, 540.08s/it]Best trial: 26. Best value: 0.544069:  23%|██▎       | 23/100 [3:07:16<11:03:07, 516.72s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                            Best trial: 26. Best value: 0.544069:  23%|██▎       | 23/100 [3:07:17<11:03:07, 516.72s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 52.0 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
52.0 M    Trainable params
0         Non-trainable params
52.0 M    Total params
208.001   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.216
Metric Loss/val improved by 0.025 >= min_delta = 0.0. New best score: 1.191
Metric Loss/val improved by 0.067 >= min_delta = 0.0. New best score: 1.124
Metric Loss/val improved by 0.108 >= min_delta = 0.0. New best score: 1.016
Metric Loss/val improved by 0.189 >= min_delta = 0.0. New best score: 0.827
Metric Loss/val improved by 0.012 >= min_delta = 0.0. New best score: 0.815
Metric Loss/val improved by 0.032 >= min_delta = 0.0. New best score: 0.784
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                            Best trial: 26. Best value: 0.544069:  23%|██▎       | 23/100 [3:17:21<11:03:07, 516.72s/it]Best trial: 26. Best value: 0.544069:  23%|██▎       | 23/100 [3:17:21<11:03:07, 516.72s/it]Best trial: 26. Best value: 0.544069:  24%|██▍       | 24/100 [3:17:21<11:27:50, 543.03s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                            Best trial: 26. Best value: 0.544069:  24%|██▍       | 24/100 [3:17:21<11:27:50, 543.03s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=800, n_heads=4, e_layers=8, hidden_d_model=880, seq_layers=2, last_d_model=576, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=192, pos_d_model=208, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.22, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=8, fc_layer_type='mha', conv_out_dim=192), exp_settings='seq_len-42-lr-0.0013-d-800-hid_d-880-last_d-576-time_d-192-e_layers-8-tok_conv_k-10-dropout-0.22-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0013243653899533243, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 17:35:36,746] Trial 44 finished with value: 0.7774407997727395 and parameters: {'hidden_d_model': 880, 'token_conv_kernel': 10, 'last_d_model': 576, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 192, 'pos_d_model': 208, 'd_model': 800, 'conv_out_dim': 192, 'e_layers': 8, 'learning_rate': 0.0013243653899533243, 'dropout': 0.22, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 17:35:37,006] The parameter 'norm_type' in trial#45 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=864, n_heads=4, e_layers=8, hidden_d_model=528, seq_layers=2, last_d_model=576, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=224, pos_d_model=176, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.2, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=8, fc_layer_type='mha', conv_out_dim=224), exp_settings='seq_len-42-lr-0.0006-d-864-hid_d-528-last_d-576-time_d-224-e_layers-8-tok_conv_k-9-dropout-0.2-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0005819625410114576, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 17:45:41,142] Trial 45 finished with value: 0.6023522030562163 and parameters: {'hidden_d_model': 528, 'token_conv_kernel': 9, 'last_d_model': 576, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 224, 'pos_d_model': 176, 'd_model': 864, 'conv_out_dim': 224, 'e_layers': 8, 'learning_rate': 0.0005819625410114576, 'dropout': 0.2, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 17:45:41,427] The parameter 'norm_type' in trial#48 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 21.6 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
21.6 M    Trainable params
0         Non-trainable params
21.6 M    Total params
86.515    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.197
Metric Loss/val improved by 0.079 >= min_delta = 0.0. New best score: 1.118
Metric Loss/val improved by 0.263 >= min_delta = 0.0. New best score: 0.855
Metric Loss/val improved by 0.021 >= min_delta = 0.0. New best score: 0.833
Monitored metric Loss/val did not improve in the last 15 records. Best score: 0.833. Signaling Trainer to stop.
                                                                                            Best trial: 26. Best value: 0.544069:  24%|██▍       | 24/100 [3:22:37<11:27:50, 543.03s/it]Best trial: 26. Best value: 0.544069:  24%|██▍       | 24/100 [3:22:37<11:27:50, 543.03s/it]Best trial: 26. Best value: 0.544069:  25%|██▌       | 25/100 [3:22:37<9:53:44, 475.00s/it] /home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                           Best trial: 26. Best value: 0.544069:  25%|██▌       | 25/100 [3:22:37<9:53:44, 475.00s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 47.7 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
47.7 M    Trainable params
0         Non-trainable params
47.7 M    Total params
190.776   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.211
Metric Loss/val improved by 0.003 >= min_delta = 0.0. New best score: 1.207
Metric Loss/val improved by 0.014 >= min_delta = 0.0. New best score: 1.193
Metric Loss/val improved by 0.019 >= min_delta = 0.0. New best score: 1.174
Metric Loss/val improved by 0.085 >= min_delta = 0.0. New best score: 1.089
Metric Loss/val improved by 0.074 >= min_delta = 0.0. New best score: 1.015
Metric Loss/val improved by 0.070 >= min_delta = 0.0. New best score: 0.945
Metric Loss/val improved by 0.040 >= min_delta = 0.0. New best score: 0.905
Metric Loss/val improved by 0.033 >= min_delta = 0.0. New best score: 0.872
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                           Best trial: 26. Best value: 0.544069:  25%|██▌       | 25/100 [3:32:13<9:53:44, 475.00s/it]Best trial: 26. Best value: 0.544069:  25%|██▌       | 25/100 [3:32:13<9:53:44, 475.00s/it]Best trial: 26. Best value: 0.544069:  26%|██▌       | 26/100 [3:32:13<10:23:00, 505.14s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                            Best trial: 26. Best value: 0.544069:  26%|██▌       | 26/100 [3:32:13<10:23:00, 505.14s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=736, n_heads=4, e_layers=4, hidden_d_model=704, seq_layers=2, last_d_model=544, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=160, pos_d_model=112, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.24000000000000002, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=8, fc_layer_type='mha', conv_out_dim=224), exp_settings='seq_len-42-lr-0.0014-d-736-hid_d-704-last_d-544-time_d-160-e_layers-4-tok_conv_k-9-dropout-0.24000000000000002-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0013654360268547168, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 17:50:57,433] Trial 48 finished with value: 0.6391129188239575 and parameters: {'hidden_d_model': 704, 'token_conv_kernel': 9, 'last_d_model': 544, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 160, 'pos_d_model': 112, 'd_model': 736, 'conv_out_dim': 224, 'e_layers': 4, 'learning_rate': 0.0013654360268547168, 'dropout': 0.24000000000000002, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 17:50:57,674] The parameter 'norm_type' in trial#49 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=832, n_heads=4, e_layers=8, hidden_d_model=640, seq_layers=2, last_d_model=160, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=144, pos_d_model=128, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.28, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=8, fc_layer_type='mha', conv_out_dim=192), exp_settings='seq_len-42-lr-0.0002-d-832-hid_d-640-last_d-160-time_d-144-e_layers-8-tok_conv_k-10-dropout-0.28-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.00015507262684365852, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 18:00:32,901] Trial 49 finished with value: 0.658009915612638 and parameters: {'hidden_d_model': 640, 'token_conv_kernel': 10, 'last_d_model': 160, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 144, 'pos_d_model': 128, 'd_model': 832, 'conv_out_dim': 192, 'e_layers': 8, 'learning_rate': 0.00015507262684365852, 'dropout': 0.28, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 18:00:33,548] The parameter 'norm_type' in trial#51 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 23.9 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
23.9 M    Trainable params
0         Non-trainable params
23.9 M    Total params
95.430    Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.212
Metric Loss/val improved by 0.026 >= min_delta = 0.0. New best score: 1.185
Metric Loss/val improved by 0.101 >= min_delta = 0.0. New best score: 1.084
Metric Loss/val improved by 0.113 >= min_delta = 0.0. New best score: 0.971
Metric Loss/val improved by 0.105 >= min_delta = 0.0. New best score: 0.866
Monitored metric Loss/val did not improve in the last 15 records. Best score: 0.866. Signaling Trainer to stop.
                                                                                            Best trial: 26. Best value: 0.544069:  26%|██▌       | 26/100 [3:36:07<10:23:00, 505.14s/it]Best trial: 26. Best value: 0.544069:  26%|██▌       | 26/100 [3:36:07<10:23:00, 505.14s/it]Best trial: 26. Best value: 0.544069:  27%|██▋       | 27/100 [3:36:07<8:35:35, 423.77s/it] /home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                           Best trial: 26. Best value: 0.544069:  27%|██▋       | 27/100 [3:36:07<8:35:35, 423.77s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 49.6 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
49.6 M    Trainable params
0         Non-trainable params
49.6 M    Total params
198.251   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.257
Metric Loss/val improved by 0.003 >= min_delta = 0.0. New best score: 1.254
Metric Loss/val improved by 0.182 >= min_delta = 0.0. New best score: 1.072
Metric Loss/val improved by 0.128 >= min_delta = 0.0. New best score: 0.945
Metric Loss/val improved by 0.041 >= min_delta = 0.0. New best score: 0.904
Metric Loss/val improved by 0.149 >= min_delta = 0.0. New best score: 0.755
Monitored metric Loss/val did not improve in the last 15 records. Best score: 0.755. Signaling Trainer to stop.
                                                                                           Best trial: 26. Best value: 0.544069:  27%|██▋       | 27/100 [3:44:24<8:35:35, 423.77s/it]Best trial: 26. Best value: 0.544069:  27%|██▋       | 27/100 [3:44:24<8:35:35, 423.77s/it]Best trial: 26. Best value: 0.544069:  28%|██▊       | 28/100 [3:44:24<8:55:01, 445.85s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                           Best trial: 26. Best value: 0.544069:  28%|██▊       | 28/100 [3:44:24<8:55:01, 445.85s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=736, n_heads=4, e_layers=4, hidden_d_model=624, seq_layers=2, last_d_model=256, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=144, pos_d_model=192, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.2, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=8, fc_layer_type='mha', conv_out_dim=320), exp_settings='seq_len-42-lr-0.0005-d-736-hid_d-624-last_d-256-time_d-144-e_layers-4-tok_conv_k-9-dropout-0.2-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0005052300456261845, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 18:04:26,812] Trial 51 finished with value: 0.7157439459115267 and parameters: {'hidden_d_model': 624, 'token_conv_kernel': 9, 'last_d_model': 256, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 144, 'pos_d_model': 192, 'd_model': 736, 'conv_out_dim': 320, 'e_layers': 4, 'learning_rate': 0.0005052300456261845, 'dropout': 0.2, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 18:04:27,092] The parameter 'norm_type' in trial#52 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=7, input_dim=84, dec_in=84, output_dim=1, d_model=832, n_heads=4, e_layers=8, hidden_d_model=784, seq_layers=2, last_d_model=128, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=144, pos_d_model=176, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.28, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=8, fc_layer_type='mha', conv_out_dim=352), exp_settings='seq_len-42-lr-0.0005-d-832-hid_d-784-last_d-128-time_d-144-e_layers-8-tok_conv_k-7-dropout-0.28-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0004655471396811688, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 18:12:44,193] Trial 52 finished with value: 0.6316178752109409 and parameters: {'hidden_d_model': 784, 'token_conv_kernel': 7, 'last_d_model': 128, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 144, 'pos_d_model': 176, 'd_model': 832, 'conv_out_dim': 352, 'e_layers': 8, 'learning_rate': 0.0004655471396811688, 'dropout': 0.28, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 18:12:44,435] The parameter 'norm_type' in trial#54 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 51.4 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
51.4 M    Trainable params
0         Non-trainable params
51.4 M    Total params
205.476   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.197
Metric Loss/val improved by 0.035 >= min_delta = 0.0. New best score: 1.162
Metric Loss/val improved by 0.105 >= min_delta = 0.0. New best score: 1.057
Metric Loss/val improved by 0.016 >= min_delta = 0.0. New best score: 1.041
Metric Loss/val improved by 0.029 >= min_delta = 0.0. New best score: 1.012
Metric Loss/val improved by 0.159 >= min_delta = 0.0. New best score: 0.854
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 0.854
Metric Loss/val improved by 0.010 >= min_delta = 0.0. New best score: 0.844
Metric Loss/val improved by 0.068 >= min_delta = 0.0. New best score: 0.775
Metric Loss/val improved by 0.016 >= min_delta = 0.0. New best score: 0.760
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                           Best trial: 26. Best value: 0.544069:  28%|██▊       | 28/100 [3:53:56<8:55:01, 445.85s/it]Best trial: 26. Best value: 0.544069:  28%|██▊       | 28/100 [3:53:56<8:55:01, 445.85s/it]Best trial: 26. Best value: 0.544069:  29%|██▉       | 29/100 [3:53:56<9:32:34, 483.87s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                           Best trial: 26. Best value: 0.544069:  29%|██▉       | 29/100 [3:53:57<9:32:34, 483.87s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 40.0 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
40.0 M    Trainable params
0         Non-trainable params
40.0 M    Total params
159.801   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.255
Metric Loss/val improved by 0.296 >= min_delta = 0.0. New best score: 0.958
Metric Loss/val improved by 0.058 >= min_delta = 0.0. New best score: 0.900
Metric Loss/val improved by 0.042 >= min_delta = 0.0. New best score: 0.858
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                           Best trial: 26. Best value: 0.544069:  29%|██▉       | 29/100 [4:02:04<9:32:34, 483.87s/it]Best trial: 26. Best value: 0.544069:  29%|██▉       | 29/100 [4:02:04<9:32:34, 483.87s/it]Best trial: 26. Best value: 0.544069:  30%|███       | 30/100 [4:02:04<9:25:46, 484.95s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                           Best trial: 26. Best value: 0.544069:  30%|███       | 30/100 [4:02:04<9:25:46, 484.95s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=832, n_heads=4, e_layers=8, hidden_d_model=672, seq_layers=2, last_d_model=544, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=144, pos_d_model=128, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.24000000000000002, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=8, fc_layer_type='mha', conv_out_dim=320), exp_settings='seq_len-42-lr-0.0009-d-832-hid_d-672-last_d-544-time_d-144-e_layers-8-tok_conv_k-10-dropout-0.24000000000000002-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.000918133583008643, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 18:22:16,763] Trial 54 finished with value: 0.6184450522065164 and parameters: {'hidden_d_model': 672, 'token_conv_kernel': 10, 'last_d_model': 544, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 144, 'pos_d_model': 128, 'd_model': 832, 'conv_out_dim': 320, 'e_layers': 8, 'learning_rate': 0.000918133583008643, 'dropout': 0.24000000000000002, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 18:22:17,073] The parameter 'norm_type' in trial#56 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=8, input_dim=84, dec_in=84, output_dim=1, d_model=736, n_heads=4, e_layers=8, hidden_d_model=656, seq_layers=2, last_d_model=416, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=176, pos_d_model=176, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.22, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=8, fc_layer_type='mha', conv_out_dim=288), exp_settings='seq_len-42-lr-0.0002-d-736-hid_d-656-last_d-416-time_d-176-e_layers-8-tok_conv_k-8-dropout-0.22-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.00018794769180371416, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 18:30:24,242] Trial 56 finished with value: 0.7127888297662139 and parameters: {'hidden_d_model': 656, 'token_conv_kernel': 8, 'last_d_model': 416, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 176, 'pos_d_model': 176, 'd_model': 736, 'conv_out_dim': 288, 'e_layers': 8, 'learning_rate': 0.00018794769180371416, 'dropout': 0.22, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 18:30:24,520] The parameter 'norm_type' in trial#58 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 28.0 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
28.0 M    Trainable params
0         Non-trainable params
28.0 M    Total params
112.101   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.243
Metric Loss/val improved by 0.003 >= min_delta = 0.0. New best score: 1.240
Metric Loss/val improved by 0.005 >= min_delta = 0.0. New best score: 1.235
Metric Loss/val improved by 0.259 >= min_delta = 0.0. New best score: 0.976
Metric Loss/val improved by 0.068 >= min_delta = 0.0. New best score: 0.908
Metric Loss/val improved by 0.112 >= min_delta = 0.0. New best score: 0.796
Monitored metric Loss/val did not improve in the last 15 records. Best score: 0.796. Signaling Trainer to stop.
                                                                                           Best trial: 26. Best value: 0.544069:  30%|███       | 30/100 [4:07:47<9:25:46, 484.95s/it]Best trial: 26. Best value: 0.544069:  30%|███       | 30/100 [4:07:47<9:25:46, 484.95s/it]Best trial: 26. Best value: 0.544069:  31%|███       | 31/100 [4:07:47<8:28:39, 442.31s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                           Best trial: 26. Best value: 0.544069:  31%|███       | 31/100 [4:07:47<8:28:39, 442.31s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 26.0 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
26.0 M    Trainable params
0         Non-trainable params
26.0 M    Total params
104.105   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.247
Metric Loss/val improved by 0.024 >= min_delta = 0.0. New best score: 1.223
Metric Loss/val improved by 0.105 >= min_delta = 0.0. New best score: 1.118
Metric Loss/val improved by 0.132 >= min_delta = 0.0. New best score: 0.986
Metric Loss/val improved by 0.136 >= min_delta = 0.0. New best score: 0.850
Metric Loss/val improved by 0.049 >= min_delta = 0.0. New best score: 0.801
Metric Loss/val improved by 0.016 >= min_delta = 0.0. New best score: 0.785
Metric Loss/val improved by 0.030 >= min_delta = 0.0. New best score: 0.755
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                           Best trial: 26. Best value: 0.544069:  31%|███       | 31/100 [4:14:13<8:28:39, 442.31s/it]Best trial: 26. Best value: 0.544069:  31%|███       | 31/100 [4:14:13<8:28:39, 442.31s/it]Best trial: 26. Best value: 0.544069:  32%|███▏      | 32/100 [4:14:13<8:02:03, 425.35s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                           Best trial: 26. Best value: 0.544069:  32%|███▏      | 32/100 [4:14:13<8:02:03, 425.35s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=704, n_heads=4, e_layers=6, hidden_d_model=480, seq_layers=2, last_d_model=704, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=128, pos_d_model=160, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.24000000000000002, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=8, fc_layer_type='mha', conv_out_dim=192), exp_settings='seq_len-42-lr-0.0002-d-704-hid_d-480-last_d-704-time_d-128-e_layers-6-tok_conv_k-10-dropout-0.24000000000000002-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.00023318462525200552, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 18:36:07,072] Trial 58 finished with value: 0.7174004152417184 and parameters: {'hidden_d_model': 480, 'token_conv_kernel': 10, 'last_d_model': 704, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 128, 'pos_d_model': 160, 'd_model': 704, 'conv_out_dim': 192, 'e_layers': 6, 'learning_rate': 0.00023318462525200552, 'dropout': 0.24000000000000002, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 18:36:07,333] The parameter 'norm_type' in trial#60 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=672, n_heads=4, e_layers=6, hidden_d_model=624, seq_layers=2, last_d_model=448, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=96, pos_d_model=144, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.24000000000000002, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=8, fc_layer_type='mha', conv_out_dim=224), exp_settings='seq_len-42-lr-0.0007-d-672-hid_d-624-last_d-448-time_d-96-e_layers-6-tok_conv_k-9-dropout-0.24000000000000002-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0006538531791679735, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 18:42:32,848] Trial 60 finished with value: 0.630182613991201 and parameters: {'hidden_d_model': 624, 'token_conv_kernel': 9, 'last_d_model': 448, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 96, 'pos_d_model': 144, 'd_model': 672, 'conv_out_dim': 224, 'e_layers': 6, 'learning_rate': 0.0006538531791679735, 'dropout': 0.24000000000000002, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 18:42:33,107] The parameter 'norm_type' in trial#61 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 57.4 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
57.4 M    Trainable params
0         Non-trainable params
57.4 M    Total params
229.745   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.214
Metric Loss/val improved by 0.064 >= min_delta = 0.0. New best score: 1.150
Metric Loss/val improved by 0.154 >= min_delta = 0.0. New best score: 0.995
Metric Loss/val improved by 0.064 >= min_delta = 0.0. New best score: 0.931
Metric Loss/val improved by 0.107 >= min_delta = 0.0. New best score: 0.824
Metric Loss/val improved by 0.028 >= min_delta = 0.0. New best score: 0.797
Metric Loss/val improved by 0.020 >= min_delta = 0.0. New best score: 0.777
Metric Loss/val improved by 0.037 >= min_delta = 0.0. New best score: 0.741
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                           Best trial: 26. Best value: 0.544069:  32%|███▏      | 32/100 [4:24:57<8:02:03, 425.35s/it]Best trial: 26. Best value: 0.544069:  32%|███▏      | 32/100 [4:24:58<8:02:03, 425.35s/it]Best trial: 26. Best value: 0.544069:  33%|███▎      | 33/100 [4:24:58<9:08:32, 491.23s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                           Best trial: 26. Best value: 0.544069:  33%|███▎      | 33/100 [4:24:58<9:08:32, 491.23s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 51.3 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
51.3 M    Trainable params
0         Non-trainable params
51.3 M    Total params
205.033   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.245
Metric Loss/val improved by 0.008 >= min_delta = 0.0. New best score: 1.237
Metric Loss/val improved by 0.056 >= min_delta = 0.0. New best score: 1.181
Metric Loss/val improved by 0.212 >= min_delta = 0.0. New best score: 0.969
Metric Loss/val improved by 0.000 >= min_delta = 0.0. New best score: 0.969
Metric Loss/val improved by 0.086 >= min_delta = 0.0. New best score: 0.882
Metric Loss/val improved by 0.008 >= min_delta = 0.0. New best score: 0.875
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                           Best trial: 26. Best value: 0.544069:  33%|███▎      | 33/100 [4:34:39<9:08:32, 491.23s/it]Best trial: 26. Best value: 0.544069:  33%|███▎      | 33/100 [4:34:39<9:08:32, 491.23s/it]Best trial: 26. Best value: 0.544069:  34%|███▍      | 34/100 [4:34:39<9:30:07, 518.29s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                           Best trial: 26. Best value: 0.544069:  34%|███▍      | 34/100 [4:34:39<9:30:07, 518.29s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=800, n_heads=4, e_layers=10, hidden_d_model=800, seq_layers=2, last_d_model=448, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=128, pos_d_model=96, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.24000000000000002, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=8, fc_layer_type='mha', conv_out_dim=320), exp_settings='seq_len-42-lr-0.0003-d-800-hid_d-800-last_d-448-time_d-128-e_layers-10-tok_conv_k-9-dropout-0.24000000000000002-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0002842467973357371, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 18:53:17,787] Trial 61 finished with value: 0.6414523400366308 and parameters: {'hidden_d_model': 800, 'token_conv_kernel': 9, 'last_d_model': 448, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 128, 'pos_d_model': 96, 'd_model': 800, 'conv_out_dim': 320, 'e_layers': 10, 'learning_rate': 0.0002842467973357371, 'dropout': 0.24000000000000002, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 18:53:18,104] The parameter 'norm_type' in trial#63 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=7, input_dim=84, dec_in=84, output_dim=1, d_model=992, n_heads=4, e_layers=6, hidden_d_model=896, seq_layers=2, last_d_model=512, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=112, pos_d_model=64, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.24000000000000002, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=8, fc_layer_type='mha', conv_out_dim=320), exp_settings='seq_len-42-lr-0.0003-d-992-hid_d-896-last_d-512-time_d-112-e_layers-6-tok_conv_k-7-dropout-0.24000000000000002-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0003223903008038937, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 19:02:59,219] Trial 63 finished with value: 0.6953658405691385 and parameters: {'hidden_d_model': 896, 'token_conv_kernel': 7, 'last_d_model': 512, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 112, 'pos_d_model': 64, 'd_model': 992, 'conv_out_dim': 320, 'e_layers': 6, 'learning_rate': 0.0003223903008038937, 'dropout': 0.24000000000000002, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 19:02:59,475] The parameter 'norm_type' in trial#65 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 67.0 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
67.0 M    Trainable params
0         Non-trainable params
67.0 M    Total params
268.148   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.214
Metric Loss/val improved by 0.005 >= min_delta = 0.0. New best score: 1.209
Metric Loss/val improved by 0.018 >= min_delta = 0.0. New best score: 1.191
Metric Loss/val improved by 0.123 >= min_delta = 0.0. New best score: 1.069
Metric Loss/val improved by 0.122 >= min_delta = 0.0. New best score: 0.947
Metric Loss/val improved by 0.011 >= min_delta = 0.0. New best score: 0.936
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 0.934
Metric Loss/val improved by 0.007 >= min_delta = 0.0. New best score: 0.927
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                           Best trial: 26. Best value: 0.544069:  34%|███▍      | 34/100 [4:46:27<9:30:07, 518.29s/it]Best trial: 26. Best value: 0.544069:  34%|███▍      | 34/100 [4:46:27<9:30:07, 518.29s/it]Best trial: 26. Best value: 0.544069:  35%|███▌      | 35/100 [4:46:27<10:23:11, 575.26s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                            Best trial: 26. Best value: 0.544069:  35%|███▌      | 35/100 [4:46:27<10:23:11, 575.26s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 46.5 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
46.5 M    Trainable params
0         Non-trainable params
46.5 M    Total params
186.150   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.223
Metric Loss/val improved by 0.015 >= min_delta = 0.0. New best score: 1.208
Metric Loss/val improved by 0.084 >= min_delta = 0.0. New best score: 1.124
Metric Loss/val improved by 0.036 >= min_delta = 0.0. New best score: 1.088
Metric Loss/val improved by 0.284 >= min_delta = 0.0. New best score: 0.804
Metric Loss/val improved by 0.083 >= min_delta = 0.0. New best score: 0.720
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                            Best trial: 26. Best value: 0.544069:  35%|███▌      | 35/100 [4:55:14<10:23:11, 575.26s/it]Best trial: 26. Best value: 0.544069:  35%|███▌      | 35/100 [4:55:14<10:23:11, 575.26s/it]Best trial: 26. Best value: 0.544069:  36%|███▌      | 36/100 [4:55:14<9:58:09, 560.77s/it] /home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                           Best trial: 26. Best value: 0.544069:  36%|███▌      | 36/100 [4:55:14<9:58:09, 560.77s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=8, input_dim=84, dec_in=84, output_dim=1, d_model=992, n_heads=4, e_layers=8, hidden_d_model=592, seq_layers=2, last_d_model=384, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=144, pos_d_model=80, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.28, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=8, fc_layer_type='mha', conv_out_dim=288), exp_settings='seq_len-42-lr-0.0002-d-992-hid_d-592-last_d-384-time_d-144-e_layers-8-tok_conv_k-8-dropout-0.28-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.00015722298271604368, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 19:14:47,401] Trial 65 finished with value: 0.6285589203238487 and parameters: {'hidden_d_model': 592, 'token_conv_kernel': 8, 'last_d_model': 384, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 144, 'pos_d_model': 80, 'd_model': 992, 'conv_out_dim': 288, 'e_layers': 8, 'learning_rate': 0.00015722298271604368, 'dropout': 0.28, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 19:14:47,711] The parameter 'norm_type' in trial#68 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=11, input_dim=84, dec_in=84, output_dim=1, d_model=768, n_heads=4, e_layers=8, hidden_d_model=704, seq_layers=2, last_d_model=416, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=160, pos_d_model=112, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.26, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=8, fc_layer_type='mha', conv_out_dim=352), exp_settings='seq_len-42-lr-0.0003-d-768-hid_d-704-last_d-416-time_d-160-e_layers-8-tok_conv_k-11-dropout-0.26-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0002926365648788036, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 19:23:34,366] Trial 68 finished with value: 0.6066091772168876 and parameters: {'hidden_d_model': 704, 'token_conv_kernel': 11, 'last_d_model': 416, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 160, 'pos_d_model': 112, 'd_model': 768, 'conv_out_dim': 352, 'e_layers': 8, 'learning_rate': 0.0002926365648788036, 'dropout': 0.26, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 19:23:34,627] The parameter 'norm_type' in trial#70 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 45.4 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
45.4 M    Trainable params
0         Non-trainable params
45.4 M    Total params
181.544   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.239
Metric Loss/val improved by 0.048 >= min_delta = 0.0. New best score: 1.192
Metric Loss/val improved by 0.097 >= min_delta = 0.0. New best score: 1.095
Metric Loss/val improved by 0.246 >= min_delta = 0.0. New best score: 0.850
Metric Loss/val improved by 0.043 >= min_delta = 0.0. New best score: 0.806
Metric Loss/val improved by 0.191 >= min_delta = 0.0. New best score: 0.615
Monitored metric Loss/val did not improve in the last 15 records. Best score: 0.615. Signaling Trainer to stop.
                                                                                           Best trial: 26. Best value: 0.544069:  36%|███▌      | 36/100 [5:04:18<9:58:09, 560.77s/it]Best trial: 26. Best value: 0.544069:  36%|███▌      | 36/100 [5:04:18<9:58:09, 560.77s/it]Best trial: 26. Best value: 0.544069:  37%|███▋      | 37/100 [5:04:18<9:43:28, 555.69s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                           Best trial: 26. Best value: 0.544069:  37%|███▋      | 37/100 [5:04:18<9:43:28, 555.69s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 48.2 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
48.2 M    Trainable params
0         Non-trainable params
48.2 M    Total params
192.658   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.228
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 1.226
Metric Loss/val improved by 0.197 >= min_delta = 0.0. New best score: 1.029
Metric Loss/val improved by 0.227 >= min_delta = 0.0. New best score: 0.802
Metric Loss/val improved by 0.001 >= min_delta = 0.0. New best score: 0.801
Metric Loss/val improved by 0.019 >= min_delta = 0.0. New best score: 0.782
Metric Loss/val improved by 0.023 >= min_delta = 0.0. New best score: 0.759
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                           Best trial: 26. Best value: 0.544069:  37%|███▋      | 37/100 [5:14:13<9:43:28, 555.69s/it]Best trial: 26. Best value: 0.544069:  37%|███▋      | 37/100 [5:14:13<9:43:28, 555.69s/it]Best trial: 26. Best value: 0.544069:  38%|███▊      | 38/100 [5:14:13<9:46:23, 567.48s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                           Best trial: 26. Best value: 0.544069:  38%|███▊      | 38/100 [5:14:13<9:46:23, 567.48s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=800, n_heads=4, e_layers=8, hidden_d_model=768, seq_layers=2, last_d_model=256, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=144, pos_d_model=128, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.22, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=8, fc_layer_type='mha', conv_out_dim=224), exp_settings='seq_len-42-lr-0.0007-d-800-hid_d-768-last_d-256-time_d-144-e_layers-8-tok_conv_k-10-dropout-0.22-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0007286401458658142, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 19:32:38,195] Trial 70 finished with value: 0.5990686446428299 and parameters: {'hidden_d_model': 768, 'token_conv_kernel': 10, 'last_d_model': 256, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 144, 'pos_d_model': 128, 'd_model': 800, 'conv_out_dim': 224, 'e_layers': 8, 'learning_rate': 0.0007286401458658142, 'dropout': 0.22, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 19:32:38,468] The parameter 'norm_type' in trial#72 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=9, input_dim=84, dec_in=84, output_dim=1, d_model=832, n_heads=4, e_layers=8, hidden_d_model=688, seq_layers=2, last_d_model=288, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=176, pos_d_model=112, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.26, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=8, fc_layer_type='mha', conv_out_dim=224), exp_settings='seq_len-42-lr-0.0003-d-832-hid_d-688-last_d-288-time_d-176-e_layers-8-tok_conv_k-9-dropout-0.26-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0002808542989434991, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 19:42:33,182] Trial 72 finished with value: 0.5921673502773047 and parameters: {'hidden_d_model': 688, 'token_conv_kernel': 9, 'last_d_model': 288, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 176, 'pos_d_model': 112, 'd_model': 832, 'conv_out_dim': 224, 'e_layers': 8, 'learning_rate': 0.0002808542989434991, 'dropout': 0.26, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 19:42:33,485] The parameter 'norm_type' in trial#74 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 40.5 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
40.5 M    Trainable params
0         Non-trainable params
40.5 M    Total params
162.103   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.223
Metric Loss/val improved by 0.013 >= min_delta = 0.0. New best score: 1.210
Metric Loss/val improved by 0.126 >= min_delta = 0.0. New best score: 1.084
Metric Loss/val improved by 0.130 >= min_delta = 0.0. New best score: 0.954
Metric Loss/val improved by 0.157 >= min_delta = 0.0. New best score: 0.797
Metric Loss/val improved by 0.017 >= min_delta = 0.0. New best score: 0.780
Metric Loss/val improved by 0.047 >= min_delta = 0.0. New best score: 0.733
Metric Loss/val improved by 0.009 >= min_delta = 0.0. New best score: 0.725
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                           Best trial: 26. Best value: 0.544069:  38%|███▊      | 38/100 [5:22:37<9:46:23, 567.48s/it]Best trial: 26. Best value: 0.544069:  38%|███▊      | 38/100 [5:22:37<9:46:23, 567.48s/it]Best trial: 26. Best value: 0.544069:  39%|███▉      | 39/100 [5:22:37<9:17:27, 548.32s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                           Best trial: 26. Best value: 0.544069:  39%|███▉      | 39/100 [5:22:37<9:17:27, 548.32s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 35.3 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
35.3 M    Trainable params
0         Non-trainable params
35.3 M    Total params
141.161   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.205
Metric Loss/val improved by 0.073 >= min_delta = 0.0. New best score: 1.132
Metric Loss/val improved by 0.199 >= min_delta = 0.0. New best score: 0.933
Metric Loss/val improved by 0.136 >= min_delta = 0.0. New best score: 0.797
Metric Loss/val improved by 0.013 >= min_delta = 0.0. New best score: 0.784
Metric Loss/val improved by 0.039 >= min_delta = 0.0. New best score: 0.745
Monitored metric Loss/val did not improve in the last 15 records. Best score: 0.745. Signaling Trainer to stop.
                                                                                           Best trial: 26. Best value: 0.544069:  39%|███▉      | 39/100 [5:28:45<9:17:27, 548.32s/it]Best trial: 26. Best value: 0.544069:  39%|███▉      | 39/100 [5:28:45<9:17:27, 548.32s/it]Best trial: 26. Best value: 0.544069:  40%|████      | 40/100 [5:28:45<8:14:14, 494.25s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                           Best trial: 26. Best value: 0.544069:  40%|████      | 40/100 [5:28:45<8:14:14, 494.25s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=736, n_heads=4, e_layers=8, hidden_d_model=720, seq_layers=2, last_d_model=384, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=208, pos_d_model=176, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.24000000000000002, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=8, fc_layer_type='mha', conv_out_dim=256), exp_settings='seq_len-42-lr-0.0003-d-736-hid_d-720-last_d-384-time_d-208-e_layers-8-tok_conv_k-10-dropout-0.24000000000000002-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0002610867819161126, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 19:50:56,796] Trial 74 finished with value: 0.5920273706316947 and parameters: {'hidden_d_model': 720, 'token_conv_kernel': 10, 'last_d_model': 384, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 208, 'pos_d_model': 176, 'd_model': 736, 'conv_out_dim': 256, 'e_layers': 8, 'learning_rate': 0.0002610867819161126, 'dropout': 0.24000000000000002, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 19:50:57,066] The parameter 'norm_type' in trial#76 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=768, n_heads=4, e_layers=6, hidden_d_model=640, seq_layers=2, last_d_model=288, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=160, pos_d_model=96, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.24000000000000002, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=8, fc_layer_type='mha', conv_out_dim=320), exp_settings='seq_len-42-lr-0.0005-d-768-hid_d-640-last_d-288-time_d-160-e_layers-6-tok_conv_k-10-dropout-0.24000000000000002-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0005284332788416281, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 19:57:04,878] Trial 76 finished with value: 0.5858683411031962 and parameters: {'hidden_d_model': 640, 'token_conv_kernel': 10, 'last_d_model': 288, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 160, 'pos_d_model': 96, 'd_model': 768, 'conv_out_dim': 320, 'e_layers': 6, 'learning_rate': 0.0005284332788416281, 'dropout': 0.24000000000000002, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 19:57:05,162] The parameter 'norm_type' in trial#77 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 74.9 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
74.9 M    Trainable params
0         Non-trainable params
74.9 M    Total params
299.783   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.184
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 1.182
Metric Loss/val improved by 0.031 >= min_delta = 0.0. New best score: 1.151
Metric Loss/val improved by 0.252 >= min_delta = 0.0. New best score: 0.899
Metric Loss/val improved by 0.019 >= min_delta = 0.0. New best score: 0.880
Metric Loss/val improved by 0.021 >= min_delta = 0.0. New best score: 0.859
Metric Loss/val improved by 0.013 >= min_delta = 0.0. New best score: 0.846
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                           Best trial: 26. Best value: 0.544069:  40%|████      | 40/100 [5:42:25<8:14:14, 494.25s/it]Best trial: 26. Best value: 0.544069:  40%|████      | 40/100 [5:42:25<8:14:14, 494.25s/it]Best trial: 26. Best value: 0.544069:  41%|████      | 41/100 [5:42:25<9:42:13, 592.10s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                           Best trial: 26. Best value: 0.544069:  41%|████      | 41/100 [5:42:25<9:42:13, 592.10s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 41.9 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
41.9 M    Trainable params
0         Non-trainable params
41.9 M    Total params
167.525   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.213
Metric Loss/val improved by 0.054 >= min_delta = 0.0. New best score: 1.159
Metric Loss/val improved by 0.008 >= min_delta = 0.0. New best score: 1.151
Metric Loss/val improved by 0.218 >= min_delta = 0.0. New best score: 0.933
Metric Loss/val improved by 0.081 >= min_delta = 0.0. New best score: 0.851
Metric Loss/val improved by 0.033 >= min_delta = 0.0. New best score: 0.818
Metric Loss/val improved by 0.024 >= min_delta = 0.0. New best score: 0.795
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                           Best trial: 26. Best value: 0.544069:  41%|████      | 41/100 [5:50:39<9:42:13, 592.10s/it]Best trial: 26. Best value: 0.544069:  41%|████      | 41/100 [5:50:39<9:42:13, 592.10s/it]Best trial: 26. Best value: 0.544069:  42%|████▏     | 42/100 [5:50:39<9:04:02, 562.81s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                           Best trial: 26. Best value: 0.544069:  42%|████▏     | 42/100 [5:50:40<9:04:02, 562.81s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=7, input_dim=84, dec_in=84, output_dim=1, d_model=864, n_heads=4, e_layers=12, hidden_d_model=752, seq_layers=2, last_d_model=96, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=176, pos_d_model=144, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.28, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=8, fc_layer_type='mha', conv_out_dim=256), exp_settings='seq_len-42-lr-0.0003-d-864-hid_d-752-last_d-96-time_d-176-e_layers-12-tok_conv_k-7-dropout-0.28-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0002661585736080566, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 20:10:45,308] Trial 77 finished with value: 0.6362951874732972 and parameters: {'hidden_d_model': 752, 'token_conv_kernel': 7, 'last_d_model': 96, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 176, 'pos_d_model': 144, 'd_model': 864, 'conv_out_dim': 256, 'e_layers': 12, 'learning_rate': 0.0002661585736080566, 'dropout': 0.28, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 20:10:45,572] The parameter 'norm_type' in trial#80 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=736, n_heads=4, e_layers=8, hidden_d_model=736, seq_layers=2, last_d_model=384, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=144, pos_d_model=128, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.26, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=8, fc_layer_type='mha', conv_out_dim=320), exp_settings='seq_len-42-lr-0.0001-d-736-hid_d-736-last_d-384-time_d-144-e_layers-8-tok_conv_k-10-dropout-0.26-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.00014703561832454654, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 20:18:59,762] Trial 80 finished with value: 0.6494739277288317 and parameters: {'hidden_d_model': 736, 'token_conv_kernel': 10, 'last_d_model': 384, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 144, 'pos_d_model': 128, 'd_model': 736, 'conv_out_dim': 320, 'e_layers': 8, 'learning_rate': 0.00014703561832454654, 'dropout': 0.26, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 20:19:00,023] The parameter 'norm_type' in trial#81 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 41.1 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
41.1 M    Trainable params
0         Non-trainable params
41.1 M    Total params
164.249   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.213
Metric Loss/val improved by 0.084 >= min_delta = 0.0. New best score: 1.129
Metric Loss/val improved by 0.255 >= min_delta = 0.0. New best score: 0.874
Metric Loss/val improved by 0.099 >= min_delta = 0.0. New best score: 0.776
Monitored metric Loss/val did not improve in the last 15 records. Best score: 0.776. Signaling Trainer to stop.
                                                                                           Best trial: 26. Best value: 0.544069:  42%|████▏     | 42/100 [5:59:18<9:04:02, 562.81s/it]Best trial: 26. Best value: 0.544069:  42%|████▏     | 42/100 [5:59:18<9:04:02, 562.81s/it]Best trial: 26. Best value: 0.544069:  43%|████▎     | 43/100 [5:59:18<8:42:02, 549.52s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                           Best trial: 26. Best value: 0.544069:  43%|████▎     | 43/100 [5:59:18<8:42:02, 549.52s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 59.2 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
59.2 M    Trainable params
0         Non-trainable params
59.2 M    Total params
236.920   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.242
Metric Loss/val improved by 0.010 >= min_delta = 0.0. New best score: 1.232
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 1.230
Metric Loss/val improved by 0.074 >= min_delta = 0.0. New best score: 1.155
Metric Loss/val improved by 0.165 >= min_delta = 0.0. New best score: 0.990
Metric Loss/val improved by 0.163 >= min_delta = 0.0. New best score: 0.827
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                           Best trial: 26. Best value: 0.544069:  43%|████▎     | 43/100 [6:10:36<8:42:02, 549.52s/it]Best trial: 26. Best value: 0.544069:  43%|████▎     | 43/100 [6:10:36<8:42:02, 549.52s/it]Best trial: 26. Best value: 0.544069:  44%|████▍     | 44/100 [6:10:36<9:08:56, 588.16s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                           Best trial: 26. Best value: 0.544069:  44%|████▍     | 44/100 [6:10:37<9:08:56, 588.16s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=672, n_heads=4, e_layers=10, hidden_d_model=704, seq_layers=2, last_d_model=160, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=192, pos_d_model=144, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.3, use_pos_enc=True, bidirectional=False, norm_type='layer', num_heads=8, fc_layer_type='mha', conv_out_dim=224), exp_settings='seq_len-42-lr-0.0003-d-672-hid_d-704-last_d-160-time_d-192-e_layers-10-tok_conv_k-10-dropout-0.3-bs-1024-norm_type-layer-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.00031703855818223346, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 20:27:38,281] Trial 81 finished with value: 0.6445460598915815 and parameters: {'hidden_d_model': 704, 'token_conv_kernel': 10, 'last_d_model': 160, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 192, 'pos_d_model': 144, 'd_model': 672, 'conv_out_dim': 224, 'e_layers': 10, 'learning_rate': 0.00031703855818223346, 'dropout': 0.3, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'layer', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 20:27:38,559] The parameter 'norm_type' in trial#83 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=832, n_heads=4, e_layers=10, hidden_d_model=784, seq_layers=2, last_d_model=640, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=160, pos_d_model=128, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.24000000000000002, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=8, fc_layer_type='mha', conv_out_dim=192), exp_settings='seq_len-42-lr-0.0002-d-832-hid_d-784-last_d-640-time_d-160-e_layers-10-tok_conv_k-10-dropout-0.24000000000000002-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.00017182042418667184, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 20:38:56,594] Trial 83 finished with value: 0.7079893549904228 and parameters: {'hidden_d_model': 784, 'token_conv_kernel': 10, 'last_d_model': 640, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 160, 'pos_d_model': 128, 'd_model': 832, 'conv_out_dim': 192, 'e_layers': 10, 'learning_rate': 0.00017182042418667184, 'dropout': 0.24000000000000002, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 20:38:56,922] The parameter 'norm_type' in trial#86 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 52.0 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
52.0 M    Trainable params
0         Non-trainable params
52.0 M    Total params
207.904   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.222
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 1.221
Metric Loss/val improved by 0.021 >= min_delta = 0.0. New best score: 1.199
Metric Loss/val improved by 0.346 >= min_delta = 0.0. New best score: 0.853
Metric Loss/val improved by 0.045 >= min_delta = 0.0. New best score: 0.808
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                           Best trial: 26. Best value: 0.544069:  44%|████▍     | 44/100 [6:20:20<9:08:56, 588.16s/it]Best trial: 26. Best value: 0.544069:  44%|████▍     | 44/100 [6:20:20<9:08:56, 588.16s/it]Best trial: 26. Best value: 0.544069:  45%|████▌     | 45/100 [6:20:20<8:57:54, 586.80s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                           Best trial: 26. Best value: 0.544069:  45%|████▌     | 45/100 [6:20:20<8:57:54, 586.80s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 35.1 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
35.1 M    Trainable params
0         Non-trainable params
35.1 M    Total params
140.202   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.207
Metric Loss/val improved by 0.017 >= min_delta = 0.0. New best score: 1.190
Metric Loss/val improved by 0.083 >= min_delta = 0.0. New best score: 1.107
Metric Loss/val improved by 0.159 >= min_delta = 0.0. New best score: 0.948
Metric Loss/val improved by 0.061 >= min_delta = 0.0. New best score: 0.887
Metric Loss/val improved by 0.038 >= min_delta = 0.0. New best score: 0.849
Metric Loss/val improved by 0.064 >= min_delta = 0.0. New best score: 0.785
Metric Loss/val improved by 0.023 >= min_delta = 0.0. New best score: 0.762
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                           Best trial: 26. Best value: 0.544069:  45%|████▌     | 45/100 [6:28:04<8:57:54, 586.80s/it]Best trial: 26. Best value: 0.544069:  45%|████▌     | 45/100 [6:28:04<8:57:54, 586.80s/it]Best trial: 26. Best value: 0.544069:  46%|████▌     | 46/100 [6:28:04<8:15:01, 550.03s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                           Best trial: 26. Best value: 0.544069:  46%|████▌     | 46/100 [6:28:04<8:15:01, 550.03s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=11, input_dim=84, dec_in=84, output_dim=1, d_model=832, n_heads=4, e_layers=8, hidden_d_model=848, seq_layers=2, last_d_model=320, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=176, pos_d_model=112, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.22, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=8, fc_layer_type='mha', conv_out_dim=320), exp_settings='seq_len-42-lr-0.0003-d-832-hid_d-848-last_d-320-time_d-176-e_layers-8-tok_conv_k-11-dropout-0.22-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.00031001180692356633, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 20:48:40,233] Trial 86 finished with value: 0.6018124096095563 and parameters: {'hidden_d_model': 848, 'token_conv_kernel': 11, 'last_d_model': 320, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 176, 'pos_d_model': 112, 'd_model': 832, 'conv_out_dim': 320, 'e_layers': 8, 'learning_rate': 0.00031001180692356633, 'dropout': 0.22, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 20:48:40,466] The parameter 'norm_type' in trial#88 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=10, input_dim=84, dec_in=84, output_dim=1, d_model=832, n_heads=4, e_layers=6, hidden_d_model=688, seq_layers=2, last_d_model=320, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=160, pos_d_model=112, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.26, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=8, fc_layer_type='mha', conv_out_dim=128), exp_settings='seq_len-42-lr-0.0004-d-832-hid_d-688-last_d-320-time_d-160-e_layers-6-tok_conv_k-10-dropout-0.26-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.00042281901569874775, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 20:56:24,441] Trial 88 finished with value: 0.604638189636171 and parameters: {'hidden_d_model': 688, 'token_conv_kernel': 10, 'last_d_model': 320, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 160, 'pos_d_model': 112, 'd_model': 832, 'conv_out_dim': 128, 'e_layers': 6, 'learning_rate': 0.00042281901569874775, 'dropout': 0.26, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 20:56:24,719] The parameter 'norm_type' in trial#89 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 26.9 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
26.9 M    Trainable params
0         Non-trainable params
26.9 M    Total params
107.676   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.254
Metric Loss/val improved by 0.005 >= min_delta = 0.0. New best score: 1.249
Metric Loss/val improved by 0.065 >= min_delta = 0.0. New best score: 1.184
Metric Loss/val improved by 0.285 >= min_delta = 0.0. New best score: 0.899
Metric Loss/val improved by 0.015 >= min_delta = 0.0. New best score: 0.884
Metric Loss/val improved by 0.019 >= min_delta = 0.0. New best score: 0.865
Metric Loss/val improved by 0.010 >= min_delta = 0.0. New best score: 0.855
Metric Loss/val improved by 0.002 >= min_delta = 0.0. New best score: 0.853
Metric Loss/val improved by 0.009 >= min_delta = 0.0. New best score: 0.844
Metric Loss/val improved by 0.010 >= min_delta = 0.0. New best score: 0.834
`Trainer.fit` stopped: `max_epochs=30` reached.
                                                                                           Best trial: 26. Best value: 0.544069:  46%|████▌     | 46/100 [6:34:43<8:15:01, 550.03s/it]Best trial: 26. Best value: 0.544069:  46%|████▌     | 46/100 [6:34:43<8:15:01, 550.03s/it]Best trial: 26. Best value: 0.544069:  47%|████▋     | 47/100 [6:34:43<7:25:53, 504.77s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                           Best trial: 26. Best value: 0.544069:  47%|████▋     | 47/100 [6:34:44<7:25:53, 504.77s/it]Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 45.7 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
45.7 M    Trainable params
0         Non-trainable params
45.7 M    Total params
182.684   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.218
Metric Loss/val improved by 0.018 >= min_delta = 0.0. New best score: 1.199
Metric Loss/val improved by 0.045 >= min_delta = 0.0. New best score: 1.155
Metric Loss/val improved by 0.049 >= min_delta = 0.0. New best score: 1.106
Metric Loss/val improved by 0.225 >= min_delta = 0.0. New best score: 0.881
Metric Loss/val improved by 0.027 >= min_delta = 0.0. New best score: 0.854
Monitored metric Loss/val did not improve in the last 15 records. Best score: 0.854. Signaling Trainer to stop.
                                                                                           Best trial: 26. Best value: 0.544069:  47%|████▋     | 47/100 [6:41:57<7:25:53, 504.77s/it]Best trial: 26. Best value: 0.544069:  47%|████▋     | 47/100 [6:41:57<7:25:53, 504.77s/it]Best trial: 26. Best value: 0.544069:  48%|████▊     | 48/100 [6:41:57<6:59:04, 483.55s/it]/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [8, 32] and step=16, but the range is not divisible by `step`. It will be replaced by [8, 24].
  warnings.warn(
                                                                                           Best trial: 26. Best value: 0.544069:  48%|████▊     | 48/100 [6:41:58<6:59:04, 483.55s/it]
Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=8, input_dim=84, dec_in=84, output_dim=1, d_model=672, n_heads=4, e_layers=6, hidden_d_model=576, seq_layers=2, last_d_model=256, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=192, pos_d_model=128, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.24000000000000002, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=8, fc_layer_type='mha', conv_out_dim=288), exp_settings='seq_len-42-lr-0.0003-d-672-hid_d-576-last_d-256-time_d-192-e_layers-6-tok_conv_k-8-dropout-0.24000000000000002-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.0003080389714810565, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 21:03:03,629] Trial 89 finished with value: 0.6157861828804017 and parameters: {'hidden_d_model': 576, 'token_conv_kernel': 8, 'last_d_model': 256, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 192, 'pos_d_model': 128, 'd_model': 672, 'conv_out_dim': 288, 'e_layers': 6, 'learning_rate': 0.0003080389714810565, 'dropout': 0.24000000000000002, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 21:03:03,939] The parameter 'norm_type' in trial#91 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])

Running experiment with config:
Namespace(general_settings=Namespace(seed=17, is_training=1, task_name='default', description='train', comment='optuna search'), output_paths=Namespace(train_log_dir='/data3/lsf/Pein/Power-Prediction/train_log/24-08-12', res_output_dir='/data3/lsf/Pein/Power-Prediction/res_output/24-08-12', final_best_metrics_log_path='/data3/lsf/Pein/Power-Prediction/final_best_metric/24-08-12.log', optuna_study_dir='/data3/lsf/Pein/Power-Prediction/optuna_study/24-08-12', wandb_output_dir='/data3/lsf/Pein/Power-Prediction/wandb_output/optuna/24-08-12'), data_paths=Namespace(data_root_dir='/data3/lsf/Pein/Power-Prediction/data', train_path='train_farm_99_withTime.csv', test_path='test_farm_99_withTime.csv'), data_settings=Namespace(train_val_split=0.2, target='power', inverse=True, num_workers=8, data='WindPower', scale_x_type='standard', scale_y_type='standard'), model_settings=Namespace(name='MLP_v3', token_conv_kernel=11, input_dim=84, dec_in=84, output_dim=1, d_model=768, n_heads=4, e_layers=8, hidden_d_model=704, seq_layers=2, last_d_model=384, top_k=5, num_kernels=6, activation_type='gelu', token_d_model=24, time_d_model=192, pos_d_model=96, combine_type='add', seq_len=42, pred_len=1, min_y_value=0.0, dropout=0.26, use_pos_enc=True, bidirectional=False, norm_type='batch', num_heads=8, fc_layer_type='mha', conv_out_dim=320), exp_settings='seq_len-42-lr-0.0003-d-768-hid_d-704-last_d-384-time_d-192-e_layers-8-tok_conv_k-11-dropout-0.26-bs-1024-norm_type-batch-use_pos_enc-True-num_heads-8-fc_layer_type-mha', training_settings=Namespace(gradient_clip_val=1, train_epochs=30, batch_size=1024, early_stop_patience=15, learning_rate=0.00027373312734509667, loss='MSE', lradj='TST', pct_start=0.3, use_amp=False, moving_avg=30, decomp_method='moving_avg', use_norm=1, down_sampling_layers=2, down_sampling_window=2, down_sampling_method='avg'), scheduler_settings=Namespace(type='OneCycleLR', T_max=20, weight_decay=0.0001, patience=5, reduce_factor=0.2, pct_start=0.3, eta_min='1e-6', warmup_steps=0), gpu_settings=Namespace(use_gpu=True, gpu_id=0, use_multi_gpu=False, use_all_gpus_for_search=False), additional_settings=None, logging_settings=Namespace(use_wandb=False))


[I 2024-08-12 21:10:17,642] Trial 91 finished with value: 0.6835662733763457 and parameters: {'hidden_d_model': 704, 'token_conv_kernel': 11, 'last_d_model': 384, 'seq_len': 42, 'token_d_model': 24, 'time_d_model': 192, 'pos_d_model': 96, 'd_model': 768, 'conv_out_dim': 320, 'e_layers': 8, 'learning_rate': 0.00027373312734509667, 'dropout': 0.26, 'combine_type': 'add', 'use_pos_enc': True, 'norm_type': 'batch', 'fc_layer_type': 'mha', 'batch_size': 1024, 'train_epochs': 30}. Best is trial 26 with value: 0.5440689027309418.
[W 2024-08-12 21:10:17,911] The parameter 'norm_type' in trial#93 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.
Base config keys: dict_keys(['general_settings', 'output_paths', 'data_paths', 'data_settings', 'model_settings', 'exp_settings', 'training_settings', 'scheduler_settings', 'gpu_settings', 'additional_settings', 'logging_settings'])
Suggested hyperparameters keys: dict_keys(['hidden_d_model', 'token_conv_kernel', 'last_d_model', 'seq_len', 'token_d_model', 'time_d_model', 'pos_d_model', 'd_model', 'conv_out_dim', 'e_layers', 'learning_rate', 'dropout', 'combine_type', 'use_pos_enc', 'norm_type', 'fc_layer_type', 'batch_size', 'train_epochs'])
Seed set to 17
/home/lsf/anaconda3/envs/Pein_310/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data3/lsf/Pein/Power-Prediction/run_scripts/run_opt ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name      | Type    | Params | Mode 
----------------------------------------------
0 | model     | Model   | 49.8 M | train
1 | criterion | MSELoss | 0      | train
----------------------------------------------
49.8 M    Trainable params
0         Non-trainable params
49.8 M    Total params
199.213   Total estimated model params size (MB)
Metric Loss/val improved. New best score: 1.243
Metric Loss/val improved by 0.024 >= min_delta = 0.0. New best score: 1.219
Metric Loss/val improved by 0.097 >= min_delta = 0.0. New best score: 1.122
Metric Loss/val improved by 0.159 >= min_delta = 0.0. New best score: 0.963
Metric Loss/val improved by 0.103 >= min_delta = 0.0. New best score: 0.861
Metric Loss/val improved by 0.062 >= min_delta = 0.0. New best score: 0.798
Metric Loss/val improved by 0.096 >= min_delta = 0.0. New best score: 0.702
